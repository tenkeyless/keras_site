---
title: VAEë¥¼ ì‚¬ìš©í•œ ì•½ë¬¼ ë¶„ì ìƒì„±
linkTitle: VAEë¡œ ì•½ë¬¼ ë¶„ì ìƒì„±
toc: true
weight: 25
type: docs
math: true
---

{{< keras/original checkedAt="2024-11-23" >}}

**{{< t f_author >}}** [Victor Basu](https://www.linkedin.com/in/victor-basu-520958147)  
**{{< t f_date_created >}}** 2022/03/10  
**{{< t f_last_modified >}}** 2022/03/24  
**{{< t f_description >}}** ì•½ë¬¼ ë°œê²¬ì„ ìœ„í•œ ì»¨ë³¼ë£¨ì…˜ ë³€ë¶„ ì˜¤í† ì¸ì½”ë” (VAE, Variational AutoEncoder) êµ¬í˜„

{{< keras/version v=2 >}}

{{< cards cols="2" >}}
{{< card link="https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/molecule_generation.ipynb" title="Colab" tag="Colab" tagType="warning">}}
{{< card link="https://github.com/keras-team/keras-io/blob/master/examples/generative/molecule_generation.py" title="GitHub" tag="GitHub">}}
{{< /cards >}}

## ì†Œê°œ {#introduction}

ì´ ì˜ˆì‹œì—ì„œëŠ”, Variational Autoencoder (VAE)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•½ë¬¼ ë°œê²¬ì„ ìœ„í•œ ë¶„ìë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ì´ ì˜ˆì‹œëŠ” ë…¼ë¬¸ [Automatic chemical design using a data-driven continuous representation of molecules](https://arxiv.org/abs/1610.02415)ì™€ [MolGAN: An implicit generative model for small molecular graphs](https://arxiv.org/abs/1805.11973)ë¥¼ ì°¸ì¡°í•˜ì˜€ìŠµë‹ˆë‹¤.

ë…¼ë¬¸ **Automatic chemical design using a data-driven continuous representation of molecules**ì— ì„¤ëª…ëœ ëª¨ë¸ì€,
í™”í•©ë¬¼ì˜ ì—´ë¦° ê³µê°„(open-ended spaces)ì„ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ìƒˆë¡œìš´ ë¶„ìë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ì´ ëª¨ë¸ì€ ì¸ì½”ë”(Encoder), ë””ì½”ë”(Decoder) ë° ì˜ˆì¸¡ê¸°(Predictor)ì˜ ì„¸ ê°€ì§€ êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
ì¸ì½”ë”ëŠ” ë¶„ìì˜ ì´ì‚° í‘œí˜„ì„ ì‹¤ìˆ˜ë¡œ ëœ ì—°ì† ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ ,
ë””ì½”ë”ëŠ” ì´ëŸ¬í•œ ì—°ì† ë²¡í„°ë¥¼ ë‹¤ì‹œ ì´ì‚°ì ì¸ ë¶„ì í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
ì˜ˆì¸¡ê¸°ëŠ” ë¶„ìì˜ ì—°ì†ì ì¸ ë²¡í„° í‘œí˜„ì—ì„œ í™”í•™ì  íŠ¹ì„±ì„ ì¶”ì •í•©ë‹ˆë‹¤.
ì—°ì†ì ì¸ í‘œí˜„ì€ ê¸°ìš¸ê¸° ê¸°ë°˜ ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì í™”ëœ ê¸°ëŠ¥ì„± í™”í•©ë¬¼ì„ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

![intro](/images/examples/generative/molecule_generation/3CtPMzM.png)

- **ê·¸ë¦¼ (a)**

  - ë¶„ì ì„¤ê³„ë¥¼ ìœ„í•œ ì˜¤í† ì¸ì½”ë” ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ, ê³µë™ íŠ¹ì„±(joint property) ì˜ˆì¸¡ ëª¨ë¸ì„ í¬í•¨í•©ë‹ˆë‹¤.
  - SMILES ë¬¸ìì—´ê³¼ ê°™ì€ ì´ì‚°ì ì¸ ë¶„ì í‘œí˜„ì„ ì‹œì‘ìœ¼ë¡œ, ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ëŠ” ê° ë¶„ìë¥¼ ì ì¬ ê³µê°„ì˜ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - ì´ ë²¡í„°ëŠ” ì‹¤ì§ˆì ìœ¼ë¡œ ì—°ì†ì ì¸ ë¶„ì í‘œí˜„ì…ë‹ˆë‹¤.
    - ì ì¬ ê³µê°„ì˜ í•œ ì§€ì ì„ ì£¼ë©´, ë””ì½”ë” ë„¤íŠ¸ì›Œí¬ëŠ” ì´ì— í•´ë‹¹í•˜ëŠ” SMILES ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.
  - ë‹¤ì¤‘ ë ˆì´ì–´ í¼ì…‰íŠ¸ë¡  ë„¤íŠ¸ì›Œí¬ëŠ” ê° ë¶„ìì™€ ì—°ê´€ëœ ëª©í‘œ íŠ¹ì„±ì˜ ê°’ì„ ì¶”ì •í•©ë‹ˆë‹¤.

- **ê·¸ë¦¼ (b)**
  - ì—°ì†ì ì¸ ì ì¬ ê³µê°„ì—ì„œ ê¸°ìš¸ê¸° ê¸°ë°˜ ìµœì í™”ì…ë‹ˆë‹¤.
  - ì ì¬ í‘œí˜„ `z`ì— ê¸°ë°˜í•œ ë¶„ìì˜ íŠ¹ì„±ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì„œë¸Œê²Œì´íŠ¸ ëª¨ë¸ `f(z)`ì„ íŠ¸ë ˆì´ë‹í•œ í›„,
  - ìš°ë¦¬ëŠ” `f(z)`ë¥¼ `z`ì— ëŒ€í•´ ìµœì í™”í•˜ì—¬ íŠ¹ì •í•œ ëª©í‘œ íŠ¹ì„±ì— ë¶€í•©í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ìƒˆë¡œìš´ ì ì¬ í‘œí˜„ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - ì´ëŸ¬í•œ ìƒˆë¡œìš´ ì ì¬ í‘œí˜„ì€ SMILES ë¬¸ìì—´ë¡œ ë””ì½”ë”©ëœ í›„, ì´ë“¤ì˜ íŠ¹ì„±ì´ ì‹¤í—˜ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

MolGANì˜ ì„¤ëª…ê³¼ êµ¬í˜„ì— ëŒ€í•´ì„œëŠ” Keras ì˜ˆì‹œ [**WGAN-GP with R-GCN for the generation of small molecular graphs**]({{< relref "/docs/examples/generative/wgan-graphs" >}})ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.
í˜„ì¬ ì˜ˆì‹œì—ì„œ ì‚¬ìš©ëœ ë§ì€ í•¨ìˆ˜ëŠ” ìœ„ì˜ Keras ì˜ˆì‹œì—ì„œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.

## ì…‹ì—… {#setup}

RDKitëŠ” í™”í•™ì •ë³´í•™ ë° ê¸°ê³„ í•™ìŠµì„ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ íˆ´í‚·ì…ë‹ˆë‹¤.
ì´ íˆ´í‚·ì€ ì•½ë¬¼ ë°œê²¬ ë¶„ì•¼ì—ì„œ íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤.
ì´ ì˜ˆì‹œì—ì„œëŠ”, RDKitë¥¼ ì‚¬ìš©í•˜ì—¬ SMILESë¥¼ ë¶„ì ê°ì²´ë¡œ í¸ë¦¬í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë³€í™˜í•œ í›„,
ì´ ë¶„ì ê°ì²´ì—ì„œ ì›ìì™€ ê²°í•©ì˜ ì§‘í•©ì„ ì–»ìŠµë‹ˆë‹¤.

[WGAN-GP with R-GCN for the generation of small molecular graphs]({{< relref "/docs/examples/generative/wgan-graphs" >}})ì—ì„œ ì¸ìš©í•˜ìë©´:

> **"SMILESëŠ” ì£¼ì–´ì§„ ë¶„ìì˜ êµ¬ì¡°ë¥¼ ASCII ë¬¸ìì—´ì˜ í˜•íƒœë¡œ í‘œí˜„í•©ë‹ˆë‹¤. SMILES ë¬¸ìì—´ì€ ì‘ì€ ë¶„ìì˜ ê²½ìš°, ë¹„êµì  ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ì••ì¶•ëœ ì¸ì½”ë”©ì…ë‹ˆë‹¤. ë¶„ìë¥¼ ë¬¸ìì—´ë¡œ ì¸ì½”ë”©í•¨ìœ¼ë¡œì¨, ì£¼ì–´ì§„ ë¶„ìì˜ ë°ì´í„°ë² ì´ìŠ¤ ë°/ë˜ëŠ” ì›¹ ê²€ìƒ‰ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. RDKitëŠ” ì£¼ì–´ì§„ SMILESë¥¼ ì •í™•í•˜ê²Œ ë¶„ì ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ë©°, ì´ë¥¼ í†µí•´ ìˆ˜ë§ì€ ë¶„ì íŠ¹ì„±/íŠ¹ì§•ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."**

```python
!pip -q install rdkit-pypi==2021.9.4
```

```python
import ast

import pandas as pd
import numpy as np

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import matplotlib.pyplot as plt
from rdkit import Chem, RDLogger
from rdkit.Chem import BondType
from rdkit.Chem.Draw import MolsToGridImage

RDLogger.DisableLog("rdApp.*")
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.6 MB 1.2 MB/s
[?25h

```

{{% /details %}}

## ë°ì´í„°ì„¸íŠ¸ {#dataset}

ìš°ë¦¬ëŠ” [**ZINC â€“ A Free Database of Commercially Available Compounds for Virtual Screening**](https://bit.ly/3IVBI4x) ë°ì´í„°ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ì´ ë°ì´í„°ì…‹ì—ëŠ” SMILES í‘œê¸°ë¡œ í‘œí˜„ëœ ë¶„ì ê³µì‹ê³¼ í•¨ê»˜,
**logP** (ë¬¼-ì˜¥íƒ„ì˜¬ ë¶„ë°° ê³„ìˆ˜), **SAS** (í•©ì„± ìš©ì´ì„± ì ìˆ˜),
**QED** (ì•½ë¬¼ ìœ ì‚¬ì„±ì˜ ì •ì„±ì  ì¶”ì •)ê³¼ ê°™ì€ í•´ë‹¹ ë¶„ìì˜ ë¶„ì íŠ¹ì„±ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```python
csv_path = keras.utils.get_file(
    "/content/250k_rndm_zinc_drugs_clean_3.csv",
    "https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv",
)

df = pd.read_csv("/content/250k_rndm_zinc_drugs_clean_3.csv")
df["smiles"] = df["smiles"].apply(lambda s: s.replace("\n", ""))
df.head()
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Downloading data from https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv
22606589/22606589 [==============================] - 0s 0us/step
```

{{% /details %}}

|     | smiles                                                | logP    | qed      | SAS      |
| --- | ----------------------------------------------------- | ------- | -------- | -------- |
| 0   | CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1               | 5.05060 | 0.702012 | 2.084095 |
| 1   | C\[C@@H\]1CC(Nc2cncc(-c3nncn3C)c2)C\[C@@H\](C)C1      | 3.11370 | 0.928975 | 3.432004 |
| 2   | N#Cc1ccc(-c2ccc(O\[C@@H\](C(=O)N3CCCC3)c3ccccc3)...   | 4.96778 | 0.599682 | 2.470633 |
| 3   | CCOC(=O)\[C@@H\]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...   | 4.00022 | 0.690944 | 2.822753 |
| 4   | N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C(\[O-\])\[C@H\](C#... | 3.60956 | 0.789027 | 4.035182 |

## í•˜ì´í¼íŒŒë¼ë¯¸í„° {#hyperparameters}

```python
SMILE_CHARSET = '["C", "B", "F", "I", "H", "O", "N", "S", "P", "Cl", "Br"]'

bond_mapping = {"SINGLE": 0, "DOUBLE": 1, "TRIPLE": 2, "AROMATIC": 3}
bond_mapping.update(
    {0: BondType.SINGLE, 1: BondType.DOUBLE, 2: BondType.TRIPLE, 3: BondType.AROMATIC}
)
SMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)

MAX_MOLSIZE = max(df["smiles"].str.len())
SMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))
index_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))
atom_mapping = dict(SMILE_to_index)
atom_mapping.update(index_to_SMILE)

BATCH_SIZE = 100
EPOCHS = 10

VAE_LR = 5e-4
NUM_ATOMS = 120  # ìµœëŒ€ ì›ì ìˆ˜

ATOM_DIM = len(SMILE_CHARSET)  # ì›ì íƒ€ì… ìˆ˜
BOND_DIM = 4 + 1  # ê²°í•© íƒ€ì… ìˆ˜
LATENT_DIM = 435  # ì ì¬ ê³µê°„ í¬ê¸°


def smiles_to_graph(smiles):
    # SMILESë¥¼ ë¶„ì ê°ì²´ë¡œ ë³€í™˜
    molecule = Chem.MolFromSmiles(smiles)

    # ì¸ì ‘ í–‰ë ¬ê³¼ íŠ¹ì„± í…ì„œ ì´ˆê¸°í™”
    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), "float32")
    features = np.zeros((NUM_ATOMS, ATOM_DIM), "float32")

    # ë¶„ìì˜ ê° ì›ìì— ëŒ€í•´ ë£¨í”„ ì‹¤í–‰
    for atom in molecule.GetAtoms():
        i = atom.GetIdx()
        atom_type = atom_mapping[atom.GetSymbol()]
        features[i] = np.eye(ATOM_DIM)[atom_type]
        # ì›-í™‰ ì´ì›ƒì— ëŒ€í•´ ë£¨í”„ ì‹¤í–‰
        for neighbor in atom.GetNeighbors():
            j = neighbor.GetIdx()
            bond = molecule.GetBondBetweenAtoms(i, j)
            bond_type_idx = bond_mapping[bond.GetBondType().name]
            adjacency[bond_type_idx, [i, j], [j, i]] = 1

    # ê²°í•©ì´ ì—†ëŠ” ê²½ìš°, ë§ˆì§€ë§‰ ì±„ë„ì— 1ì„ ì¶”ê°€í•˜ì—¬ "ë¹„ê²°í•©"ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    # Notice: channels-first
    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1

    # ì›ìê°€ ì—†ëŠ” ê²½ìš°, ë§ˆì§€ë§‰ ì—´ì— 1ì„ ì¶”ê°€í•˜ì—¬ "ë¹„ì›ì"ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤
    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1

    return adjacency, features


def graph_to_molecule(graph):
    # ê·¸ë˜í”„ ì–¸íŒ©
    adjacency, features = graph

    # RWMolì€ ìˆ˜ì • ê°€ëŠ¥í•œ ë¶„ì ê°ì²´ì…ë‹ˆë‹¤
    molecule = Chem.RWMol()

    # "ë¹„ì›ì" ë° ê²°í•©ì´ ì—†ëŠ” ì›ìë¥¼ ì œê±°
    keep_idx = np.where(
        (np.argmax(features, axis=1) != ATOM_DIM - 1)
        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)
    )[0]
    features = features[keep_idx]
    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]

    # ë¶„ìì— ì›ì ì¶”ê°€
    for atom_type_idx in np.argmax(features, axis=1):
        atom = Chem.Atom(atom_mapping[atom_type_idx])
        _ = molecule.AddAtom(atom)

    # ë¶„ì ë‚´ ì›ì ê°„ ê²°í•© ì¶”ê°€; [symmetric] ì¸ ì¸ì ‘ í…ì„œì˜ ìƒì‚¼ê°í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•¨
    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)
    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):
        if atom_i == atom_j or bond_ij == BOND_DIM - 1:
            continue
        bond_type = bond_mapping[bond_ij]
        molecule.AddBond(int(atom_i), int(atom_j), bond_type)

    # ë¶„ì ì •í™”; ì •í™”ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€
    # https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization ì„ ì°¸ì¡°í•˜ì„¸ìš”
    flag = Chem.SanitizeMol(molecule, catchErrors=True)
    # ì—„ê²©í•˜ê²Œ ì²˜ë¦¬. ì •í™”ê°€ ì‹¤íŒ¨í•˜ë©´ Noneì„ ë°˜í™˜
    if flag != Chem.SanitizeFlags.SANITIZE_NONE:
        return None

    return molecule
```

## íŠ¸ë ˆì´ë‹ ì„¸íŠ¸ ìƒì„± {#generate-training-set}

```python
train_df = df.sample(frac=0.75, random_state=42)  # random stateëŠ” ì‹œë“œ ê°’ì…ë‹ˆë‹¤
train_df.reset_index(drop=True, inplace=True)

adjacency_tensor, feature_tensor, qed_tensor = [], [], []
for idx in range(8000):
    adjacency, features = smiles_to_graph(train_df.loc[idx]["smiles"])
    qed = train_df.loc[idx]["qed"]
    adjacency_tensor.append(adjacency)
    feature_tensor.append(features)
    qed_tensor.append(qed)

adjacency_tensor = np.array(adjacency_tensor)
feature_tensor = np.array(feature_tensor)
qed_tensor = np.array(qed_tensor)


class RelationalGraphConvLayer(keras.layers.Layer):
    def __init__(
        self,
        units=128,
        activation="relu",
        use_bias=False,
        kernel_initializer="glorot_uniform",
        bias_initializer="zeros",
        kernel_regularizer=None,
        bias_regularizer=None,
        **kwargs
    ):
        super().__init__(**kwargs)

        self.units = units
        self.activation = keras.activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = keras.initializers.get(kernel_initializer)
        self.bias_initializer = keras.initializers.get(bias_initializer)
        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)
        self.bias_regularizer = keras.regularizers.get(bias_regularizer)

    def build(self, input_shape):
        bond_dim = input_shape[0][1]
        atom_dim = input_shape[1][2]

        self.kernel = self.add_weight(
            shape=(bond_dim, atom_dim, self.units),
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            trainable=True,
            name="W",
            dtype=tf.float32,
        )

        if self.use_bias:
            self.bias = self.add_weight(
                shape=(bond_dim, 1, self.units),
                initializer=self.bias_initializer,
                regularizer=self.bias_regularizer,
                trainable=True,
                name="b",
                dtype=tf.float32,
            )

        self.built = True

    def call(self, inputs, training=False):
        adjacency, features = inputs
        # ì´ì›ƒìœ¼ë¡œë¶€í„° ì •ë³´ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤
        x = tf.matmul(adjacency, features[:, None, :, :])
        # ì„ í˜• ë³€í™˜ì„ ì ìš©í•©ë‹ˆë‹¤
        x = tf.matmul(x, self.kernel)
        if self.use_bias:
            x += self.bias
        # ê²°í•© íƒ€ì… ì°¨ì›ì„ ì¤„ì…ë‹ˆë‹¤
        x_reduced = tf.reduce_sum(x, axis=1)
        # ë¹„ì„ í˜• ë³€í™˜ì„ ì ìš©í•©ë‹ˆë‹¤
        return self.activation(x_reduced)
```

## ì¸ì½”ë” ë° ë””ì½”ë” ë¹Œë“œ {#build-the-encoder-and-decoder}

ì¸ì½”ë”ëŠ” ë¶„ìì˜ ê·¸ë˜í”„ ì¸ì ‘ í–‰ë ¬(adjacency matrix)ê³¼ íŠ¹ì„± í–‰ë ¬ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
ì´ íŠ¹ì„±ë“¤ì€ ê·¸ë˜í”„ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ í†µí•´ ì²˜ë¦¬ëœ í›„,
í”Œë˜íŠ¼(flatten)ë˜ê³  ì—¬ëŸ¬ Dense ë ˆì´ì–´ë¥¼ í†µí•´,
`z_mean`ê³¼ `log_var`, ì¦‰ ë¶„ìì˜ ì ì¬ ê³µê°„ í‘œí˜„ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.

**ê·¸ë˜í”„ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´**: ê´€ê³„ì  ê·¸ë˜í”„ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ëŠ” ë¹„ì„ í˜• ë³€í™˜ëœ ì´ì›ƒ ì§‘ê³„ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
ì´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$
H^{l+1} = Ïƒ(D^{-1} @ A @ H^{l+1} @ W^{l})
$$

ì—¬ê¸°ì„œ $\sigma$ëŠ” ë¹„ì„ í˜• ë³€í™˜(ì£¼ë¡œ ReLU í™œì„±í™” í•¨ìˆ˜)ì„ ë‚˜íƒ€ë‚´ê³ ,
$A$ëŠ” ì¸ì ‘ í…ì„œ, $H^{l}$ëŠ” `l`ë²ˆì§¸ ë ˆì´ì–´ì˜ íŠ¹ì„± í…ì„œ,
$D^{-1}$ëŠ” $A^$ì˜ ì—­ ëŒ€ê°(inverse diagonal) í–‰ë ¬,
$W^{l}$ëŠ” `l`ë²ˆì§¸ ë ˆì´ì–´ì—ì„œ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ í…ì„œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
íŠ¹íˆ, ê° ê²°í•© ìœ í˜•(ê´€ê³„)ë§ˆë‹¤ ëŒ€ê°(diagonal) í–‰ë ¬ì€ ê° ì›ìì— ì—°ê²°ëœ ê²°í•© ìˆ˜ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.

ì¶œì²˜: [WGAN-GP with R-GCNì„ ì´ìš©í•œ ì†Œë¶„ì ê·¸ë˜í”„ ìƒì„±]({{< relref "/docs/examples/generative/wgan-graphs" >}})

ë””ì½”ë”ëŠ” ì ì¬ ê³µê°„ í‘œí˜„ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, í•´ë‹¹ ë¶„ìì˜ ê·¸ë˜í”„ ì¸ì ‘ í–‰ë ¬ê³¼ íŠ¹ì„± í–‰ë ¬ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

```python
def get_encoder(
    gconv_units, latent_dim, adjacency_shape, feature_shape, dense_units, dropout_rate
):
    adjacency = keras.layers.Input(shape=adjacency_shape)
    features = keras.layers.Input(shape=feature_shape)

    # í•˜ë‚˜ ì´ìƒì˜ ê·¸ë˜í”„ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ í†µí•´ ì „íŒŒ
    features_transformed = features
    for units in gconv_units:
        features_transformed = RelationalGraphConvLayer(units)(
            [adjacency, features_transformed]
        )
    # 2D ë¶„ì í‘œí˜„ì„ 1Dë¡œ ì¶•ì†Œ
    x = keras.layers.GlobalAveragePooling1D()(features_transformed)

    # í•˜ë‚˜ ì´ìƒì˜ ë°€ì§‘ ë ˆì´ì–´ë¥¼ í†µí•´ ì „íŒŒ
    for units in dense_units:
        x = layers.Dense(units, activation="relu")(x)
        x = layers.Dropout(dropout_rate)(x)

    z_mean = layers.Dense(latent_dim, dtype="float32", name="z_mean")(x)
    log_var = layers.Dense(latent_dim, dtype="float32", name="log_var")(x)

    encoder = keras.Model([adjacency, features], [z_mean, log_var], name="encoder")

    return encoder


def get_decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape):
    latent_inputs = keras.Input(shape=(latent_dim,))

    x = latent_inputs
    for units in dense_units:
        x = keras.layers.Dense(units, activation="tanh")(x)
        x = keras.layers.Dropout(dropout_rate)(x)

    # ì´ì „ ë ˆì´ì–´ ì¶œë ¥(x)ì„ [ì—°ì†ì ì¸] ì¸ì ‘ í…ì„œ(x_adjacency)ë¡œ ë§¤í•‘
    x_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(x)
    x_adjacency = keras.layers.Reshape(adjacency_shape)(x_adjacency)
    # ë§ˆì§€ë§‰ ë‘ ì°¨ì›ì„ ëŒ€ì¹­í™”
    x_adjacency = (x_adjacency + tf.transpose(x_adjacency, (0, 1, 3, 2))) / 2
    x_adjacency = keras.layers.Softmax(axis=1)(x_adjacency)

    # ì´ì „ ë ˆì´ì–´ ì¶œë ¥(x)ì„ [ì—°ì†ì ì¸] íŠ¹ì„± í…ì„œ(x_features)ë¡œ ë§¤í•‘
    x_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(x)
    x_features = keras.layers.Reshape(feature_shape)(x_features)
    x_features = keras.layers.Softmax(axis=2)(x_features)

    decoder = keras.Model(
        latent_inputs, outputs=[x_adjacency, x_features], name="decoder"
    )

    return decoder
```

## ìƒ˜í”Œë§ ë ˆì´ì–´ ë¹Œë“œ {#build-the-sampling-layer}

```python
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_log_var)[0]
        dim = tf.shape(z_log_var)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
```

## VAE ë¹Œë“œ {#build-the-vae}

ì´ ëª¨ë¸ì€ ë‹¤ìŒ ë„¤ ê°€ì§€ ì†ì‹¤ì„ ìµœì í™”í•˜ë„ë¡ íŠ¸ë ˆì´ë‹ë©ë‹ˆë‹¤:

- ë²”ì£¼í˜• êµì°¨ ì—”íŠ¸ë¡œí”¼
- KL ë°œì‚° ì†ì‹¤
- ì†ì„± ì˜ˆì¸¡ ì†ì‹¤
- ê·¸ë˜í”„ ì†ì‹¤(ê¸°ìš¸ê¸° íŒ¨ë„í‹°)

ë²”ì£¼í˜• êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ì¬êµ¬ì„± ì •í™•ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
ì†ì„± ì˜ˆì¸¡ ì†ì‹¤ì€ ì ì¬ í‘œí˜„ì„ ì†ì„± ì˜ˆì¸¡ ëª¨ë¸ì— í†µê³¼ì‹œí‚¨ í›„,
ì˜ˆì¸¡ëœ ì†ì„±ê³¼ ì‹¤ì œ ì†ì„± ê°„ì˜ í‰ê·  ì œê³± ì˜¤ì°¨ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
ëª¨ë¸ì˜ ì†ì„± ì˜ˆì¸¡ì€ ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ë¥¼ í†µí•´ ìµœì í™”ë©ë‹ˆë‹¤.
ê·¸ë˜í”„ ì†ì‹¤ì€ ëª¨ë¸ì˜ ì†ì„±(QED) ì˜ˆì¸¡ì— ì˜í•´ ì¶”ê°€ì ìœ¼ë¡œ ì•ˆë‚´ë©ë‹ˆë‹¤.

ê¸°ìš¸ê¸° íŒ¨ë„í‹°ëŠ” ì›ë³¸ì‹ ê²½ë§ì—ì„œ ì‚¬ìš©ëœ ê¸°ìš¸ê¸° í´ë¦¬í•‘ ë°©ì‹ì— ëŒ€í•œ ê°œì„ ìœ¼ë¡œ,
1-Lipschitz ì—°ì†ì„±ì— ëŒ€í•œ ëŒ€ì•ˆì ì¸ ë¶€ë“œëŸ¬ìš´ ì œì•½ ì¡°ê±´ì…ë‹ˆë‹¤.
("1-Lipschitz ì—°ì†ì„±"ì€ í•¨ìˆ˜ì˜ ëª¨ë“  ì ì—ì„œ ê¸°ìš¸ê¸°ì˜ ë…¸ë¦„(ê¸¸ì´)ì´ ìµœëŒ€ 1ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.)
ì´ëŠ” ì†ì‹¤ í•¨ìˆ˜ì— ì •ê·œí™” í•­ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

```python
class MoleculeGenerator(keras.Model):
    def __init__(self, encoder, decoder, max_len, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.property_prediction_layer = layers.Dense(1)
        self.max_len = max_len

        self.train_total_loss_tracker = keras.metrics.Mean(name="train_total_loss")
        self.val_total_loss_tracker = keras.metrics.Mean(name="val_total_loss")

    def train_step(self, data):
        adjacency_tensor, feature_tensor, qed_tensor = data[0]
        graph_real = [adjacency_tensor, feature_tensor]
        self.batch_size = tf.shape(qed_tensor)[0]
        with tf.GradientTape() as tape:
            z_mean, z_log_var, qed_pred, gen_adjacency, gen_features = self(
                graph_real, training=True
            )
            graph_generated = [gen_adjacency, gen_features]
            total_loss = self._compute_loss(
                z_log_var, z_mean, qed_tensor, qed_pred, graph_real, graph_generated
            )

        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        self.train_total_loss_tracker.update_state(total_loss)
        return {"loss": self.train_total_loss_tracker.result()}

    def _compute_loss(
        self, z_log_var, z_mean, qed_true, qed_pred, graph_real, graph_generated
    ):

        adjacency_real, features_real = graph_real
        adjacency_gen, features_gen = graph_generated

        adjacency_loss = tf.reduce_mean(
            tf.reduce_sum(
                keras.losses.categorical_crossentropy(adjacency_real, adjacency_gen),
                axis=(1, 2),
            )
        )
        features_loss = tf.reduce_mean(
            tf.reduce_sum(
                keras.losses.categorical_crossentropy(features_real, features_gen),
                axis=(1),
            )
        )
        kl_loss = -0.5 * tf.reduce_sum(
            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), 1
        )
        kl_loss = tf.reduce_mean(kl_loss)

        property_loss = tf.reduce_mean(
            keras.losses.binary_crossentropy(qed_true, qed_pred)
        )

        graph_loss = self._gradient_penalty(graph_real, graph_generated)

        return kl_loss + property_loss + graph_loss + adjacency_loss + features_loss

    def _gradient_penalty(self, graph_real, graph_generated):
        # ê·¸ë˜í”„ ì–¸íŒ©
        adjacency_real, features_real = graph_real
        adjacency_generated, features_generated = graph_generated

        # ë³´ê°„ëœ ê·¸ë˜í”„(adjacency_interp ë° features_interp) ìƒì„±
        alpha = tf.random.uniform([self.batch_size])
        alpha = tf.reshape(alpha, (self.batch_size, 1, 1, 1))
        adjacency_interp = (adjacency_real * alpha) + (1 - alpha) * adjacency_generated
        alpha = tf.reshape(alpha, (self.batch_size, 1, 1))
        features_interp = (features_real * alpha) + (1 - alpha) * features_generated

        # ë³´ê°„ëœ ê·¸ë˜í”„ì˜ ë¡œì§“ ê³„ì‚°
        with tf.GradientTape() as tape:
            tape.watch(adjacency_interp)
            tape.watch(features_interp)
            _, _, logits, _, _ = self(
                [adjacency_interp, features_interp], training=True
            )

        # ë³´ê°„ëœ ê·¸ë˜í”„ì— ëŒ€í•œ ê¸°ìš¸ê¸° ê³„ì‚°
        grads = tape.gradient(logits, [adjacency_interp, features_interp])
        # ê¸°ìš¸ê¸° íŒ¨ë„í‹° ê³„ì‚°
        grads_adjacency_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2
        grads_features_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2
        return tf.reduce_mean(
            tf.reduce_mean(grads_adjacency_penalty, axis=(-2, -1))
            + tf.reduce_mean(grads_features_penalty, axis=(-1))
        )

    def inference(self, batch_size):
        z = tf.random.normal((batch_size, LATENT_DIM))
        reconstruction_adjacency, reconstruction_features = model.decoder.predict(z)
        # ì¸ì ‘ í…ì„œì— ëŒ€í•´ ì›-í•« ì¸ì½”ë”© ìˆ˜í–‰
        adjacency = tf.argmax(reconstruction_adjacency, axis=1)
        adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)
        # ì¸ì ‘ í…ì„œì—ì„œ ìê°€ ê²°í•© ì œê±°
        adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))
        # íŠ¹ì„± í…ì„œì— ëŒ€í•´ ì›-í•« ì¸ì½”ë”© ìˆ˜í–‰
        features = tf.argmax(reconstruction_features, axis=2)
        features = tf.one_hot(features, depth=ATOM_DIM, axis=2)
        return [
            graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])
            for i in range(batch_size)
        ]

    def call(self, inputs):
        z_mean, log_var = self.encoder(inputs)
        z = Sampling()([z_mean, log_var])

        gen_adjacency, gen_features = self.decoder(z)

        property_pred = self.property_prediction_layer(z_mean)

        return z_mean, log_var, property_pred, gen_adjacency, gen_features
```

## ëª¨ë¸ íŠ¸ë ˆì´ë‹ {#train-the-model}

```python
vae_optimizer = tf.keras.optimizers.Adam(learning_rate=VAE_LR)

encoder = get_encoder(
    gconv_units=[9],
    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),
    feature_shape=(NUM_ATOMS, ATOM_DIM),
    latent_dim=LATENT_DIM,
    dense_units=[512],
    dropout_rate=0.0,
)
decoder = get_decoder(
    dense_units=[128, 256, 512],
    dropout_rate=0.2,
    latent_dim=LATENT_DIM,
    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),
    feature_shape=(NUM_ATOMS, ATOM_DIM),
)

model = MoleculeGenerator(encoder, decoder, MAX_MOLSIZE)

model.compile(vae_optimizer)
history = model.fit([adjacency_tensor, feature_tensor, qed_tensor], epochs=EPOCHS)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Epoch 1/10
250/250 [==============================] - 24s 84ms/step - loss: 68958.3946
Epoch 2/10
250/250 [==============================] - 20s 79ms/step - loss: 68819.8421
Epoch 3/10
250/250 [==============================] - 20s 79ms/step - loss: 68830.6720
Epoch 4/10
250/250 [==============================] - 20s 79ms/step - loss: 68816.1486
Epoch 5/10
250/250 [==============================] - 20s 79ms/step - loss: 68825.9977
Epoch 6/10
250/250 [==============================] - 19s 78ms/step - loss: 68818.0771
Epoch 7/10
250/250 [==============================] - 19s 77ms/step - loss: 68815.8525
Epoch 8/10
250/250 [==============================] - 20s 78ms/step - loss: 68820.5459
Epoch 9/10
250/250 [==============================] - 21s 83ms/step - loss: 68806.9465
Epoch 10/10
250/250 [==============================] - 21s 84ms/step - loss: 68805.9879
```

{{% /details %}}

## ì¶”ë¡  {#inference}

ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì ì¬ ê³µê°„ì˜ ë‹¤ì–‘í•œ ì§€ì ì—ì„œ ìƒˆë¡œìš´ ìœ íš¨ ë¶„ìë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

### ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ unique ë¶„ì ìƒì„± {#generate-unique-molecules-with-the-model}

```python
molecules = model.inference(1000)

MolsToGridImage(
    [m for m in molecules if m is not None][:1000], molsPerRow=5, subImgSize=(260, 160)
)
```

![png](/images/examples/generative/molecule_generation/molecule_generation_21_0.png)

### ë¶„ì íŠ¹ì„±(QAE)ì— ë”°ë¥¸ ì ì¬ ê³µê°„ í´ëŸ¬ìŠ¤í„° í‘œì‹œ {#display-latent-space-clusters-with-respect-to-molecular-properties-qae}

```python
def plot_latent(vae, data, labels):
    # ì ì¬ ê³µê°„ì—ì„œ íŠ¹ì„±ì— ë”°ë¥¸ 2D í”Œë¡¯ì„ í‘œì‹œí•©ë‹ˆë‹¤.
    z_mean, _ = vae.encoder.predict(data)
    plt.figure(figsize=(12, 10))
    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)
    plt.colorbar()
    plt.xlabel("z[0]")
    plt.ylabel("z[1]")
    plt.show()


plot_latent(model, [adjacency_tensor[:8000], feature_tensor[:8000]], qed_tensor[:8000])
```

![png](/images/examples/generative/molecule_generation/molecule_generation_23_0.png)

## ê²°ë¡  {#conclusion}

ì´ ì˜ˆì œì—ì„œëŠ”, 2016ë…„ì˜ "ë°ì´í„° ê¸°ë°˜ ì—°ì† í‘œí˜„ì„ ì‚¬ìš©í•˜ëŠ” ìë™ í™”í•™ ì„¤ê³„" ë…¼ë¬¸ê³¼
2018ë…„ì˜ "MolGAN" ë…¼ë¬¸ì˜ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ê²°í•©í–ˆìŠµë‹ˆë‹¤.
ì „ìëŠ” SMILES ì…ë ¥ì„ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ì—¬ SMILES í˜•ì‹ì˜ ë¶„ì ë¬¸ìì—´ì„ ìƒì„±í•˜ë ¤ í•˜ê³ ,
í›„ìëŠ” SMILES ì…ë ¥ì„ ê·¸ë˜í”„(ì¸ì ‘ í–‰ë ¬ê³¼ íŠ¹ì„± í–‰ë ¬ì˜ ì¡°í•©)ë¡œ ê³ ë ¤í•˜ì—¬ ë¶„ìë¥¼ ê·¸ë˜í”„ë¡œ ìƒì„±í•˜ë ¤ í•©ë‹ˆë‹¤.

ì´ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ë°©ì‹ì€ í™”í•™ ê³µê°„ì„ íƒìƒ‰í•˜ëŠ” ìƒˆë¡œìš´ ìœ í˜•ì˜ ìœ ë„ëœ ê²½ì‚¬ ê¸°ë°˜ ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

ì˜ˆì œëŠ” HuggingFaceì—ì„œ ì œê³µë©ë‹ˆë‹¤.

| íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸                                                                                                                                                                       | ë°ëª¨                                                                                                                                                                                          |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-molecule%20generation%20with%20VAE-black.svg)](https://huggingface.co/keras-io/drug-molecule-generation-with-VAE) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-molecule%20generation%20with%20VAE-black.svg)](https://huggingface.co/spaces/keras-io/generating-drug-molecule-with-VAE) |
