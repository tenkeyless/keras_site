---
title: KerasHubë¥¼ ì‚¬ìš©í•œ GPT2 í…ìŠ¤íŠ¸ ìƒì„±
linkTitle: GPT2 í…ìŠ¤íŠ¸ ìƒì„± (KerasHub)
toc: true
weight: 20
type: docs
---

{{< keras/original checkedAt="2024-11-23" >}}

**{{< t f_author >}}** Chen Qian  
**{{< t f_date_created >}}** 2023/04/17  
**{{< t f_last_modified >}}** 2024/04/12  
**{{< t f_description >}}** KerasHubì˜ GPT2 ëª¨ë¸ê³¼ `samplers`ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±.

{{< keras/version v=3 >}}

{{< cards cols="2" >}}
{{< card link="https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/gpt2_text_generation_with_kerashub.ipynb" title="Colab" tag="Colab" tagType="warning">}}
{{< card link="https://github.com/keras-team/keras-io/blob/master/examples/generative/gpt2_text_generation_with_kerashub.py" title="GitHub" tag="GitHub">}}
{{< /cards >}}

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” [KerasHub]({{< relref "/docs/keras_hub" >}})ë¥¼ ì‚¬ìš©í•˜ì—¬,
ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì¸
[GPT-2 ëª¨ë¸](https://openai.com/research/better-language-models)(ì›ë˜ OpenAIì—ì„œ ê°œë°œ)ì„ ë¶ˆëŸ¬ì˜¤ê³ ,
íŠ¹ì • í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ë¯¸ì„¸ íŠ¸ë ˆì´ë‹(finetuning)ì„ ì§„í–‰í•œ í›„,
ì‚¬ìš©ì ì…ë ¥(í”„ë¡¬í”„íŠ¸)ì— ê¸°ë°˜í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.
ë˜í•œ, GPT2ê°€ ì¤‘êµ­ì–´ì™€ ê°™ì€ ë¹„ì˜ì–´ê¶Œ ì–¸ì–´ì— ë¹ ë¥´ê²Œ ì ì‘í•˜ëŠ” ë°©ì‹ì„ ë°°ìš°ê²Œ ë©ë‹ˆë‹¤.

## ì‹œì‘í•˜ê¸° ì „ì— {#before-we-begin}

Colabì€ ì—¬ëŸ¬ ê°€ì§€ ëŸ°íƒ€ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
**Runtime -> Change runtime type**ìœ¼ë¡œ ì´ë™í•˜ì—¬,
GPU í•˜ë“œì›¨ì–´ ê°€ì†ê¸° ëŸ°íƒ€ì„(12GB ì´ìƒì˜ í˜¸ìŠ¤íŠ¸ RAM ë° ì•½ 15GBì˜ GPU RAMì´ ìˆì–´ì•¼ í•¨)ì„ ì„ íƒí•˜ì„¸ìš”.
GPT-2 ëª¨ë¸ì„ ë¯¸ì„¸ íŠ¸ë ˆì´ë‹í•  ì˜ˆì •ì´ë¯€ë¡œ, CPU ëŸ°íƒ€ì„ì—ì„œëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.

## KerasHub ì„¤ì¹˜, ë°±ì—”ë“œ ì„ íƒ ë° ì¢…ì†ì„± import {#install-kerashub-choose-backend-and-import-dependencies}

ì´ ì˜ˆì œì—ì„œëŠ” [Keras 3]({{< relref "/docs/keras_3" >}})ë¥¼ ì‚¬ìš©í•˜ì—¬,
`"tensorflow"`, `"jax"`, ë˜ëŠ” `"torch"` ì¤‘ ì–´ëŠ ê²ƒì´ë“  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
KerasHubì—ëŠ” Keras 3ì— ëŒ€í•œ ì§€ì›ì´ ë‚´ì¥ë˜ì–´ ìˆìœ¼ë©°,
ì‚¬ìš©í•˜ë ¤ëŠ” ë°±ì—”ë“œë¥¼ ì„ íƒí•˜ë ¤ë©´ `"KERAS_BACKEND"` í™˜ê²½ ë³€ìˆ˜ë¥¼ ë³€ê²½í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.
ì•„ë˜ì—ì„œëŠ” JAX ë°±ì—”ë“œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

```python
!pip install git+https://github.com/keras-team/keras-hub.git -q
```

```python
import os

os.environ["KERAS_BACKEND"] = "jax"  # ë˜ëŠ” "tensorflow" ë˜ëŠ” "torch"

import keras_hub
import keras
import tensorflow as tf
import time

keras.mixed_precision.set_global_policy("mixed_float16")
```

## ìƒì„±í˜• Large Language Models (LLMs) ì†Œê°œ {#introduction-to-generative-large-language-models-llms}

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„° ì½”í¼ìŠ¤ì— ëŒ€í•´ íŠ¸ë ˆì´ë‹ëœ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ,
í…ìŠ¤íŠ¸ ìƒì„±, ì§ˆë¬¸ ì‘ë‹µ, ê¸°ê³„ ë²ˆì—­ ë“± ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì—ì„œ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

ìƒì„±í˜• LLMì€ ì¼ë°˜ì ìœ¼ë¡œ (Googleì´ 2017ë…„ ê°œë°œí•œ)
[Transformer ì•„í‚¤í…ì²˜](https://arxiv.org/abs/1706.03762)ì™€ ê°™ì€ ë”¥ëŸ¬ë‹ ì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°,
ìˆ˜ì‹­ì–µ ê°œì˜ ë‹¨ì–´ê°€ í¬í•¨ëœ ë°©ëŒ€í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ë ˆì´ë‹ë©ë‹ˆë‹¤.
Google [LaMDA](https://blog.google/technology/ai/lamda/)ì™€
[PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) ê°™ì€ ëª¨ë¸ì€
ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œ íŠ¸ë ˆì´ë‹ë˜ë©°,
ì´ë¥¼ í†µí•´ ì—¬ëŸ¬ ì‘ì—…ì—ì„œ ì¶œë ¥ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ìƒì„±í˜• LLMì˜ í•µì‹¬ì€ ë¬¸ì¥ì—ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ,
ì´ëŠ” **Causal LM Pretraining**ì´ë¼ê³  ë¶ˆë¦½ë‹ˆë‹¤.
ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ LLMì€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ì¼ê´€ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ë” ê¹Šì´ ìˆëŠ” ë…¼ì˜ëŠ” [Stanford CS324 LLM ìˆ˜ì—…](https://stanford-cs324.github.io/winter2022/lectures/introduction/)ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

## KerasHub ì†Œê°œ {#introduction-to-kerashub}

ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° êµ¬ì¶•í•˜ê³  íŠ¸ë ˆì´ë‹í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë³µì¡í•˜ê³  ë¹„ìš©ì´ ë§ì´ ë“­ë‹ˆë‹¤.
ë‹¤í–‰íˆë„, ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ LLMë“¤ì´ ìˆìŠµë‹ˆë‹¤.
[KerasHub]({{< relref "/docs/keras_hub" >}})ëŠ” ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ìµœì‹  ëª¨ë¸ì„ ì œê³µí•˜ì—¬,
ë³„ë„ì˜ íŠ¸ë ˆì´ë‹ ì—†ì´ë„ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

KerasHubëŠ” ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì˜ ì „ì²´ ê°œë°œ ì£¼ê¸°ë¥¼ ì§€ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ,
ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸ê³¼ ëª¨ë“ˆí™”ëœ ë¹Œë”© ë¸”ë¡ì„ ëª¨ë‘ ì œê³µí•˜ì—¬,
ê°œë°œìê°€ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸ì„ ì¬ì‚¬ìš©í•˜ê±°ë‚˜ ìì‹ ë§Œì˜ LLMì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìš”ì•½í•˜ìë©´, KerasHubëŠ” ìƒì„±í˜• LLMì„ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- `generate()` ë©”ì„œë“œë¥¼ ì œê³µí•˜ëŠ” ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸,
  ì˜ˆ: [`keras_hub.models.GPT2CausalLM`]({{< relref "/docs/api/keras_hub/models/gpt2/gpt2_causal_lm#gpt2causallm-class" >}}) ë° [`keras_hub.models.OPTCausalLM`]({{< relref "/docs/api/keras_hub/models/opt/opt_causal_lm#optcausallm-class" >}}).
- í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ Top-K, ë¹” ì„œì¹˜, ëŒ€ì¡°ì  ì„œì¹˜ì™€ ê°™ì€ ìƒ˜í”Œë§ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ëŠ” `Sampler` í´ë˜ìŠ¤.
  ì´ ìƒ˜í”ŒëŸ¬ë“¤ì€ ì»¤ìŠ¤í…€ ëª¨ë¸ê³¼ í•¨ê»˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ GPT-2 ëª¨ë¸ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ìƒì„± {#load-a-pre-trained-gpt-2-model-and-generate-some-text}

KerasHubëŠ” [Google Bert](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) ë° [GPT-2](https://openai.com/research/better-language-models)ì™€ ê°™ì€ ì—¬ëŸ¬ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.
ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì€ [KerasHub ì €ì¥ì†Œ](https://github.com/keras-team/keras-hub/tree/master/keras_hub/models)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ì™€ ê°™ì´ GPT-2 ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ê²ƒì€ ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤:

```python
# íŠ¸ë ˆì´ë‹ê³¼ ìƒì„±ì„ ë” ë¹ ë¥´ê²Œ í•˜ê¸° ìœ„í•´ ì „ì²´ ê¸¸ì´ 1024 ëŒ€ì‹  ê¸¸ì´ 128ì˜ ì „ì²˜ë¦¬ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
preprocessor = keras_hub.models.GPT2CausalLMPreprocessor.from_preset(
    "gpt2_base_en",
    sequence_length=128,
)
gpt2_lm = keras_hub.models.GPT2CausalLM.from_preset(
    "gpt2_base_en", preprocessor=preprocessor
)
```

ëª¨ë¸ì´ ë¡œë“œë˜ë©´ ë°”ë¡œ í…ìŠ¤íŠ¸ ìƒì„±ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì§ì ‘ ì‹œë„í•´ë³´ì„¸ìš”.
ë‹¨ìˆœíˆ _generate()_ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

```python
start = time.time()

output = gpt2_lm.generate("My trip to Yosemite was", max_length=200)
print("\nGPT-2 output:")
print(output)

end = time.time()
print(f"TOTAL TIME ELAPSED: {end - start:.2f}s")
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
GPT-2 output:
My trip to Yosemite was pretty awesome. The first time I went I didn't know how to go and it was pretty hard to get around. It was a bit like going on an adventure with a friend. The only things I could do were hike and climb the mountain. It's really cool to know you're not alone in this world. It's a lot of fun. I'm a little worried that I might not get to the top of the mountain in time to see the sunrise and sunset of the day. I think the weather is going to get a little warmer in the coming years.
```

```plain
This post is a little more in-depth on how to go on the trail. It covers how to hike on the Sierra Nevada, how to hike with the Sierra Nevada, how to hike in the Sierra Nevada, how to get to the top of the mountain, and how to get to the top with your own gear.
```

```plain
The Sierra Nevada is a very popular trail in Yosemite
TOTAL TIME ELAPSED: 25.36s
```

{{% /details %}}

ë˜ë‹¤ë¥¸ ì˜ˆì œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”:

```python
start = time.time()

output = gpt2_lm.generate("That Italian restaurant is", max_length=200)
print("\nGPT-2 output:")
print(output)

end = time.time()
print(f"TOTAL TIME ELAPSED: {end - start:.2f}s")
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
GPT-2 output:
That Italian restaurant is known for its delicious food, and the best part is that it has a full bar, with seating for a whole host of guests. And that's only because it's located at the heart of the neighborhood.
```

```plain
The menu at the Italian restaurant is pretty straightforward:
```

```plain
The menu consists of three main dishes:
```

```plain
Italian sausage
```

```plain
Bolognese
```

```plain
Sausage
```

```plain
Bolognese with cheese
```

```plain
Sauce with cream
```

```plain
Italian sausage with cheese
```

```plain
Bolognese with cheese
```

```plain
And the main menu consists of a few other things.
```

```plain
There are two tables: the one that serves a menu of sausage and bolognese with cheese (the one that serves the menu of sausage and bolognese with cheese) and the one that serves the menu of sausage and bolognese with cheese. The two tables are also open 24 hours a day, 7 days a week.
```

```plain
TOTAL TIME ELAPSED: 1.55s
```

{{% /details %}}

ë‘ ë²ˆì§¸ í˜¸ì¶œì´ í›¨ì”¬ ë¹ ë¥¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ëŠ” ì²« ë²ˆì§¸ ì‹¤í–‰ì—ì„œ ê³„ì‚° ê·¸ë˜í”„ê°€ [XLA ì»´íŒŒì¼](https://www.tensorflow.org/xla)ë˜ì—ˆê³ ,
ê·¸ ì´í›„ë¡œëŠ” ë°±ê·¸ë¼ìš´ë“œ(behind the scenes)ì—ì„œ ì¬ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì´ ê´œì°®ì•„ ë³´ì´ì§€ë§Œ, ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ íŒŒì¸ íŠœë‹ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## KerasHubì˜ GPT-2 ëª¨ë¸ì— ëŒ€í•´ ë” ì•Œì•„ë³´ê¸° {#more-on-the-gpt-2-model-from-kerashub}

ì´ì œ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ ì‹¤ì œë¡œ íŒŒì¸ íŠœë‹ì„ ì§„í–‰í•  ì˜ˆì •ì´ì§€ë§Œ,
ê·¸ ì „ì— GPT-2ì™€ í•¨ê»˜ ì‘ì—…í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë“¤ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

GPT-2 ì½”ë“œ ì „ì²´ëŠ” [ì—¬ê¸°](https://github.com/keras-team/keras-hub/blob/master/keras_hub/models/gpt2/)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ê°œë…ì ìœ¼ë¡œ `GPT2CausalLM`ì€ KerasHubì˜ ì—¬ëŸ¬ ëª¨ë“ˆë¡œ ê³„ì¸µì ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë©°,
ëª¨ë‘ _from_preset()_ í•¨ìˆ˜ë¥¼ í†µí•´ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- `keras_hub.models.GPT2Tokenizer`: GPT-2 ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” í† í¬ë‚˜ì´ì €ë¡œ, [byte-pair encoder](https://huggingface.co/course/chapter6/5?fw=pt)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- [`keras_hub.models.GPT2CausalLMPreprocessor`]({{< relref "/docs/api/keras_hub/models/gpt2/gpt2_causal_lm_preprocessor#gpt2causallmpreprocessor-class" >}}): GPT-2 Causal LM íŠ¸ë ˆì´ë‹ì— ì‚¬ìš©ë˜ëŠ” ì „ì²˜ë¦¬ê¸°ì…ë‹ˆë‹¤. í† í¬ë‚˜ì´ì§•ì„ ë¹„ë¡¯í•´ ë ˆì´ë¸” ìƒì„± ë° ì¢…ë£Œ í† í° ì¶”ê°€ì™€ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- [`keras_hub.models.GPT2Backbone`]({{< relref "/docs/api/keras_hub/models/gpt2/gpt2_backbone#gpt2backbone-class" >}}): GPT-2 ëª¨ë¸ë¡œ, [`keras_hub.layers.TransformerDecoder`]({{< relref "/docs/api/keras_hub/modeling_layers/transformer_decoder#transformerdecoder-class" >}})ì˜ ìŠ¤íƒì…ë‹ˆë‹¤. ì´ëŠ” ë³´í†µ `GPT-2`ë¡œ ë¶ˆë¦½ë‹ˆë‹¤.
- [`keras_hub.models.GPT2CausalLM`]({{< relref "/docs/api/keras_hub/models/gpt2/gpt2_causal_lm#gpt2causallm-class" >}}): `GPT2Backbone`ì„ ê°ì‹¸ë©°, `GPT2Backbone`ì˜ ì¶œë ¥ì„ ì„ë² ë”© í–‰ë ¬ê³¼ ê³±í•˜ì—¬ ì–´íœ˜ í† í°ì— ëŒ€í•œ ë¡œê·¸ í™•ë¥ ì„ ìƒì„±í•©ë‹ˆë‹¤.

## Reddit ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸ íŠœë‹í•˜ê¸° {#finetune-on-reddit-dataset}

ì´ì œ KerasHubì˜ GPT-2 ëª¨ë¸ì— ëŒ€í•œ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ,
ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ì—¬ íŠ¹ì • ìŠ¤íƒ€ì¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ì§§ê±°ë‚˜ ê¸´, ì—„ê²©í•˜ê±°ë‚˜ ìºì£¼ì–¼í•œ ìŠ¤íƒ€ì¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Reddit ë°ì´í„°ì…‹ì„ ì˜ˆì‹œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
import tensorflow_datasets as tfds

reddit_ds = tfds.load("reddit_tifu", split="train", as_supervised=True)
```

Reddit TensorFlow Datasetì˜ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ë‘ ê°€ì§€ íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤:

- **document**: ê²Œì‹œë¬¼ì˜ í…ìŠ¤íŠ¸.
- **title**: ì œëª©.

```python
for document, title in reddit_ds:
    print(document.numpy())
    print(title.numpy())
    break
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
b"me and a friend decided to go to the beach last sunday. we loaded up and headed out. we were about half way there when i decided that i was not leaving till i had seafood. \n\nnow i'm not talking about red lobster. no friends i'm talking about a low country boil. i found the restaurant and got directions. i don't know if any of you have heard about the crab shack on tybee island but let me tell you it's worth it. \n\nwe arrived and was seated quickly. we decided to get a seafood sampler for two and split it. the waitress bought it out on separate platters for us. the amount of food was staggering. two types of crab, shrimp, mussels, crawfish, andouille sausage, red potatoes, and corn on the cob. i managed to finish it and some of my friends crawfish and mussels. it was a day to be a fat ass. we finished paid for our food and headed to the beach. \n\nfunny thing about seafood. it runs through me faster than a kenyan \n\nwe arrived and walked around a bit. it was about 45min since we arrived at the beach when i felt a rumble from the depths of my stomach. i ignored it i didn't want my stomach to ruin our fun. i pushed down the feeling and continued. about 15min later the feeling was back and stronger than before. again i ignored it and continued. 5min later it felt like a nuclear reactor had just exploded in my stomach. i started running. i yelled to my friend to hurry the fuck up. \n\nrunning in sand is extremely hard if you did not know this. we got in his car and i yelled at him to floor it. my stomach was screaming and if he didn't hurry i was gonna have this baby in his car and it wasn't gonna be pretty. after a few red lights and me screaming like a woman in labor we made it to the store. \n\ni practically tore his car door open and ran inside. i ran to the bathroom opened the door and barely got my pants down before the dam burst and a flood of shit poured from my ass. \n\ni finished up when i felt something wet on my ass. i rubbed it thinking it was back splash. no, mass was covered in the after math of me abusing the toilet. i grabbed all the paper towels i could and gave my self a whores bath right there. \n\ni sprayed the bathroom down with the air freshener and left. an elderly lady walked in quickly and closed the door. i was just about to walk away when i heard gag. instead of walking i ran. i got to the car and told him to get the hell out of there."
b'liking seafood'
```

{{% /details %}}

ì´ ê²½ìš°, ì–¸ì–´ ëª¨ë¸ì—ì„œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ìˆìœ¼ë¯€ë¡œ, 'document' í”¼ì²˜ë§Œ í•„ìš”í•©ë‹ˆë‹¤.

```python
train_ds = (
    reddit_ds.map(lambda document, _: document)
    .batch(32)
    .cache()
    .prefetch(tf.data.AUTOTUNE)
)
```

ì´ì œ ìµìˆ™í•œ _fit()_ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
`GPT2CausalLM`ì´ [`keras_hub.models.Task`]({{< relref "/docs/api/keras_hub/base_classes/task#task-class" >}}) ì¸ìŠ¤í„´ìŠ¤ì´ê¸° ë•Œë¬¸ì—,
`fit` ë©”ì„œë“œ ë‚´ì—ì„œ `preprocessor`ê°€ ìë™ìœ¼ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤.

ì´ ë‹¨ê³„ëŠ” GPU ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ë©°, ì „ì²´ íŠ¸ë ˆì´ë‹ì„ ì™„ë£Œí•˜ë ¤ë©´ ì‹œê°„ì´ ê½¤ ê±¸ë¦½ë‹ˆë‹¤.
ì—¬ê¸°ì„œëŠ” ë°ëª¨ ëª©ì ìœ¼ë¡œ ë°ì´í„°ì…‹ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
train_ds = train_ds.take(500)
num_epochs = 1

# ì„ í˜•ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” í•™ìŠµë¥ .
learning_rate = keras.optimizers.schedules.PolynomialDecay(
    5e-5,
    decay_steps=train_ds.cardinality() * num_epochs,
    end_learning_rate=0.0,
)
loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
gpt2_lm.compile(
    optimizer=keras.optimizers.Adam(learning_rate),
    loss=loss,
    weighted_metrics=["accuracy"],
)

gpt2_lm.fit(train_ds, epochs=num_epochs)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
 500/500 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 75s 120ms/step - accuracy: 0.3189 - loss: 3.3653

<keras.src.callbacks.history.History at 0x7f2af3fda410>
```

{{% /details %}}

íŒŒì¸ íŠœë‹ì´ ì™„ë£Œëœ í›„ì—ëŠ”, ë™ì¼í•œ _generate()_ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë²ˆì—ëŠ” í…ìŠ¤íŠ¸ê°€ Reddit ì‘ì„± ìŠ¤íƒ€ì¼ì— ë” ê°€ê¹Œì›Œì§€ê³ , ìƒì„±ë˜ëŠ” ê¸¸ì´ë„ íŠ¸ë ˆì´ë‹ ì„¸íŠ¸ì—ì„œ ì„¤ì •í•œ ê¸¸ì´ì— ê°€ê¹ê²Œ ë©ë‹ˆë‹¤.

```python
start = time.time()

output = gpt2_lm.generate("I like basketball", max_length=200)
print("\nGPT-2 output:")
print(output)

end = time.time()
print(f"TOTAL TIME ELAPSED: {end - start:.2f}s")
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
GPT-2 output:
I like basketball. it has the greatest shot of all time and the best shot of all time. i have to play a little bit more and get some practice time.
```

```plain
today i got the opportunity to play in a tournament in a city that is very close to my school so i was excited to see how it would go. i had just been playing with a few other guys, so i thought i would go and play a couple games with them.
```

```plain
after a few games i was pretty confident and confident in myself. i had just gotten the opportunity and had to get some practice time.
```

```plain
so i go to the
TOTAL TIME ELAPSED: 21.13s
```

{{% /details %}}

## ìƒ˜í”Œë§ ë°©ë²•ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° {#into-the-sampling-method}

KerasHubì—ì„œëŠ” contrastive search, Top-K, beam sampling ë“±ì˜ ëª‡ ê°€ì§€ ìƒ˜í”Œë§ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.
ê¸°ë³¸ì ìœ¼ë¡œ `GPT2CausalLM`ì€ Top-K ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ìƒ˜í”Œë§ ë°©ë²•ì„ ì„ íƒí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

Optimizerì™€ activation í•¨ìˆ˜ì²˜ëŸ¼, ì»¤ìŠ¤í…€ ìƒ˜í”ŒëŸ¬ë¥¼ ì§€ì •í•˜ëŠ” ë°©ë²•ì—ëŠ” ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤:

- ë¬¸ìì—´ ì‹ë³„ìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "greedy"ì™€ ê°™ì´ í•˜ë©´ ê¸°ë³¸ êµ¬ì„±ì„ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.
- [`keras_hub.samplers.Sampler`]({{< relref "/docs/api/keras_hub/samplers/samplers#sampler-class" >}}) ì¸ìŠ¤í„´ìŠ¤ ì „ë‹¬ì„ í†µí•´,
  ì»¤ìŠ¤í…€ êµ¬ì„±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ë¬¸ìì—´ ì‹ë³„ìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
gpt2_lm.compile(sampler="top_k")
output = gpt2_lm.generate("I like basketball", max_length=200)
print("\nGPT-2 output:")
print(output)

# `Sampler` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
# `GreedySampler`ëŠ” ìŠ¤ìŠ¤ë¡œ ë°˜ë³µë˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
greedy_sampler = keras_hub.samplers.GreedySampler()
gpt2_lm.compile(sampler=greedy_sampler)

output = gpt2_lm.generate("I like basketball", max_length=200)
print("\nGPT-2 output:")
print(output)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
GPT-2 output:
I like basketball, and this is a pretty good one.
```

```plain
first off, my wife is pretty good, she is a very good basketball player and she is really, really good at playing basketball.
```

```plain
she has an amazing game called basketball, it is a pretty fun game.
```

```plain
i play it on the couch.  i'm sitting there, watching the game on the couch.  my wife is playing with her phone.  she's playing on the phone with a bunch of people.
```

```plain
my wife is sitting there and watching basketball.  she's sitting there watching
```

```plain
GPT-2 output:
I like basketball, but i don't like to play it.
```

```plain
so i was playing basketball at my local high school, and i was playing with my friends.
```

```plain
i was playing with my friends, and i was playing with my brother, who was playing basketball with his brother.
```

```plain
so i was playing with my brother, and he was playing with his brother's brother.
```

```plain
so i was playing with my brother, and he was playing with his brother's brother.
```

```plain
so i was playing with my brother, and he was playing with his brother's brother.
```

```plain
so i was playing with my brother, and he was playing with his brother's brother.
```

```plain
so i was playing with my brother, and he was playing with his brother's brother.
```

```plain
so i was playing with my brother, and he was playing with his brother
```

{{% /details %}}

KerasHub `Sampler` í´ë˜ìŠ¤ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€,
[ì—¬ê¸°](https://github.com/keras-team/keras-hub/tree/master/keras_hub/samplers)ì—ì„œ ì½”ë“œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì¤‘êµ­ ì‹œ ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸ íŠœë‹í•˜ê¸° {#finetune-on-chinese-poem-dataset}

GPT-2ë¥¼ ë¹„ì˜ì–´ê¶Œ ë°ì´í„°ì…‹ì—ì„œë„ íŒŒì¸ íŠœë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì¤‘êµ­ì–´ë¥¼ ì•„ëŠ” ë…ìë“¤ì„ ìœ„í•´, ì´ ì„¹ì…˜ì—ì„œëŠ” GPT-2ë¥¼ ì¤‘êµ­ ì‹œ ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸ íŠœë‹í•˜ì—¬ ëª¨ë¸ì„ ì‹œì¸ìœ¼ë¡œ ë§Œë“œëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤!

GPT-2ëŠ” byte-pair encoderë¥¼ ì‚¬ìš©í•˜ë©°,
ì›ë˜ì˜ ì‚¬ì „ íŠ¸ë ˆì´ë‹ ë°ì´í„°ì…‹ì—ëŠ” ì¼ë¶€ ì¤‘êµ­ì–´ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—,
ì›ë˜ì˜ vocabì„ ì‚¬ìš©í•˜ì—¬ ì¤‘êµ­ì–´ ë°ì´í„°ì…‹ì—ì„œ íŒŒì¸ íŠœë‹ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
!# ì¤‘êµ­ ì‹œ ë°ì´í„°ì…‹ ë¡œë“œ
!git clone https://github.com/chinese-poetry/chinese-poetry.git
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Cloning into 'chinese-poetry'...
```

{{% /details %}}

JSON íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ë°ëª¨ ëª©ì ì„ ìœ„í•´ ã€Šå…¨å”è¯—ã€‹ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
import os
import json

poem_collection = []
for file in os.listdir("chinese-poetry/å…¨å”è¯—"):
    if ".json" not in file or "poet" not in file:
        continue
    full_filename = "%s/%s" % ("chinese-poetry/å…¨å”è¯—", file)
    with open(full_filename, "r") as f:
        content = json.load(f)
        poem_collection.extend(content)

paragraphs = ["".join(data["paragraphs"]) for data in poem_collection]
```

ìƒ˜í”Œ ë°ì´í„°ë¥¼ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```python
print(paragraphs[0])
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
æ¯‹è¬‚æ”¯å±±éšªï¼Œæ­¤å±±èƒ½å¹¾ä½•ã€‚å´å¶”åå¹´å¤¢ï¼ŒçŸ¥æ­·å¹¾è¹‰è·ã€‚
```

{{% /details %}}

Reddit ì˜ˆì œì™€ ìœ ì‚¬í•˜ê²Œ, TF ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜í•œ í›„, ì¼ë¶€ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ íŠ¸ë ˆì´ë‹í•©ë‹ˆë‹¤.

```python
train_ds = (
    tf.data.Dataset.from_tensor_slices(paragraphs)
    .batch(16)
    .cache()
    .prefetch(tf.data.AUTOTUNE)
)

# ì „ì²´ ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•˜ëŠ” ë° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ,
# ë°ëª¨ ëª©ì ì„ ìœ„í•´ `500`ê°œì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê³ , 1ë²ˆì˜ ì—í¬í¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
train_ds = train_ds.take(500)
num_epochs = 1

learning_rate = keras.optimizers.schedules.PolynomialDecay(
    5e-4,
    decay_steps=train_ds.cardinality() * num_epochs,
    end_learning_rate=0.0,
)
loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
gpt2_lm.compile(
    optimizer=keras.optimizers.Adam(learning_rate),
    loss=loss,
    weighted_metrics=["accuracy"],
)

gpt2_lm.fit(train_ds, epochs=num_epochs)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
 500/500 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 49s 71ms/step - accuracy: 0.2357 - loss: 2.8196

<keras.src.callbacks.history.History at 0x7f2b2c192bc0>
```

{{% /details %}}

ê²°ê³¼ë¥¼ í™•ì¸í•´ ë´…ì‹œë‹¤!

```python
output = gpt2_lm.generate("æ˜¨å¤œé›¨ç–é£éª¤", max_length=200)
print(output)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
æ˜¨å¤œé›¨ç–é£éª¤ï¼Œçˆ²è‡¨æ±Ÿå±±é™¢çŸ­éœã€‚çŸ³æ·¡å±±é™µé•·çˆ²ç¾£ï¼Œè‡¨çŸ³å±±éè™•è‡¨ç¾£ã€‚ç¾é™ªæ²³åŸƒè²çˆ²ç¾£ï¼Œæ¼æ¼æ¼é‚Šé™µå¡˜
```

{{% /details %}}

ë‚˜ì˜ì§€ ì•Šë„¤ìš” ğŸ˜€
