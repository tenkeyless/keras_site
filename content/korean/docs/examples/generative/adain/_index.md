---
title: AdaINì„ ì‚¬ìš©í•œ ì‹ ê²½ ìŠ¤íƒ€ì¼ ì „ì†¡
linkTitle: AdaIN ì‹ ê²½ ìŠ¤íƒ€ì¼ ì „ì†¡
toc: true
weight: 19
type: docs
math: true
---

{{< keras/original checkedAt="2024-11-23" >}}

**{{< t f_author >}}** [Aritra Roy Gosthipaty](https://twitter.com/arig23498), [Ritwik Raha](https://twitter.com/ritwik_raha)  
**{{< t f_date_created >}}** 2021/11/08  
**{{< t f_last_modified >}}** 2021/11/08  
**{{< t f_description >}}** Adaptive Instance Normalizationì„ ì‚¬ìš©í•œ ì‹ ê²½ ìŠ¤íƒ€ì¼ ì „ì†¡

{{< keras/version v=2 >}}

{{< cards cols="2" >}}
{{< card link="https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/adain.ipynb" title="Colab" tag="Colab" tagType="warning">}}
{{< card link="https://github.com/keras-team/keras-io/blob/master/examples/generative/adain.py" title="GitHub" tag="GitHub">}}
{{< /cards >}}

## ì†Œê°œ {#introduction}

[Neural Style Transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)ì€
í•˜ë‚˜ì˜ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ì„ ë‹¤ë¥¸ ì´ë¯¸ì§€ì˜ ì½˜í…ì¸ ì— ì ìš©í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
ì´ëŠ” Gatys et al.ì´ ë°œí‘œí•œ ê¸°ë…ë¹„ì ì¸ ë…¼ë¬¸
["A Neural Algorithm of Artistic Style"](https://arxiv.org/abs/1508.06576)ì—ì„œ ì²˜ìŒ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤.
ì´ ë°©ë²•ì˜ ì£¼ìš” í•œê³„ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ëŠë¦° ë°˜ë³µ ìµœì í™” ê³¼ì •(slow iterative optimization process)ì„ ì‚¬ìš©í•œë‹¤ëŠ” ì ì—ì„œ,
ì‹¤í–‰ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´í›„ [Batch Normalization](https://arxiv.org/abs/1502.03167),
[Instance Normalization](https://arxiv.org/abs/1701.02096),
[Conditional Instance Normalization](https://arxiv.org/abs/1610.07629) ë“±ì˜ ë…¼ë¬¸ë“¤ì´,
Neural Style Transferë¥¼ ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í–ˆìœ¼ë©°,
ë” ì´ìƒ ëŠë¦° ë°˜ë³µ ê³¼ì •ì„ ìš”êµ¬í•˜ì§€ ì•Šê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

ì´ ë…¼ë¬¸ë“¤ì„ ì´ì–´, Xun Huangê³¼ Serge BelongieëŠ”
[Adaptive Instance Normalization](https://arxiv.org/abs/1703.06868) (AdaIN)ì„ ì œì•ˆí•˜ì˜€ìœ¼ë©°,
ì´ë¥¼ í†µí•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì„ì˜ì˜ ìŠ¤íƒ€ì¼ì„ ì „ì†¡í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

ì´ ì˜ˆì‹œì—ì„œëŠ” Neural Style Transferë¥¼ ìœ„í•œ Adaptive Instance Normalizationì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ì•„ë˜ ê·¸ë¦¼ì€ ë‹¨ **30 ì—í¬í¬** ë™ì•ˆ íŠ¸ë ˆì´ë‹ëœ AdaIN ëª¨ë¸ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

![Style transfer sample gallery](/images/examples/generative/adain/zDjDuea.png)

ë˜í•œ ì´ [Hugging Face ë°ëª¨](https://huggingface.co/spaces/ariG23498/nst)ë¥¼ í†µí•´,
ìì‹ ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì²´í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì…‹ì—… {#setup}

í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ import í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
ë˜í•œ ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•´ ì‹œë“œë¥¼ ì„¤ì •í•˜ê³ ,
ì „ì—­ ë³€ìˆ˜ëŠ” ìš°ë¦¬ê°€ ì›í•˜ëŠ” ëŒ€ë¡œ ë³€ê²½í•  ìˆ˜ ìˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.

```python
import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
from tensorflow.keras import layers

# ì „ì—­ ë³€ìˆ˜ ì •ì˜.
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 64
# ì‹œê°„ ì œì•½ì„ ìœ„í•´ ë‹¨ì¼ ì—í¬í¬ë¡œ íŠ¸ë ˆì´ë‹.
# ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ë ¤ë©´, ìµœì†Œ 30 ì—í¬í¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
EPOCHS = 1
AUTOTUNE = tf.data.AUTOTUNE
```

## ìŠ¤íƒ€ì¼ ì „ì´ ìƒ˜í”Œ ê°¤ëŸ¬ë¦¬ {#style-transfer-sample-gallery}

Neural Style Transferì—ì„œëŠ” ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ì™€ ì½˜í…ì¸  ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.
ì´ ì˜ˆì‹œì—ì„œëŠ” [Best Artworks of All Time](https://www.kaggle.com/ikarus777/best-artworks-of-all-time)ì„ ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹ìœ¼ë¡œ,
[Pascal VOC](https://www.tensorflow.org/datasets/catalog/voc)ì„ ì½˜í…ì¸  ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì´ëŠ” ì›ë³¸ ë…¼ë¬¸ì˜ êµ¬í˜„ê³¼ ë‹¤ì†Œ ë‹¤ë¦…ë‹ˆë‹¤.
ì›ë³¸ ë…¼ë¬¸ì—ì„œëŠ” [WIKI-Art](https://paperswithcode.com/dataset/wikiart)ì„ ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹ìœ¼ë¡œ,
[MSCOCO](https://cocodataset.org/#home)ì„ ì½˜í…ì¸  ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
ì´ ì˜ˆì‹œì—ì„œëŠ” ìµœì†Œí•œì˜ ì˜ˆì‹œë¥¼ ë§Œë“¤ë©´ì„œë„, ì¬í˜„ ê°€ëŠ¥ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ì´ëŸ¬í•œ ë³€ê²½ì„ í–ˆìŠµë‹ˆë‹¤.

## Kaggleì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ {#downloading-the-dataset-from-kaggle}

[Best Artworks of All Time](https://www.kaggle.com/ikarus777/best-artworks-of-all-time) ë°ì´í„°ì…‹ì€
Kaggleì— í˜¸ìŠ¤íŒ…ë˜ì–´ ìˆìœ¼ë©°, Colabì—ì„œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ ì‰½ê²Œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- Kaggle API í‚¤ê°€ ì—†ëŠ” ê²½ìš°, [ì—¬ê¸°](https://github.com/Kaggle/kaggle-api)ì˜ ì§€ì¹¨ì„ ë”°ë¼ Kaggle API í‚¤ë¥¼ ì–»ìœ¼ì„¸ìš”.
- ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ Kaggle API í‚¤ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.

  ```python
  from google.colab import files
  files.upload()
  ```

- ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ API í‚¤ë¥¼ ì ì ˆí•œ ë””ë ‰í„°ë¦¬ë¡œ ì˜®ê¸°ê³  ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.

  ```console
  $ mkdir ~/.kaggle
  $ cp kaggle.json ~/.kaggle/
  $ chmod 600 ~/.kaggle/kaggle.json
  $ kaggle datasets download ikarus777/best-artworks-of-all-time
  $ unzip -qq best-artworks-of-all-time.zip
  $ rm -rf images
  $ mv resized artwork
  $ rm best-artworks-of-all-time.zip artists.csv
  ```

## [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) íŒŒì´í”„ë¼ì¸{#tfdatahttpswwwtensorfloworgapi_docspythontfdata-pipeline}

ì´ ì„¹ì…˜ì—ì„œëŠ”, í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹ì˜ ê²½ìš°, í´ë”ì—ì„œ ì´ë¯¸ì§€ë¥¼ ë””ì½”ë”©í•˜ê³  ë³€í™˜ ë° ë¦¬ì‚¬ì´ì¦ˆí•©ë‹ˆë‹¤.
ì½˜í…ì¸  ì´ë¯¸ì§€ëŠ” `tfds` ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬,
ì´ë¯¸ [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) ë°ì´í„°ì…‹ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.

ìŠ¤íƒ€ì¼ê³¼ ì½˜í…ì¸  ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì¤€ë¹„í•œ í›„,
ì´ ë‘˜ì„ zipí•˜ì—¬ ëª¨ë¸ì´ ì‚¬ìš©í•  ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ë§Œë“­ë‹ˆë‹¤.

```python
def decode_and_resize(image_path):
    """ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œì—ì„œ ì´ë¯¸ì§€ë¥¼ ë””ì½”ë”©í•˜ê³  ë¦¬ì‚¬ì´ì¦ˆí•©ë‹ˆë‹¤.

    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ.

    Returns:
        ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€.
    """
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.convert_image_dtype(image, dtype="float32")
    image = tf.image.resize(image, IMAGE_SIZE)
    return image


def extract_image_from_voc(element):
    """PascalVOC ë°ì´í„°ì…‹ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        element: ë°ì´í„° ë”•ì…”ë„ˆë¦¬.

    Returns:
        ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€.
    """
    image = element["image"]
    image = tf.image.convert_image_dtype(image, dtype="float32")
    image = tf.image.resize(image, IMAGE_SIZE)
    return image


# ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ì˜ íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
style_images = os.listdir("/content/artwork/resized")
style_images = [os.path.join("/content/artwork/resized", path) for path in style_images]

# ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ train, val, testë¡œ ë¶„ë¦¬
total_style_images = len(style_images)
train_style = style_images[: int(0.8 * total_style_images)]
val_style = style_images[int(0.8 * total_style_images) : int(0.9 * total_style_images)]
test_style = style_images[int(0.9 * total_style_images) :]

# ìŠ¤íƒ€ì¼ ë° ì½˜í…ì¸  tf.data ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
train_style_ds = (
    tf.data.Dataset.from_tensor_slices(train_style)
    .map(decode_and_resize, num_parallel_calls=AUTOTUNE)
    .repeat()
)
train_content_ds = tfds.load("voc", split="train").map(extract_image_from_voc).repeat()

val_style_ds = (
    tf.data.Dataset.from_tensor_slices(val_style)
    .map(decode_and_resize, num_parallel_calls=AUTOTUNE)
    .repeat()
)
val_content_ds = (
    tfds.load("voc", split="validation").map(extract_image_from_voc).repeat()
)

test_style_ds = (
    tf.data.Dataset.from_tensor_slices(test_style)
    .map(decode_and_resize, num_parallel_calls=AUTOTUNE)
    .repeat()
)
test_content_ds = (
    tfds.load("voc", split="test")
    .map(extract_image_from_voc, num_parallel_calls=AUTOTUNE)
    .repeat()
)

# ìŠ¤íƒ€ì¼ ë° ì½˜í…ì¸  ë°ì´í„°ì…‹ì„ zipí•©ë‹ˆë‹¤.
train_ds = (
    tf.data.Dataset.zip((train_style_ds, train_content_ds))
    .shuffle(BATCH_SIZE * 2)
    .batch(BATCH_SIZE)
    .prefetch(AUTOTUNE)
)

val_ds = (
    tf.data.Dataset.zip((val_style_ds, val_content_ds))
    .shuffle(BATCH_SIZE * 2)
    .batch(BATCH_SIZE)
    .prefetch(AUTOTUNE)
)

test_ds = (
    tf.data.Dataset.zip((test_style_ds, test_content_ds))
    .shuffle(BATCH_SIZE * 2)
    .batch(BATCH_SIZE)
    .prefetch(AUTOTUNE)
)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
[1mDownloading and preparing dataset voc/2007/4.0.0 (download: 868.85 MiB, generated: Unknown size, total: 868.85 MiB) to /root/tensorflow_datasets/voc/2007/4.0.0...[0m

Dl Completed...: 0 url [00:00, ? url/s]

Dl Size...: 0 MiB [00:00, ? MiB/s]

Extraction completed...: 0 file [00:00, ? file/s]
0 examples [00:00, ? examples/s]

Shuffling and writing examples to /root/tensorflow_datasets/voc/2007/4.0.0.incompleteP16YU5/voc-test.tfrecord

  0%|          | 0/4952 [00:00<?, ? examples/s]

0 examples [00:00, ? examples/s]

Shuffling and writing examples to /root/tensorflow_datasets/voc/2007/4.0.0.incompleteP16YU5/voc-train.tfrecord

  0%|          | 0/2501 [00:00<?, ? examples/s]

0 examples [00:00, ? examples/s]

Shuffling and writing examples to /root/tensorflow_datasets/voc/2007/4.0.0.incompleteP16YU5/voc-validation.tfrecord

  0%|          | 0/2510 [00:00<?, ? examples/s]

[1mDataset voc downloaded and prepared to /root/tensorflow_datasets/voc/2007/4.0.0. Subsequent calls will reuse this data.[0m
```

{{% /details %}}

## ë°ì´í„° ì‹œê°í™” {#visualizing-the-data}

íŠ¸ë ˆì´ë‹ ì „ì— ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
ìš°ë¦¬ì˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´,
ë°ì´í„°ì…‹ì—ì„œ 10ê°œì˜ ìƒ˜í”Œì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

```python
style, content = next(iter(train_ds))
fig, axes = plt.subplots(nrows=10, ncols=2, figsize=(5, 30))
[ax.axis("off") for ax in np.ravel(axes)]

for (axis, style_image, content_image) in zip(axes, style[0:10], content[0:10]):
    (ax_style, ax_content) = axis
    ax_style.imshow(style_image)
    ax_style.set_title("Style Image")

    ax_content.imshow(content_image)
    ax_content.set_title("Content Image")
```

![png](/images/examples/generative/adain/adain_8_0.png)

## ì•„í‚¤í…ì³ {#architecture}

ìŠ¤íƒ€ì¼ ì „ì´ ë„¤íŠ¸ì›Œí¬ëŠ” ì½˜í…ì¸  ì´ë¯¸ì§€ì™€ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„,
ìŠ¤íƒ€ì¼ì´ ì „ì´ëœ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
AdaINì˜ ì €ìë“¤ì€ ì´ë¥¼ ìœ„í•´ ê°„ë‹¨í•œ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

![AdaIN architecture](/images/examples/generative/adain/JbIfoyE.png)

ì½˜í…ì¸  ì´ë¯¸ì§€(`C`)ì™€ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€(`S`)ëŠ” ëª¨ë‘ ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ë¡œ ì…ë ¥ë©ë‹ˆë‹¤.
ì´ëŸ¬í•œ ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ì˜ ì¶œë ¥(íŠ¹ì„± ë§µ)ì€ AdaIN ë ˆì´ì–´ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
AdaIN ë ˆì´ì–´ëŠ” ê²°í•©ëœ íŠ¹ì„± ë§µì„ ê³„ì‚°í•˜ë©°,
ì´ íŠ¹ì„± ë§µì€ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ë””ì½”ë” ë„¤íŠ¸ì›Œí¬ì— ì „ë‹¬ë©ë‹ˆë‹¤.
ë””ì½”ë”ëŠ” ì‹ ê²½ë§ì„ í†µí•´ ìŠ¤íƒ€ì¼ì´ ì „ì´ëœ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

$$
t = AdaIn(f_c, f_s)
$$

$$
T = g(t)
$$

ìŠ¤íƒ€ì¼ íŠ¹ì„± ë§µ($f_s$)ê³¼ ì½˜í…ì¸  íŠ¹ì„± ë§µ($f_c$)ì€ AdaIN ë ˆì´ì–´ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
ì´ ë ˆì´ì–´ëŠ” ê²°í•©ëœ íŠ¹ì„± ë§µ $t$ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
í•¨ìˆ˜ $g$ëŠ” ë””ì½”ë”(ìƒì„±ê¸°) ë„¤íŠ¸ì›Œí¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

### ì¸ì½”ë” {#encoder}

ì¸ì½”ë”ëŠ” [ImageNet](https://www.image-net.org/)ì—ì„œ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ VGG19 ëª¨ë¸ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.
ìš°ë¦¬ëŠ” ëª¨ë¸ì„ `block4-conv1` ë ˆì´ì–´ì—ì„œ ì˜ë¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ì¶œë ¥ ë ˆì´ì–´ëŠ” ì €ìë“¤ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ëŒ€ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.

```python
def get_encoder():
    vgg19 = keras.applications.VGG19(
        include_top=False,
        weights="imagenet",
        input_shape=(*IMAGE_SIZE, 3),
    )
    vgg19.trainable = False
    mini_vgg19 = keras.Model(vgg19.input, vgg19.get_layer("block4_conv1").output)

    inputs = layers.Input([*IMAGE_SIZE, 3])
    mini_vgg19_out = mini_vgg19(inputs)
    return keras.Model(inputs, mini_vgg19_out, name="mini_vgg19")
```

### Adaptive Instance Normalization {#adaptive-instance-normalization}

AdaIN ë ˆì´ì–´ëŠ” ì½˜í…ì¸  ì´ë¯¸ì§€ì™€ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ì˜ íŠ¹ì„±ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
ì´ ë ˆì´ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ì •ì‹ìœ¼ë¡œ ì •ì˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$
AdaIn(x, y) = \sigma(y)(\frac{x-\mu(x)}{\sigma(x)})+\mu(y)
$$

ì—¬ê¸°ì„œ $\sigma$ëŠ” í•´ë‹¹ ë³€ìˆ˜ì— ëŒ€í•œ í‘œì¤€ í¸ì°¨, $\mu$ëŠ” í‰ê· ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
ìœ„ì˜ ë°©ì •ì‹ì—ì„œ ì½˜í…ì¸  íŠ¹ì„± ë§µ $f_c$ì˜ í‰ê· ê³¼ ë¶„ì‚°ì€ ìŠ¤íƒ€ì¼ íŠ¹ì„± ë§µ $f_s$ì˜ í‰ê· ê³¼ ë¶„ì‚°ì— ë§ì¶°ì§‘ë‹ˆë‹¤.

AdaIN ë ˆì´ì–´ëŠ” ì €ìë“¤ì´ ì œì•ˆí•œ ë°”ì— ë”°ë¼ í‰ê· ê³¼ ë¶„ì‚° ì´ì™¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ë˜í•œ, ì´ ë ˆì´ì–´ëŠ” íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì´ëŸ¬í•œ ì´ìœ ë¡œ ìš°ë¦¬ëŠ” _Keras ë ˆì´ì–´_ ëŒ€ì‹  _Python í•¨ìˆ˜_ ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ì´ í•¨ìˆ˜ëŠ” ìŠ¤íƒ€ì¼ê³¼ ì½˜í…ì¸  íŠ¹ì„± ë§µì„ ë°›ì•„ì„œ ì´ë¯¸ì§€ì˜ í‰ê· ê³¼ í‘œì¤€ í¸ì°¨ë¥¼ ê³„ì‚°í•˜ê³ ,
adaptive instance normalized íŠ¹ì„± ë§µì„ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
def get_mean_std(x, epsilon=1e-5):
    axes = [1, 2]

    # í…ì„œì˜ í‰ê· ê³¼ í‘œì¤€ í¸ì°¨ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    mean, variance = tf.nn.moments(x, axes=axes, keepdims=True)
    standard_deviation = tf.sqrt(variance + epsilon)
    return mean, standard_deviation


def ada_in(style, content):
    """AdaIN íŠ¹ì„± ë§µì„ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        style: ìŠ¤íƒ€ì¼ íŠ¹ì„± ë§µ.
        content: ì½˜í…ì¸  íŠ¹ì„± ë§µ.

    Returns:
        AdaIN íŠ¹ì„± ë§µ.
    """
    content_mean, content_std = get_mean_std(content)
    style_mean, style_std = get_mean_std(style)
    t = style_std * (content - content_mean) / content_std + style_mean
    return t
```

### ë””ì½”ë” {#decoder}

ì €ìë“¤ì€ ë””ì½”ë” ë„¤íŠ¸ì›Œí¬ê°€ ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ì™€ ëŒ€ì¹­ì ìœ¼ë¡œ ë°˜ì „ë˜ì–´ì•¼ í•œë‹¤ê³  ëª…ì‹œí–ˆìŠµë‹ˆë‹¤.
ìš°ë¦¬ëŠ” ì¸ì½”ë”ë¥¼ ëŒ€ì¹­ì ìœ¼ë¡œ ë°˜ì „ì‹œì¼œ ë””ì½”ë”ë¥¼ êµ¬ì¶•í–ˆìœ¼ë©°,
íŠ¹ì„± ë§µì˜ ê³µê°„ í•´ìƒë„ë¥¼ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•´ `UpSampling2D` ë ˆì´ì–´ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

ì €ìë“¤ì€ ë””ì½”ë” ë„¤íŠ¸ì›Œí¬ì— ì–´ë–¤ ì •ê·œí™” ë ˆì´ì–´ë„ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ê²½ê³ í•˜ê³  ìˆìœ¼ë©°,
ì‹¤ì œë¡œ ë°°ì¹˜ ì •ê·œí™”ë‚˜ ì¸ìŠ¤í„´ìŠ¤ ì •ê·œí™”ë¥¼ í¬í•¨í•˜ë©´ ì „ì²´ ë„¤íŠ¸ì›Œí¬ì˜ ì„±ëŠ¥ì´ ì €í•˜ëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

ì´ ë¶€ë¶„ì€ ì „ì²´ ì•„í‚¤í…ì²˜ ì¤‘ì—ì„œ ìœ ì¼í•˜ê²Œ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ë¶€ë¶„ì…ë‹ˆë‹¤.

```python
def get_decoder():
    config = {"kernel_size": 3, "strides": 1, "padding": "same", "activation": "relu"}
    decoder = keras.Sequential(
        [
            layers.InputLayer((None, None, 512)),
            layers.Conv2D(filters=512, **config),
            layers.UpSampling2D(),
            layers.Conv2D(filters=256, **config),
            layers.Conv2D(filters=256, **config),
            layers.Conv2D(filters=256, **config),
            layers.Conv2D(filters=256, **config),
            layers.UpSampling2D(),
            layers.Conv2D(filters=128, **config),
            layers.Conv2D(filters=128, **config),
            layers.UpSampling2D(),
            layers.Conv2D(filters=64, **config),
            layers.Conv2D(
                filters=3,
                kernel_size=3,
                strides=1,
                padding="same",
                activation="sigmoid",
            ),
        ]
    )
    return decoder
```

### ì†ì‹¤ í•¨ìˆ˜ {#loss-functions}

ì—¬ê¸°ì—ì„œëŠ” ì‹ ê²½ ìŠ¤íƒ€ì¼ ì „ì´ ëª¨ë¸ì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
ì €ìë“¤ì€ ë„¤íŠ¸ì›Œí¬ì˜ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´, ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ VGG-19ë¥¼ ì‚¬ìš©í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤.
ì´ëŠ” ì˜¤ì§ ë””ì½”ë” ë„¤íŠ¸ì›Œí¬ë¥¼ íŠ¸ë ˆì´ë‹í•˜ëŠ” ë°ë§Œ ì‚¬ìš©ë  ê²ƒì´ë¼ëŠ” ì ì„ ê¸°ì–µí•´ì•¼ í•©ë‹ˆë‹¤.
ì´ ì†ì‹¤ (\mathcal{L}\_t)ì€ ì½˜í…ì¸  ì†ì‹¤ ($\mathcal{L}_c$)ê³¼ ìŠ¤íƒ€ì¼ ì†ì‹¤ ($\mathcal{L}_s$)ì˜ ê°€ì¤‘ ì¡°í•©ì…ë‹ˆë‹¤.
$\lambda$ í•­ì€ ìŠ¤íƒ€ì¼ ì „ì´ì˜ ì–‘ì„ ì¡°ì ˆí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

$$
\mathcal{L}_t = \mathcal{L}_c + \lambda \mathcal{L}_s
$$

### ì»¨í…ì¸  ì†ì‹¤ {#content-loss}

ì´ëŠ” ì½˜í…ì¸  ì´ë¯¸ì§€ íŠ¹ì„±ê³¼ ì‹ ê²½ ìŠ¤íƒ€ì¼ ì „ì´ ì´ë¯¸ì§€ íŠ¹ì„± ê°„ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ì…ë‹ˆë‹¤.

$$
\mathcal{L}_c = ||f(g(t))-t||_2
$$

ì—¬ê¸°ì„œ ì €ìë“¤ì€ ì›ë³¸ ì´ë¯¸ì§€ì˜ íŠ¹ì„±ì„ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ ,
AdaIn ë ˆì´ì–´ì˜ ì¶œë ¥ $t$ë¥¼ ì½˜í…ì¸  ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤.
ì´ëŠ” ìˆ˜ë ´ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.

### ìŠ¤íƒ€ì¼ ì†ì‹¤ {#style-loss}

ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” [Gram Matrix](https://mathworld.wolfram.com/GramMatrix.html)ë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ ,
ì €ìë“¤ì€ í†µê³„ì  íŠ¹ì„±(í‰ê·  ë° ë¶„ì‚°)ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•  ê²ƒì„ ì œì•ˆí•˜ë©°, ì´ëŠ” ê°œë…ì ìœ¼ë¡œ ë” ê¹”ë”í•©ë‹ˆë‹¤.
ë‹¤ìŒ ë°©ì •ì‹ì„ í†µí•´ ì‰½ê²Œ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

![png](/images/examples/generative/adain/Ctclhn3.png)

```latex
\mathcal{L}_s = \sum_{i=1}^{L} || \mu(\phi_i(g(t)))-\mu(\phi_i(s)) ||_2 + \sum_{i=1}^{L} ||  \sigma(\phi(g(t))) - \sigma(\phi_i(s))||_2
```

ì—¬ê¸°ì„œ `theta`ëŠ” VGG-19ì—ì„œ ì†ì‹¤ì„ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë ˆì´ì–´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
ì´ ê²½ìš° í•´ë‹¹ ë ˆì´ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- `block1_conv1`
- `block1_conv2`
- `block1_conv3`
- `block1_conv4`

```python
def get_loss_net():
    vgg19 = keras.applications.VGG19(
        include_top=False, weights="imagenet", input_shape=(*IMAGE_SIZE, 3)
    )
    vgg19.trainable = False
    layer_names = ["block1_conv1", "block2_conv1", "block3_conv1", "block4_conv1"]
    outputs = [vgg19.get_layer(name).output for name in layer_names]
    mini_vgg19 = keras.Model(vgg19.input, outputs)

    inputs = layers.Input([*IMAGE_SIZE, 3])
    mini_vgg19_out = mini_vgg19(inputs)
    return keras.Model(inputs, mini_vgg19_out, name="loss_net")
```

## ì‹ ê²½ ìŠ¤íƒ€ì¼ ì „ì´ {#neural-style-transfer}

ì´ê²ƒì€ íŠ¸ë ˆì´ë„ˆ ëª¨ë“ˆì…ë‹ˆë‹¤.
ìš°ë¦¬ëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) ì„œë¸Œí´ë˜ìŠ¤ ë‚´ì— ê°ìŒ‰ë‹ˆë‹¤.
ì´ë¥¼ í†µí•´ `model.fit()` ë£¨í”„ì—ì„œ ë°œìƒí•˜ëŠ” ì‘ì—…ì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
class NeuralStyleTransfer(tf.keras.Model):
    def __init__(self, encoder, decoder, loss_net, style_weight, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.loss_net = loss_net
        self.style_weight = style_weight

    def compile(self, optimizer, loss_fn):
        super().compile()
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.style_loss_tracker = keras.metrics.Mean(name="style_loss")
        self.content_loss_tracker = keras.metrics.Mean(name="content_loss")
        self.total_loss_tracker = keras.metrics.Mean(name="total_loss")

    def train_step(self, inputs):
        style, content = inputs

        # ì½˜í…ì¸ ì™€ ìŠ¤íƒ€ì¼ ì†ì‹¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        loss_content = 0.0
        loss_style = 0.0

        with tf.GradientTape() as tape:
            # ìŠ¤íƒ€ì¼ ë° ì½˜í…ì¸  ì´ë¯¸ì§€ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
            style_encoded = self.encoder(style)
            content_encoded = self.encoder(content)

            # AdaIN íƒ€ê²Ÿ íŠ¹ì„± ë§µì„ ê³„ì‚°í•©ë‹ˆë‹¤.
            t = ada_in(style=style_encoded, content=content_encoded)

            # ì‹ ê²½ ìŠ¤íƒ€ì¼ ì „ì´ëœ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            reconstructed_image = self.decoder(t)

            # ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
            reconstructed_vgg_features = self.loss_net(reconstructed_image)
            style_vgg_features = self.loss_net(style)
            loss_content = self.loss_fn(t, reconstructed_vgg_features[-1])
            for inp, out in zip(style_vgg_features, reconstructed_vgg_features):
                mean_inp, std_inp = get_mean_std(inp)
                mean_out, std_out = get_mean_std(out)
                loss_style += self.loss_fn(mean_inp, mean_out) + self.loss_fn(
                    std_inp, std_out
                )
            loss_style = self.style_weight * loss_style
            total_loss = loss_content + loss_style

        # ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ê³ , ë””ì½”ë”ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
        trainable_vars = self.decoder.trainable_variables
        gradients = tape.gradient(total_loss, trainable_vars)
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # ì¶”ì ê¸°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        self.style_loss_tracker.update_state(loss_style)
        self.content_loss_tracker.update_state(loss_content)
        self.total_loss_tracker.update_state(total_loss)
        return {
            "style_loss": self.style_loss_tracker.result(),
            "content_loss": self.content_loss_tracker.result(),
            "total_loss": self.total_loss_tracker.result(),
        }

    def test_step(self, inputs):
        style, content = inputs

        # ì½˜í…ì¸ ì™€ ìŠ¤íƒ€ì¼ ì†ì‹¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        loss_content = 0.0
        loss_style = 0.0

        # ìŠ¤íƒ€ì¼ ë° ì½˜í…ì¸  ì´ë¯¸ì§€ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
        style_encoded = self.encoder(style)
        content_encoded = self.encoder(content)

        # AdaIN íƒ€ê²Ÿ íŠ¹ì„± ë§µì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        t = ada_in(style=style_encoded, content=content_encoded)

        # ì‹ ê²½ ìŠ¤íƒ€ì¼ ì „ì´ëœ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        reconstructed_image = self.decoder(t)

        # ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        recons_vgg_features = self.loss_net(reconstructed_image)
        style_vgg_features = self.loss_net(style)
        loss_content = self.loss_fn(t, recons_vgg_features[-1])
        for inp, out in zip(style_vgg_features, recons_vgg_features):
            mean_inp, std_inp = get_mean_std(inp)
            mean_out, std_out = get_mean_std(out)
            loss_style += self.loss_fn(mean_inp, mean_out) + self.loss_fn(
                std_inp, std_out
            )
        loss_style = self.style_weight * loss_style
        total_loss = loss_content + loss_style

        # ì¶”ì ê¸°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        self.style_loss_tracker.update_state(loss_style)
        self.content_loss_tracker.update_state(loss_content)
        self.total_loss_tracker.update_state(total_loss)
        return {
            "style_loss": self.style_loss_tracker.result(),
            "content_loss": self.content_loss_tracker.result(),
            "total_loss": self.total_loss_tracker.result(),
        }

    @property
    def metrics(self):
        return [
            self.style_loss_tracker,
            self.content_loss_tracker,
            self.total_loss_tracker,
        ]
```

## íŠ¸ë ˆì´ë‹ ëª¨ë‹ˆí„° ì½œë°± {#train-monitor-callback}

ì´ ì½œë°±ì€ ê° epochê°€ ëë‚  ë•Œë§ˆë‹¤ ìŠ¤íƒ€ì¼ ì „ì´ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì‹œê°í™”í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
ìŠ¤íƒ€ì¼ ì „ì´ì˜ ëª©í‘œëŠ” ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•  ìˆ˜ ì—†ìœ¼ë©°, ì£¼ê´€ì ìœ¼ë¡œ í‰ê°€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
ì´ëŸ¬í•œ ì´ìœ ë¡œ, ì‹œê°í™”ëŠ” ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ì¤‘ìš”í•œ ì¸¡ë©´ì…ë‹ˆë‹¤.

```python
test_style, test_content = next(iter(test_ds))


class TrainMonitor(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        # ìŠ¤íƒ€ì¼ ë° ì½˜í…ì¸  ì´ë¯¸ì§€ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
        test_style_encoded = self.model.encoder(test_style)
        test_content_encoded = self.model.encoder(test_content)

        # AdaIN íŠ¹ì„± ë§µì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        test_t = ada_in(style=test_style_encoded, content=test_content_encoded)
        test_reconstructed_image = self.model.decoder(test_t)

        # ìŠ¤íƒ€ì¼, ì½˜í…ì¸  ë° NST ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))
        ax[0].imshow(tf.keras.utils.array_to_img(test_style[0]))
        ax[0].set_title(f"Style: {epoch:03d}")

        ax[1].imshow(tf.keras.utils.array_to_img(test_content[0]))
        ax[1].set_title(f"Content: {epoch:03d}")

        ax[2].imshow(
            tf.keras.utils.array_to_img(test_reconstructed_image[0])
        )
        ax[2].set_title(f"NST: {epoch:03d}")

        plt.show()
        plt.close()
```

## ëª¨ë¸ íŠ¸ë ˆì´ë‹ {#train-the-model}

ì´ ì„¹ì…˜ì—ì„œëŠ” ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜, ê·¸ë¦¬ê³  íŠ¸ë ˆì´ë„ˆ ëª¨ë“ˆì„ ì •ì˜í•©ë‹ˆë‹¤.
ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜ë¡œ íŠ¸ë ˆì´ë„ˆ ëª¨ë“ˆì„ ì»´íŒŒì¼í•œ í›„, íŠ¸ë ˆì´ë‹ì„ ì§„í–‰í•©ë‹ˆë‹¤.

**ì°¸ê³ **: ì‹œê°„ ì œí•œìœ¼ë¡œ ì¸í•´ ëª¨ë¸ì„ í•œ epoch ë™ì•ˆë§Œ íŠ¸ë ˆì´ë‹í•˜ì§€ë§Œ,
ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ë ¤ë©´ ìµœì†Œ 30 epoch ë™ì•ˆ íŠ¸ë ˆì´ë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.

```python
optimizer = keras.optimizers.Adam(learning_rate=1e-5)
loss_fn = keras.losses.MeanSquaredError()

encoder = get_encoder()
loss_net = get_loss_net()
decoder = get_decoder()

model = NeuralStyleTransfer(
    encoder=encoder, decoder=decoder, loss_net=loss_net, style_weight=4.0
)

model.compile(optimizer=optimizer, loss_fn=loss_fn)

history = model.fit(
    train_ds,
    epochs=EPOCHS,
    steps_per_epoch=50,
    validation_data=val_ds,
    validation_steps=50,
    callbacks=[TrainMonitor()],
)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5
80142336/80134624 [==============================] - 1s 0us/step
80150528/80134624 [==============================] - 1s 0us/step
50/50 [==============================] - ETA: 0s - style_loss: 213.1439 - content_loss: 141.1564 - total_loss: 354.3002
```

![png](/images/examples/generative/adain/adain_23_1.png)

```plain
50/50 [==============================] - 124s 2s/step - style_loss: 213.1439 - content_loss: 141.1564 - total_loss: 354.3002 - val_style_loss: 167.0819 - val_content_loss: 129.0497 - val_total_loss: 296.1316
```

{{% /details %}}

## ì¶”ë¡  {#inference}

ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•œ í›„, ì´ì œ ì„ì˜ì˜ ì½˜í…ì¸  ì´ë¯¸ì§€ì™€ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
ìƒì„±ëœ ì¶œë ¥ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•´ë´…ë‹ˆë‹¤.

**ì°¸ê³ **: ì´ ëª¨ë¸ì„ ì§ì ‘ ì‚¬ìš©í•´ë³´ê³  ì‹¶ë‹¤ë©´,
[Hugging Face ë°ëª¨](https://huggingface.co/spaces/ariG23498/nst)ë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
for style, content in test_ds.take(1):
    style_encoded = model.encoder(style)
    content_encoded = model.encoder(content)
    t = ada_in(style=style_encoded, content=content_encoded)
    reconstructed_image = model.decoder(t)
    fig, axes = plt.subplots(nrows=10, ncols=3, figsize=(10, 30))
    [ax.axis("off") for ax in np.ravel(axes)]

    for axis, style_image, content_image, reconstructed_image in zip(
        axes, style[0:10], content[0:10], reconstructed_image[0:10]
    ):
        (ax_style, ax_content, ax_reconstructed) = axis
        ax_style.imshow(style_image)
        ax_style.set_title("Style Image")
        ax_content.imshow(content_image)
        ax_content.set_title("Content Image")
        ax_reconstructed.imshow(reconstructed_image)
        ax_reconstructed.set_title("NST Image")
```

![png](/images/examples/generative/adain/adain_25_0.png)

## ê²°ë¡  {#conclusion}

Adaptive Instance Normalizationì€ ì‹¤ì‹œê°„ìœ¼ë¡œ ì„ì˜ì˜ ìŠ¤íƒ€ì¼ ì „ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
ë˜í•œ, ìŠ¤íƒ€ì¼ê³¼ ì½˜í…ì¸  ì´ë¯¸ì§€ì˜ í†µê³„ì  íŠ¹ì„±(í‰ê· ê³¼ í‘œì¤€ í¸ì°¨)ì„ ì •ë ¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œë§Œ ì´ë¥¼ ë‹¬ì„±í•œë‹¤ëŠ” ê²ƒì´ ì €ìë“¤ì˜ ì¤‘ìš”í•œ ì œì•ˆì…ë‹ˆë‹¤.

**ì°¸ê³ **: AdaINì€ ë˜í•œ [Style-GANs](https://arxiv.org/abs/1812.04948)ì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤.

## ì°¸ì¡° {#reference}

- [TF êµ¬í˜„](https://github.com/ftokarev/tf-adain)

## Acknowledgement {#acknowledgement}

[Luke Wood](https://lukewood.xyz)ì—ê²Œ ìƒì„¸í•œ ë¦¬ë·°ì— ëŒ€í•´ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.
