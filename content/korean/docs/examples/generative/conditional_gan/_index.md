---
title: Conditional GAN
linkTitle: Conditional GAN
toc: true
weight: 10
type: docs
---

{{< keras/original checkedAt="2024-11-23" >}}

**{{< t f_author >}}** [Sayak Paul](https://twitter.com/RisingSayak)  
**{{< t f_date_created >}}** 2021/07/13  
**{{< t f_last_modified >}}** 2024/01/02  
**{{< t f_description >}}** í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ì¡°ê±´ìœ¼ë¡œ GANì„ íŠ¸ë ˆì´ë‹í•˜ì—¬ ì†ìœ¼ë¡œ ì“´ ìˆ«ìë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

{{< keras/version v=3 >}}

{{< cards cols="2" >}}
{{< card link="https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/conditional_gan.ipynb" title="Colab" tag="Colab" tagType="warning">}}
{{< card link="https://github.com/keras-team/keras-io/blob/master/examples/generative/conditional_gan.py" title="GitHub" tag="GitHub">}}
{{< /cards >}}

ìƒì„±ì  ì ëŒ€ ì‹ ê²½ë§(GAN)ì„ ì‚¬ìš©í•˜ë©´ ëœë¤ ì…ë ¥ì—ì„œ ìƒˆë¡œìš´ ì´ë¯¸ì§€ ë°ì´í„°,
ë¹„ë””ì˜¤ ë°ì´í„° ë˜ëŠ” ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì¼ë°˜ì ìœ¼ë¡œ, ëœë¤ ì…ë ¥ì€ ì •ê·œ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•œ ë‹¤ìŒ,
ê·¸ëŸ´ë“¯í•œ ê²ƒ(ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤ ë“±)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì¼ë ¨ì˜ ë³€í™˜ì„ ê±°ì¹©ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜, ê°„ë‹¨í•œ [DCGAN](https://arxiv.org/abs/1511.06434)ì—ì„œëŠ” ìƒì„±í•˜ëŠ” ìƒ˜í”Œì˜ ëª¨ì–‘(ì˜ˆ: í´ë˜ìŠ¤)ì„ ì œì–´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, MNIST ì†ìœ¼ë¡œ ì“´ ìˆ«ìë¥¼ ìƒì„±í•˜ëŠ” GANì˜ ê²½ìš°,
ê°„ë‹¨í•œ DCGANì—ì„œëŠ” ìƒì„±í•˜ëŠ” ìˆ«ìì˜ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ìƒì„±í•˜ëŠ” ë‚´ìš©ì„ ì œì–´í•˜ë ¤ë©´,
(ì´ë¯¸ì§€ í´ë˜ìŠ¤ì™€ ê°™ì€) ì‹œë§¨í‹± ì…ë ¥ì— ë”°ë¼ GAN ì¶œë ¥ì„ _ì¡°ê±´í•´ì•¼_ í•©ë‹ˆë‹¤.

ì´ ì˜ˆì—ì„œëŠ”, ì£¼ì–´ì§„ í´ë˜ìŠ¤ì— ë”°ë¼ MNIST ì†ìœ¼ë¡œ ì“´ ìˆ«ìë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ”
**ì¡°ê±´ë¶€ GAN(Conditional GAN)** ì„ ë¹Œë“œí•©ë‹ˆë‹¤.
ì´ëŸ¬í•œ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ìœ ìš©í•œ ì‘ìš© í”„ë¡œê·¸ë¨ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- [ë¶ˆê· í˜• ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data)ë¥¼ ë‹¤ë£¨ê³  ìˆìœ¼ë©°,
  ë°ì´í„° ì„¸íŠ¸ë¥¼ ê· í˜• ì¡ê¸° ìœ„í•´ ì™œê³¡ëœ í´ë˜ìŠ¤ì— ëŒ€í•œ ë” ë§ì€ ì˜ˆë¥¼ ìˆ˜ì§‘í•˜ê³  ì‹¶ë‹¤ê³  ê°€ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤.
  ë°ì´í„° ìˆ˜ì§‘ì€ ê·¸ ìì²´ë¡œ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” í”„ë¡œì„¸ìŠ¤ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  ëŒ€ì‹ , ì¡°ê±´ë¶€ GANì„ íŠ¸ë ˆì´ë‹í•˜ì—¬,
  ê· í˜•ì´ í•„ìš”í•œ í´ë˜ìŠ¤ì— ëŒ€í•œ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ìƒì„±ìëŠ” ìƒì„±ëœ ìƒ˜í”Œì„ í´ë˜ìŠ¤ ë ˆì´ë¸”ê³¼ ì—°ê´€ì‹œí‚¤ëŠ” ë²•ì„ ë°°ìš°ë¯€ë¡œ,
  í•´ë‹¹ í‘œí˜„ì€ [ë‹¤ë¥¸ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…](https://arxiv.org/abs/1809.11096)ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ ì´ ì˜ˆì œë¥¼ ê°œë°œí•˜ëŠ” ë° ì‚¬ìš©ëœ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.

- [ì¡°ê±´ë¶€ ìƒì„±ì  ì ëŒ€ ì‹ ê²½ë§](https://arxiv.org/abs/1411.1784)
- [Courseraì˜ ì¡°ê±´ë¶€ ìƒì„±ì— ëŒ€í•œ ê°•ì˜](https://www.coursera.org/lecture/build-basic-generative-adversarial-networks-gans/conditional-generation-inputs-2OPrG)

GANì— ëŒ€í•œ ë³µìŠµì´ í•„ìš”í•œ ê²½ìš°,
[ì´ ë¦¬ì†ŒìŠ¤](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-12/r-3/232)ì˜ "ìƒì„±ì  ì ëŒ€ ì‹ ê²½ë§(Generative adversarial networks)" ì„¹ì…˜ì„ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ì˜ˆì œì—ëŠ” TensorFlow 2.5 ì´ìƒê³¼
ë‹¤ìŒ ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ ì„¤ì¹˜í•  ìˆ˜ ìˆëŠ” TensorFlow Docsê°€ í•„ìš”í•©ë‹ˆë‹¤.

```python
!pip install -q git+https://github.com/tensorflow/docs
```

## Imports {#imports}

```python
import keras

from keras import layers
from keras import ops
from tensorflow_docs.vis import embed
import tensorflow as tf
import numpy as np
import imageio
```

## ìƒìˆ˜ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° {#constants-and-hyperparameters}

```python
batch_size = 64
num_channels = 1
num_classes = 10
image_size = 28
latent_dim = 128
```

## MNIST ë°ì´í„° ì„¸íŠ¸ ë¡œë”© ë° ì „ì²˜ë¦¬ {#loading-the-mnist-dataset-and-preprocessing-it}

```python
# ìš°ë¦¬ëŠ” íŠ¸ë ˆì´ë‹ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ, ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì˜ˆë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
all_digits = np.concatenate([x_train, x_test])
all_labels = np.concatenate([y_train, y_test])

# í”½ì…€ ê°’ì„ [0, 1] ë²”ìœ„ë¡œ ì¡°ì •í•˜ê³ ,
# ì´ë¯¸ì§€ì— ì±„ë„ ì°¨ì›ì„ ì¶”ê°€í•˜ê³ ,
# ë ˆì´ë¸”ì„ ì›í•« ì¸ì½”ë”©í•©ë‹ˆë‹¤.
all_digits = all_digits.astype("float32") / 255.0
all_digits = np.reshape(all_digits, (-1, 28, 28, 1))
all_labels = keras.utils.to_categorical(all_labels, 10)

# tf.data.Datasetì„ ìƒì„±í•©ë‹ˆë‹¤.
dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))
dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)

print(f"Shape of training images: {all_digits.shape}")
print(f"Shape of training labels: {all_labels.shape}")
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
 11490434/11490434 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 0us/step
Shape of training images: (70000, 28, 28, 1)
Shape of training labels: (70000, 10)
```

{{% /details %}}

## ìƒì„±ìì™€ íŒë³„ìì˜ ì…ë ¥ ì±„ë„ ìˆ˜ ê³„ì‚° {#calculating-the-number-of-input-channel-for-the-generator-and-discriminator}

ì¼ë°˜ (ì¡°ê±´ì—†ëŠ”) GANì—ì„œ, ìš°ë¦¬ëŠ” ì •ê·œ ë¶„í¬ì—ì„œ (ì¼ë¶€ ê³ ì •ëœ ì°¨ì›ì˜) ë…¸ì´ì¦ˆë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
ìš°ë¦¬ì˜ ê²½ìš°, ìš°ë¦¬ëŠ” ë˜í•œ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
ìš°ë¦¬ëŠ” ìƒì„±ì(ë…¸ì´ì¦ˆ ì…ë ¥)ì˜ ì…ë ¥ ì±„ë„ê³¼ íŒë³„ì(ìƒì„±ëœ ì´ë¯¸ì§€ ì…ë ¥)ì— í´ë˜ìŠ¤ ìˆ˜ë¥¼ ì¶”ê°€í•´ì•¼ í•  ê²ƒì…ë‹ˆë‹¤.

```python
generator_in_channels = latent_dim + num_classes
discriminator_in_channels = num_channels + num_classes
print(generator_in_channels, discriminator_in_channels)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
138 11
```

{{% /details %}}

## íŒë³„ìì™€ ìƒì„±ì ìƒì„±í•˜ê¸° {#creating-the-discriminator-and-generator}

ëª¨ë¸ ì •ì˜(`discriminator`, `generator`, `ConditionalGAN`)ëŠ”
[ì´ ì˜ˆì œ]({{< relref "/docs/guides/custom_train_step_in_tensorflow" >}})ì—ì„œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.

```python
# íŒë³„ìë¥¼ ë§Œë“­ë‹ˆë‹¤.
discriminator = keras.Sequential(
    [
        keras.layers.InputLayer((28, 28, discriminator_in_channels)),
        layers.Conv2D(64, (3, 3), strides=(2, 2), padding="same"),
        layers.LeakyReLU(negative_slope=0.2),
        layers.Conv2D(128, (3, 3), strides=(2, 2), padding="same"),
        layers.LeakyReLU(negative_slope=0.2),
        layers.GlobalMaxPooling2D(),
        layers.Dense(1),
    ],
    name="discriminator",
)

# ìƒì„±ìë¥¼ ë§Œë“­ë‹ˆë‹¤.
generator = keras.Sequential(
    [
        keras.layers.InputLayer((generator_in_channels,)),
        # 128 + num_classes ê°œì˜ ê³„ìˆ˜ë¥¼ ìƒì„±í•˜ì—¬,
        # 7x7x(128 + num_classes) ë§µìœ¼ë¡œ reshapeí•˜ë ¤ê³  í•©ë‹ˆë‹¤.
        layers.Dense(7 * 7 * generator_in_channels),
        layers.LeakyReLU(negative_slope=0.2),
        layers.Reshape((7, 7, generator_in_channels)),
        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding="same"),
        layers.LeakyReLU(negative_slope=0.2),
        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding="same"),
        layers.LeakyReLU(negative_slope=0.2),
        layers.Conv2D(1, (7, 7), padding="same", activation="sigmoid"),
    ],
    name="generator",
)
```

## `ConditionalGAN` ëª¨ë¸ ìƒì„± {#creating-a-conditionalgan-model}

```python
class ConditionalGAN(keras.Model):
    def __init__(self, discriminator, generator, latent_dim):
        super().__init__()
        self.discriminator = discriminator
        self.generator = generator
        self.latent_dim = latent_dim
        self.seed_generator = keras.random.SeedGenerator(1337)
        self.gen_loss_tracker = keras.metrics.Mean(name="generator_loss")
        self.disc_loss_tracker = keras.metrics.Mean(name="discriminator_loss")

    @property
    def metrics(self):
        return [self.gen_loss_tracker, self.disc_loss_tracker]

    def compile(self, d_optimizer, g_optimizer, loss_fn):
        super().compile()
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.loss_fn = loss_fn

    def train_step(self, data):
        # ë°ì´í„°ë¥¼ ì–¸íŒ©í•©ë‹ˆë‹¤.
        real_images, one_hot_labels = data

        # ë ˆì´ë¸”ì— ë”ë¯¸ ì°¨ì›ì„ â€‹â€‹ì¶”ê°€í•˜ì—¬, ì´ë¯¸ì§€ì™€ ì—°ê²°(concatenated)í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
        # ì´ëŠ” íŒë³„ìë¥¼ ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.
        image_one_hot_labels = one_hot_labels[:, :, None, None]
        image_one_hot_labels = ops.repeat(
            image_one_hot_labels, repeats=[image_size * image_size]
        )
        image_one_hot_labels = ops.reshape(
            image_one_hot_labels, (-1, image_size, image_size, num_classes)
        )

        # ì ì¬ ê³µê°„ì—ì„œ ëœë¤ ì§€ì ì„ ìƒ˜í”Œë§í•˜ê³  ë ˆì´ë¸”ì„ ì—°ê²°(concatenate)í•©ë‹ˆë‹¤.
        # ì´ëŠ” ìƒì„±ìë¥¼ ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.
        batch_size = ops.shape(real_images)[0]
        random_latent_vectors = keras.random.normal(
            shape=(batch_size, self.latent_dim), seed=self.seed_generator
        )
        random_vector_labels = ops.concatenate(
            [random_latent_vectors, one_hot_labels], axis=1
        )

        # ë…¸ì´ì¦ˆ(ë ˆì´ë¸”ì— ë”°ë¼)ë¥¼ ë””ì½”ë”©í•˜ì—¬, ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤.
        generated_images = self.generator(random_vector_labels)

        # ì‹¤ì œ ì´ë¯¸ì§€ì™€ ê²°í•©í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì´ëŸ¬í•œ ì´ë¯¸ì§€ì™€ ë ˆì´ë¸”ì„ ì—°ê²°(concatenating)í•˜ê³  ìˆë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”.
        fake_image_and_labels = ops.concatenate(
            [generated_images, image_one_hot_labels], -1
        )
        real_image_and_labels = ops.concatenate([real_images, image_one_hot_labels], -1)
        combined_images = ops.concatenate(
            [fake_image_and_labels, real_image_and_labels], axis=0
        )

        # ì§„ì§œ ì´ë¯¸ì§€ì™€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ êµ¬ë³„í•˜ëŠ” ë¼ë²¨ì„ ì¡°ë¦½í•©ë‹ˆë‹¤.
        labels = ops.concatenate(
            [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0
        )

        # íŒë³„ê¸°ë¥¼ íŠ¸ë ˆì´ë‹ì‹œí‚µë‹ˆë‹¤.
        with tf.GradientTape() as tape:
            predictions = self.discriminator(combined_images)
            d_loss = self.loss_fn(labels, predictions)
        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)
        self.d_optimizer.apply_gradients(
            zip(grads, self.discriminator.trainable_weights)
        )

        # ì ì¬ ê³µê°„ì—ì„œ ëœë¤ ì§€ì ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
        random_latent_vectors = keras.random.normal(
            shape=(batch_size, self.latent_dim), seed=self.seed_generator
        )
        random_vector_labels = ops.concatenate(
            [random_latent_vectors, one_hot_labels], axis=1
        )

        # "ëª¨ë“  ì‹¤ì œ ì´ë¯¸ì§€(all real images)"ë¼ê³  ì íŒ ë¼ë²¨ì„ ì¡°ë¦½í•©ë‹ˆë‹¤.
        misleading_labels = ops.zeros((batch_size, 1))

        # ìƒì„±ê¸°ë¥¼ íŠ¸ë ˆì´ë‹ì‹œí‚µë‹ˆë‹¤. (íŒë³„ê¸°ì˜ ê°€ì¤‘ì¹˜ëŠ” ì—…ë°ì´íŠ¸í•´ì„œëŠ” *ì•ˆ ë©ë‹ˆë‹¤*!)
        with tf.GradientTape() as tape:
            fake_images = self.generator(random_vector_labels)
            fake_image_and_labels = ops.concatenate(
                [fake_images, image_one_hot_labels], -1
            )
            predictions = self.discriminator(fake_image_and_labels)
            g_loss = self.loss_fn(misleading_labels, predictions)
        grads = tape.gradient(g_loss, self.generator.trainable_weights)
        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))

        # ì†ì‹¤ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.
        self.gen_loss_tracker.update_state(g_loss)
        self.disc_loss_tracker.update_state(d_loss)
        return {
            "g_loss": self.gen_loss_tracker.result(),
            "d_loss": self.disc_loss_tracker.result(),
        }
```

## Conditional GAN íŠ¸ë ˆì´ë‹ {#training-the-conditional-gan}

```python
cond_gan = ConditionalGAN(
    discriminator=discriminator, generator=generator, latent_dim=latent_dim
)
cond_gan.compile(
    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),
    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),
    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),
)

cond_gan.fit(dataset, epochs=20)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Epoch 1/20
   18/1094 [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  10s 9ms/step - d_loss: 0.6321 - g_loss: 0.7887

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1704233262.157522    6737 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.

 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 24s 14ms/step - d_loss: 0.4052 - g_loss: 1.5851 - discriminator_loss: 0.4390 - generator_loss: 1.4775
Epoch 2/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.5116 - g_loss: 1.2740 - discriminator_loss: 0.4872 - generator_loss: 1.3330
Epoch 3/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.3626 - g_loss: 1.6775 - discriminator_loss: 0.3252 - generator_loss: 1.8219
Epoch 4/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.2248 - g_loss: 2.2898 - discriminator_loss: 0.3418 - generator_loss: 2.0042
Epoch 5/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6017 - g_loss: 1.0428 - discriminator_loss: 0.6076 - generator_loss: 1.0176
Epoch 6/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6395 - g_loss: 0.9258 - discriminator_loss: 0.6448 - generator_loss: 0.9134
Epoch 7/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6402 - g_loss: 0.8914 - discriminator_loss: 0.6458 - generator_loss: 0.8773
Epoch 8/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6549 - g_loss: 0.8440 - discriminator_loss: 0.6555 - generator_loss: 0.8364
Epoch 9/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6603 - g_loss: 0.8316 - discriminator_loss: 0.6606 - generator_loss: 0.8241
Epoch 10/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6594 - g_loss: 0.8169 - discriminator_loss: 0.6605 - generator_loss: 0.8218
Epoch 11/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6719 - g_loss: 0.7979 - discriminator_loss: 0.6649 - generator_loss: 0.8096
Epoch 12/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6641 - g_loss: 0.7992 - discriminator_loss: 0.6621 - generator_loss: 0.7953
Epoch 13/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6657 - g_loss: 0.7979 - discriminator_loss: 0.6624 - generator_loss: 0.7924
Epoch 14/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6586 - g_loss: 0.8220 - discriminator_loss: 0.6566 - generator_loss: 0.8174
Epoch 15/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6646 - g_loss: 0.7916 - discriminator_loss: 0.6578 - generator_loss: 0.7973
Epoch 16/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6624 - g_loss: 0.7911 - discriminator_loss: 0.6587 - generator_loss: 0.7966
Epoch 17/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6586 - g_loss: 0.8060 - discriminator_loss: 0.6550 - generator_loss: 0.7997
Epoch 18/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6526 - g_loss: 0.7946 - discriminator_loss: 0.6523 - generator_loss: 0.7948
Epoch 19/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6525 - g_loss: 0.8039 - discriminator_loss: 0.6497 - generator_loss: 0.8066
Epoch 20/20
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 9ms/step - d_loss: 0.6480 - g_loss: 0.8005 - discriminator_loss: 0.6469 - generator_loss: 0.8022

<keras.src.callbacks.history.History at 0x7f541a1b5f90>
```

{{% /details %}}

## íŠ¸ë ˆì´ë‹ëœ ìƒì„±ìë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ê°„ ë³´ê°„ {#interpolating-between-classes-with-the-trained-generator}

```python
# ë¨¼ì € ì¡°ê±´ë¶€ GANì—ì„œ íŠ¸ë ˆì´ë‹ëœ ìƒì„±ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
trained_gen = cond_gan.generator

# interpolation + 2(ì‹œì‘ ì´ë¯¸ì§€ì™€ ë§ˆì§€ë§‰ ì´ë¯¸ì§€) ì‚¬ì´ì— ìƒì„±ë  ì¤‘ê°„ ì´ë¯¸ì§€ì˜ ìˆ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
num_interpolation = 9  # @param {type:"integer"}

# ë³´ê°„ì„ ìœ„í•œ ìƒ˜í”Œ ë…¸ì´ì¦ˆì…ë‹ˆë‹¤.
interpolation_noise = keras.random.normal(shape=(1, latent_dim))
interpolation_noise = ops.repeat(interpolation_noise, repeats=num_interpolation)
interpolation_noise = ops.reshape(interpolation_noise, (num_interpolation, latent_dim))


def interpolate_class(first_number, second_number):
    # ì‹œì‘ ë° ì¢…ë£Œ ë¼ë²¨ì„ ì›í•« ì¸ì½”ë”©ëœ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    first_label = keras.utils.to_categorical([first_number], num_classes)
    second_label = keras.utils.to_categorical([second_number], num_classes)
    first_label = ops.cast(first_label, "float32")
    second_label = ops.cast(second_label, "float32")

    # ë‘ ë¼ë²¨ ì‚¬ì´ì˜ ë³´ê°„ ë²¡í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    percent_second_label = ops.linspace(0, 1, num_interpolation)[:, None]
    percent_second_label = ops.cast(percent_second_label, "float32")
    interpolation_labels = (
        first_label * (1 - percent_second_label) + second_label * percent_second_label
    )

    # ë…¸ì´ì¦ˆì™€ ë¼ë²¨ì„ ê²°í•©í•˜ê³  ìƒì„±ìë¥¼ í†µí•´ ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    noise_and_labels = ops.concatenate([interpolation_noise, interpolation_labels], 1)
    fake = trained_gen.predict(noise_and_labels)
    return fake


start_class = 2  # @param {type:"slider", min:0, max:9, step:1}
end_class = 6  # @param {type:"slider", min:0, max:9, step:1}

fake_images = interpolate_class(start_class, end_class)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
 1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 427ms/step
```

{{% /details %}}

ì—¬ê¸°ì„œ, ìš°ë¦¬ëŠ” ë¨¼ì € ì •ê·œ ë¶„í¬ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ìƒ˜í”Œë§í•œ ë‹¤ìŒ,
`num_interpolation` ë²ˆ ë°˜ë³µí•˜ê³  ê·¸ì— ë”°ë¼ ê²°ê³¼ë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
ê·¸ëŸ° ë‹¤ìŒ ë ˆì´ë¸” í•­ë“±ì„±ì´ ì–´ëŠ ì •ë„ ë¹„ìœ¨ë¡œ ì¡´ì¬í•˜ë„ë¡,
`num_interpolation` ë™ì•ˆ ê· ì¼í•˜ê²Œ ë¶„í¬í•©ë‹ˆë‹¤.

```python
fake_images *= 255.0
converted_images = fake_images.astype(np.uint8)
converted_images = ops.image.resize(converted_images, (96, 96)).numpy().astype(np.uint8)
imageio.mimsave("animation.gif", converted_images[:, :, :, 0], fps=1)
embed.embed_file("animation.gif")
```

![gif](/images/examples/generative/conditional_gan/animation.gif)

[WGAN-GP]({{< relref "/docs/examples/generative/wgan_gp" >}})ì™€ ê°™ì€ ë ˆì‹œí”¼ë¥¼ ì‚¬ìš©í•˜ë©´,
ì´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë”ìš± ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì¡°ê±´ ìƒì„±ì€ [VQ-GAN](https://arxiv.org/abs/2012.09841),
[DALL-E](https://openai.com/blog/dall-e/) ë“±ê³¼ ê°™ì€,
ë§ì€ ìµœì‹  ì´ë¯¸ì§€ ìƒì„± ì•„í‚¤í…ì²˜ì—ì„œë„ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.

[Hugging Face Hub](https://huggingface.co/keras-io/conditional-gan)ì—ì„œ í˜¸ìŠ¤íŒ…ë˜ëŠ” íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ ,
[Hugging Face Spaces](https://huggingface.co/spaces/keras-io/conditional-GAN)ì—ì„œ ë°ëª¨ë¥¼ ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
