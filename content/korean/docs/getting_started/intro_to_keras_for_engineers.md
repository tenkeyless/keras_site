---
slug: intro_to_keras_for_engineers
title: ì—”ì§€ë‹ˆì–´ë¥¼ ìœ„í•œ Keras ì†Œê°œ
toc: true
weight: 1
type: docs
---

{{< keras/original checkedAt="2024-11-18" >}}

**ì €ì:** [fchollet](https://twitter.com/fchollet)  
**ìƒì„±ì¼:** 2023/07/10  
**ìµœì¢…í¸ì§‘ì¼:** 2023/07/10  
**ì„¤ëª…:** Keras 3ì™€ì˜ ì²« ë§Œë‚¨.

{{< cards cols="2" >}}
{{< card link="https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/intro_to_keras_for_engineers.ipynb" title="Colab" tag="Colab" tagType="warning">}}
{{< card link="https://github.com/keras-team/keras-io/blob/master/guides/intro_to_keras_for_engineers.py" title="GitHub" tag="GitHub">}}
{{< /cards >}}

## ì†Œê°œ {#introduction}

Keras 3ëŠ” TensorFlow, JAX ë° PyTorchì™€ ìƒí˜¸ í˜¸í™˜ë˜ëŠ” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì£¼ìš” Keras 3 ì›Œí¬í”Œë¡œìš°ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.

## ì…‹ì—… {#setup}

ì—¬ê¸°ì„œëŠ” JAX ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
ì•„ë˜ ë¬¸ìì—´ì„ `"tensorflow"` ë˜ëŠ” `"torch"`ë¡œ ìˆ˜ì •í•˜ê³ ,
"Restart runtime"ì„ ëˆ„ë¥´ë©´, ë…¸íŠ¸ë¶ ì „ì²´ê°€ ë˜‘ê°™ì´ ì‹¤í–‰ë©ë‹ˆë‹¤!
ì´ ì „ì²´ ê°€ì´ë“œëŠ” ë°±ì—”ë“œì— êµ¬ì• ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤.

```python
import numpy as np
import os

os.environ["KERAS_BACKEND"] = "jax"

# KerasëŠ” ë°±ì—”ë“œê°€ êµ¬ì„±ëœ í›„ì— import ë˜ì–´ì•¼ í•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”.
# íŒ¨í‚¤ì§€ë¥¼ import í•œ í›„ì—ëŠ”, ë°±ì—”ë“œë¥¼ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
import keras
```

## ì²« ë²ˆì§¸ ì˜ˆì‹œ: MNIST ì»¨ë¸Œë„· {#a-first-example-a-mnist-convnet}

MNIST ìˆ«ìë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ convnetì„ íŠ¸ë ˆì´ë‹í•˜ëŠ” MLì˜ Hello Worldë¶€í„° ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ ë°ì´í„°ì…ë‹ˆë‹¤:

```python
# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  íŠ¸ë ˆì´ë‹ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•˜ê¸°
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# ì´ë¯¸ì§€ë¥¼ [0, 1] ë²”ìœ„ë¡œ ì¡°ì •í•˜ê¸°
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255
# ì´ë¯¸ì§€ê°€ (28, 28, 1) ëª¨ì–‘ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤.
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)
print(x_train.shape[0], "train samples")
print(x_test.shape[0], "test samples")
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
x_train shape: (60000, 28, 28, 1)
y_train shape: (60000,)
60000 train samples
10000 test samples
```

{{% /details %}}

ì´ê²ƒì´ ìš°ë¦¬ì˜ ëª¨ë¸ì…ë‹ˆë‹¤.

Kerasì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ ë¹Œë“œ ì˜µì…˜ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- [Sequential API]({{< relref "/docs/guides/sequential_model" >}}) (ì•„ë˜ì— ìš°ë¦¬ê°€ ì‚¬ìš©í•œ ê²ƒ)
- [Functional API]({{< relref "/docs/guides/functional_api" >}}) (ê°€ì¥ ì¼ë°˜ì ì„)
- [ì„œë¸Œí´ë˜ì‹±ì„ í†µí•´ ë‚˜ë§Œì˜ ëª¨ë¸ ì‘ì„±í•˜ê¸°]({{< relref "/docs/guides/making_new_layers_and_models_via_subclassing" >}}) (ê³ ê¸‰ ì‚¬ìš© ì¼€ì´ìŠ¤)

```python
# ëª¨ë¸ íŒŒë¼ë¯¸í„°
num_classes = 10
input_shape = (28, 28, 1)

model = keras.Sequential(
    [
        keras.layers.Input(shape=input_shape),
        keras.layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        keras.layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        keras.layers.MaxPooling2D(pool_size=(2, 2)),
        keras.layers.Conv2D(128, kernel_size=(3, 3), activation="relu"),
        keras.layers.Conv2D(128, kernel_size=(3, 3), activation="relu"),
        keras.layers.GlobalAveragePooling2D(),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(num_classes, activation="softmax"),
    ]
)
```

ì´ê²ƒì´ ìš°ë¦¬ ëª¨ë¸ì˜ summary ì…ë‹ˆë‹¤:

```python
model.summary()
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Model: "sequential"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                    â”ƒ Output Shape              â”ƒ    Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ conv2d (Conv2D)                 â”‚ (None, 26, 26, 64)        â”‚        640 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_1 (Conv2D)               â”‚ (None, 24, 24, 64)        â”‚     36,928 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d (MaxPooling2D)    â”‚ (None, 12, 12, 64)        â”‚          0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_2 (Conv2D)               â”‚ (None, 10, 10, 128)       â”‚     73,856 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_3 (Conv2D)               â”‚ (None, 8, 8, 128)         â”‚    147,584 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ global_average_pooling2d        â”‚ (None, 128)               â”‚          0 â”‚
â”‚ (GlobalAveragePooling2D)        â”‚                           â”‚            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout (Dropout)               â”‚ (None, 128)               â”‚          0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense (Dense)                   â”‚ (None, 10)                â”‚      1,290 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 260,298 (1016.79 KB)
 Trainable params: 260,298 (1016.79 KB)
 Non-trainable params: 0 (0.00 B)
```

{{% /details %}}

`compile()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜, ëª¨ë‹ˆí„°ë§í•  ë©”íŠ¸ë¦­ì„ ì§€ì •í•©ë‹ˆë‹¤.
JAX ë° TensorFlow ë°±ì—”ë“œì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ XLA ì»´íŒŒì¼ì´ ì¼œì ¸ ìˆë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”.

```python
model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="acc"),
    ],
)
```

ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•˜ê³  í‰ê°€í•´ ë³´ê² ìŠµë‹ˆë‹¤.
ë³´ì§€ ì•Šì€ ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜í™”ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•´,
íŠ¸ë ˆì´ë‹ ì¤‘ì— ë°ì´í„°ì˜ 15%ì— í•´ë‹¹í•˜ëŠ” ê²€ì¦ ë¶„í• ì„ ë”°ë¡œ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤.

```python
batch_size = 128
epochs = 20

callbacks = [
    keras.callbacks.ModelCheckpoint(filepath="model_at_epoch_{epoch}.keras"),
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=2),
]

model.fit(
    x_train,
    y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=0.15,
    callbacks=callbacks,
)
score = model.evaluate(x_test, y_test, verbose=0)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Epoch 1/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 74s 184ms/step - acc: 0.4980 - loss: 1.3832 - val_acc: 0.9609 - val_loss: 0.1513
Epoch 2/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 74s 186ms/step - acc: 0.9245 - loss: 0.2487 - val_acc: 0.9702 - val_loss: 0.0999
Epoch 3/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 70s 175ms/step - acc: 0.9515 - loss: 0.1647 - val_acc: 0.9816 - val_loss: 0.0608
Epoch 4/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 69s 174ms/step - acc: 0.9622 - loss: 0.1247 - val_acc: 0.9833 - val_loss: 0.0541
Epoch 5/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 68s 171ms/step - acc: 0.9685 - loss: 0.1083 - val_acc: 0.9860 - val_loss: 0.0468
Epoch 6/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 70s 176ms/step - acc: 0.9710 - loss: 0.0955 - val_acc: 0.9897 - val_loss: 0.0400
Epoch 7/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 69s 172ms/step - acc: 0.9742 - loss: 0.0853 - val_acc: 0.9888 - val_loss: 0.0388
Epoch 8/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 68s 169ms/step - acc: 0.9789 - loss: 0.0738 - val_acc: 0.9902 - val_loss: 0.0387
Epoch 9/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 75s 187ms/step - acc: 0.9789 - loss: 0.0691 - val_acc: 0.9907 - val_loss: 0.0341
Epoch 10/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 77s 194ms/step - acc: 0.9806 - loss: 0.0636 - val_acc: 0.9907 - val_loss: 0.0348
Epoch 11/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 74s 186ms/step - acc: 0.9812 - loss: 0.0610 - val_acc: 0.9926 - val_loss: 0.0271
Epoch 12/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 219s 550ms/step - acc: 0.9820 - loss: 0.0590 - val_acc: 0.9912 - val_loss: 0.0294
Epoch 13/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 70s 176ms/step - acc: 0.9843 - loss: 0.0504 - val_acc: 0.9918 - val_loss: 0.0316
```

{{% /details %}}

íŠ¸ë ˆì´ë‹ ì¤‘ì—, ê° ì—í¬í¬ê°€ ëë‚  ë•Œë§ˆë‹¤ ëª¨ë¸ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.
ì•„ë˜ì²˜ëŸ¼ ëª¨ë¸ì„ ìµœì‹  ìƒíƒœë¡œ ì €ì¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

```python
model.save("final_model.keras")
```

ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤:

```python
model = keras.saving.load_model("final_model.keras")
```

ë‹¤ìŒìœ¼ë¡œ, `predict()`ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ í™•ë¥  ì˜ˆì¸¡ì„ ì¿¼ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
predictions = model.predict(x_test)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
 313/313 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 9ms/step
```

{{% /details %}}

ì´ê²ƒì´ ê¸°ë³¸ ì‚¬í•­ì…ë‹ˆë‹¤!

## í¬ë¡œìŠ¤ í”„ë ˆì„ì›Œí¬ ì»¤ìŠ¤í…€ ì»´í¬ë„ŒíŠ¸ ì‘ì„± {#writing-cross-framework-custom-components}

Kerasë¥¼ ì‚¬ìš©í•˜ë©´ ë™ì¼í•œ ì½”ë“œë² ì´ìŠ¤ë¡œ,
TensorFlow, JAX, PyTorchì—ì„œ ì‘ë™í•˜ëŠ” ì»¤ìŠ¤í…€ ë ˆì´ì–´, ëª¨ë¸, ë©”íŠ¸ë¦­, ì†ì‹¤ ë° ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë¨¼ì € ì»¤ìŠ¤í…€ ë ˆì´ì–´ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

`keras.ops` ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ë©ë‹ˆë‹¤:

- NumPy APIì˜ êµ¬í˜„(ì˜ˆ: [`keras.ops.stack`]({{< relref "/docs/api/ops/numpy#stack-function" >}}) ë˜ëŠ” [`keras.ops.matmul`]({{< relref "/docs/api/ops/numpy#matmul-function" >}})
- NumPyì— ì—†ëŠ” ì‹ ê²½ë§ ì „ìš© ops ì„¸íŠ¸(ì˜ˆ: [`keras.ops.conv`]({{< relref "/docs/api/ops/nn#conv-function" >}}) ë˜ëŠ” [`keras.ops.binary_crossentropy`]({{< relref "/docs/api/ops/nn#binary_crossentropy-function" >}}))

ëª¨ë“  ë°±ì—”ë“œì—ì„œ ì‘ë™í•˜ëŠ” ì»¤ìŠ¤í…€ `Dense` ë ˆì´ì–´ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
class MyDense(keras.layers.Layer):
    def __init__(self, units, activation=None, name=None):
        super().__init__(name=name)
        self.units = units
        self.activation = keras.activations.get(activation)

    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.w = self.add_weight(
            shape=(input_dim, self.units),
            initializer=keras.initializers.GlorotNormal(),
            name="kernel",
            trainable=True,
        )

        self.b = self.add_weight(
            shape=(self.units,),
            initializer=keras.initializers.Zeros(),
            name="bias",
            trainable=True,
        )

    def call(self, inputs):
        # Keras opsë¥¼ ì‚¬ìš©í•˜ì—¬, ë°±ì—”ë“œì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ë ˆì´ì–´/ë©”íŠ¸ë¦­ ë“±ì„ ìƒì„±í•˜ì„¸ìš”.
        x = keras.ops.matmul(inputs, self.w) + self.b
        return self.activation(x)
```

ë‹¤ìŒìœ¼ë¡œ, `keras.random` ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì˜ì¡´í•˜ëŠ” ì»¤ìŠ¤í…€ `Dropout` ë ˆì´ì–´ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
class MyDropout(keras.layers.Layer):
    def __init__(self, rate, name=None):
        super().__init__(name=name)
        self.rate = rate
        # seed_generatorë¥¼ ì‚¬ìš©í•˜ì—¬ RNG ìƒíƒœë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
        # ì´ëŠ” ìƒíƒœ ìš”ì†Œ(state element)ì´ë©°,
        # ì‹œë“œ ë³€ìˆ˜ëŠ” `layer.variables`ì˜ ì¼ë¶€ë¡œ ì¶”ì ë©ë‹ˆë‹¤.
        self.seed_generator = keras.random.SeedGenerator(1337)

    def call(self, inputs):
        # ëœë¤ opsë¥¼ ìœ„í•´ `keras.random`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        return keras.random.dropout(inputs, self.rate, seed=self.seed_generator)
```

ë‹¤ìŒìœ¼ë¡œ, ë‘ ê°œì˜ ì»¤ìŠ¤í…€ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì»¤ìŠ¤í…€ ì„œë¸Œí´ë˜ìŠ¤ ëª¨ë¸ì„ ì‘ì„±í•´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
class MyModel(keras.Model):
    def __init__(self, num_classes):
        super().__init__()
        self.conv_base = keras.Sequential(
            [
                keras.layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
                keras.layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
                keras.layers.MaxPooling2D(pool_size=(2, 2)),
                keras.layers.Conv2D(128, kernel_size=(3, 3), activation="relu"),
                keras.layers.Conv2D(128, kernel_size=(3, 3), activation="relu"),
                keras.layers.GlobalAveragePooling2D(),
            ]
        )
        self.dp = MyDropout(0.5)
        self.dense = MyDense(num_classes, activation="softmax")

    def call(self, x):
        x = self.conv_base(x)
        x = self.dp(x)
        return self.dense(x)
```

ì»´íŒŒì¼í•˜ê³ , fit í•´ë³´ê² ìŠµë‹ˆë‹¤:

```python

model = MyModel(num_classes=10)
model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="acc"),
    ],
)

model.fit(
    x_train,
    y_train,
    batch_size=batch_size,
    epochs=1,  # ì—¬ê¸°ì—ì„œëŠ” ë” ë¹ ë¥´ê²Œ í•˜ê¸° ìœ„í•´, ì—í¬í¬ë¥¼ 1ë¡œ ì§€ì •í•©ë‹ˆë‹¤.
    validation_split=0.15,
)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 70s 174ms/step - acc: 0.5104 - loss: 1.3473 - val_acc: 0.9256 - val_loss: 0.2484

<keras.src.callbacks.history.History at 0x105608670>
```

{{% /details %}}

## ì„ì˜ì˜ ë°ì´í„° ì†ŒìŠ¤ì— ëŒ€í•´ ëª¨ë¸ íŠ¸ë ˆì´ë‹ {#training-models-on-arbitrary-data-sources}

ëª¨ë“  Keras ëª¨ë¸ì€ ì‚¬ìš© ì¤‘ì¸ ë°±ì—”ë“œì™€ ê´€ê³„ì—†ì´ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì— ëŒ€í•´ íŠ¸ë ˆì´ë‹ ë° í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì—¬ê¸°ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ë©ë‹ˆë‹¤:

- NumPy ë°°ì—´
- Pandas ë°ì´í„° í”„ë ˆì„
- TensorFlow [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) ê°ì²´
- PyTorch `DataLoader` ê°ì²´
- Keras `PyDataset` ê°ì²´

ì´ ì˜ˆì œëŠ” TensorFlow, JAX ë˜ëŠ” PyTorch ì¤‘ ì–´ë–¤ ê²ƒì„ Keras ë°±ì—”ë“œë¡œ ì‚¬ìš©í•˜ë“ ì§€ ëª¨ë‘ ì‘ë™í•©ë‹ˆë‹¤.

PyTorch `DataLoaders`ë¥¼ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
import torch

# TensorDataset ìƒì„±
train_torch_dataset = torch.utils.data.TensorDataset(
    torch.from_numpy(x_train), torch.from_numpy(y_train)
)
val_torch_dataset = torch.utils.data.TensorDataset(
    torch.from_numpy(x_test), torch.from_numpy(y_test)
)

# DataLoader ìƒì„±
train_dataloader = torch.utils.data.DataLoader(
    train_torch_dataset, batch_size=batch_size, shuffle=True
)
val_dataloader = torch.utils.data.DataLoader(
    val_torch_dataset, batch_size=batch_size, shuffle=False
)

model = MyModel(num_classes=10)
model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="acc"),
    ],
)
model.fit(train_dataloader, epochs=1, validation_data=val_dataloader)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
 469/469 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 81s 172ms/step - acc: 0.5502 - loss: 1.2550 - val_acc: 0.9419 - val_loss: 0.1972

<keras.src.callbacks.history.History at 0x2b3385480>
```

{{% /details %}}

ì´ì œ ì´ê²ƒì„ [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œë„í•´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
import tensorflow as tf

train_dataset = (
    tf.data.Dataset.from_tensor_slices((x_train, y_train))
    .batch(batch_size)
    .prefetch(tf.data.AUTOTUNE)
)
test_dataset = (
    tf.data.Dataset.from_tensor_slices((x_test, y_test))
    .batch(batch_size)
    .prefetch(tf.data.AUTOTUNE)
)

model = MyModel(num_classes=10)
model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="acc"),
    ],
)
model.fit(train_dataset, epochs=1, validation_data=test_dataset)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
 469/469 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 81s 172ms/step - acc: 0.5771 - loss: 1.1948 - val_acc: 0.9229 - val_loss: 0.2502

<keras.src.callbacks.history.History at 0x2b33e7df0>
```

{{% /details %}}

## ë” ì½ì–´ë³´ê¸° {#further-reading}

ì´ê²ƒìœ¼ë¡œ Keras 3ì˜ ìƒˆë¡œìš´ ë©€í‹° ë°±ì—”ë“œ ê¸°ëŠ¥ì— ëŒ€í•œ ê°„ëµí•œ ê°œìš”ë¥¼ ë§ˆì³¤ìŠµë‹ˆë‹¤.
ì´ì œ, ë‹¤ìŒ ê²ƒë“¤ì— ëŒ€í•´ ì•Œì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### `fit()`ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•˜ëŠ” ë°©ë²• {#how-to-customize-what-happens-in-fit}

ë¹„í‘œì¤€ íŠ¸ë ˆì´ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•˜ê³  ì‹¶ì§€ë§Œ,
`fit()`ì˜ ê°•ë ¥í•œ ì„±ëŠ¥ê³¼ ìœ ìš©ì„±ì„ í™œìš©í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?
ì„ì˜ì˜ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ì§€ì›í•˜ë„ë¡, `fit()`ì„ ì‰½ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- [TensorFlow `fit()`ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ]({{< relref "/docs/guides/custom_train_step_in_tensorflow">}})
- [JAX `fit()`ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ]({{< relref "/docs/guides/custom_train_step_in_jax">}})
- [PyTorch `fit()`ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ]({{< relref "/docs/guides/custom_train_step_in_torch">}})

## ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë‹ ë£¨í”„ë¥¼ ì‘ì„±í•˜ëŠ” ë°©ë²• {#how-to-write-custom-training-loops}

- [TensorFlowì—ì„œ ì²˜ìŒë¶€í„° íŠ¸ë ˆì´ë‹ ë£¨í”„ ì‘ì„±]({{< relref "/docs/guides/writing_a_custom_training_loop_in_tensorflow">}})
- [JAXì—ì„œ ì²˜ìŒë¶€í„° íŠ¸ë ˆì´ë‹ ë£¨í”„ ì‘ì„±]({{< relref "/docs/guides/writing_a_custom_training_loop_in_jax">}})
- [PyTorchì—ì„œ ì²˜ìŒë¶€í„° íŠ¸ë ˆì´ë‹ ë£¨í”„ ì‘ì„±]({{< relref "/docs/guides/writing_a_custom_training_loop_in_torch">}})

## ë¶„ì‚° íŠ¸ë ˆì´ë‹ í•˜ëŠ” ë°©ë²• {#how-to-distribute-training}

- [TensorFlow ë¶„ì‚° íŠ¸ë ˆì´ë‹ ê°€ì´ë“œ]({{< relref "/docs/guides/distributed_training_with_tensorflow">}})
- [JAX ë¶„ì‚° íŠ¸ë ˆì´ë‹ ì˜ˆì œ](https://github.com/keras-team/keras/blob/master/examples/demo_jax_distributed.py)
- [PyTorch ë¶„ì‚° íŠ¸ë ˆì´ë‹ ì˜ˆì œ](https://github.com/keras-team/keras/blob/master/examples/demo_torch_multi_gpu.py)

ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¦ê²¨ë³´ì„¸ìš”! ğŸš€
