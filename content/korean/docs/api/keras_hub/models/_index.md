---
title: KerasHub Models
linkTitle: Pretrained Models
toc: true
weight: 1
type: docs
---

{{< keras/original checkedAt="2024-11-26" >}}

KerasHub contains end-to-end implementations of popular model architectures.
These models can be created in two ways:

- Through the `from_preset()` constructor, which instantiates an object with
  a pre-trained configurations, vocabularies, and (optionally) weights.
- Through custom configuration controlled by the user.

Below, we list all presets available in the library. For more detailed usage,
browse the docstring for a particular class. For an in depth introduction
to our API, see the [getting started guide]({{< relref "/docs/guides/keras_hub/getting_started/" >}}).

## Presets

The following preset names correspond to a config and weights for a pretrained
model. Any task, preprocessor, backbone or tokenizer `from_preset()` can be used
to create a model from the saved preset.

```python
backbone = keras_hub.models.Backbone.from_preset("bert_base_en")
tokenizer = keras_hub.models.Tokenizer.from_preset("bert_base_en")
classifier = keras_hub.models.TextClassifier.from_preset("bert_base_en", num_classes=2)
preprocessor = keras_hub.models.TextClassifierPreprocessor.from_preset("bert_base_en")
```

| Preset name                            | Model                                  | Parameters | Description                                                                                                                                                                                                                                                                                                                                           |
| -------------------------------------- | -------------------------------------- | ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| albert_base_en_uncased                 | [ALBERT](albert)                       | 11.68M     | 12-layer ALBERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/albert/blob/master/README.md)                                                                                                                                                                        |
| albert_large_en_uncased                | [ALBERT](albert)                       | 17.68M     | 24-layer ALBERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/albert/blob/master/README.md)                                                                                                                                                                        |
| albert_extra_large_en_uncased          | [ALBERT](albert)                       | 58.72M     | 24-layer ALBERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/albert/blob/master/README.md)                                                                                                                                                                        |
| albert_extra_extra_large_en_uncased    | [ALBERT](albert)                       | 222.60M    | 12-layer ALBERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/albert/blob/master/README.md)                                                                                                                                                                        |
| bart_base_en                           | [BART](bart)                           | 139.42M    | 6-layer BART model where case is maintained. Trained on BookCorpus, English Wikipedia and CommonCrawl. [Model Card](https://github.com/facebookresearch/fairseq/blob/main/examples/bart/README.md)                                                                                                                                                    |
| bart_large_en                          | [BART](bart)                           | 406.29M    | 12-layer BART model where case is maintained. Trained on BookCorpus, English Wikipedia and CommonCrawl. [Model Card](https://github.com/facebookresearch/fairseq/blob/main/examples/bart/README.md)                                                                                                                                                   |
| bart_large_en_cnn                      | [BART](bart)                           | 406.29M    | The `bart_large_en` backbone model fine-tuned on the CNN+DM summarization dataset. [Model Card](https://github.com/facebookresearch/fairseq/blob/main/examples/bart/README.md)                                                                                                                                                                        |
| bert_tiny_en_uncased                   | [BERT](bert)                           | 4.39M      | 2-layer BERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/bert/blob/master/README.md)                                                                                                                                                                             |
| bert_small_en_uncased                  | [BERT](bert)                           | 28.76M     | 4-layer BERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/bert/blob/master/README.md)                                                                                                                                                                             |
| bert_medium_en_uncased                 | [BERT](bert)                           | 41.37M     | 8-layer BERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/bert/blob/master/README.md)                                                                                                                                                                             |
| bert_base_en_uncased                   | [BERT](bert)                           | 109.48M    | 12-layer BERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/bert/blob/master/README.md)                                                                                                                                                                            |
| bert_base_en                           | [BERT](bert)                           | 108.31M    | 12-layer BERT model where case is maintained. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/bert/blob/master/README.md)                                                                                                                                                                                 |
| bert_base_zh                           | [BERT](bert)                           | 102.27M    | 12-layer BERT model. Trained on Chinese Wikipedia. [Model Card](https://github.com/google-research/bert/blob/master/README.md)                                                                                                                                                                                                                        |
| bert_base_multi                        | [BERT](bert)                           | 177.85M    | 12-layer BERT model where case is maintained. Trained on trained on Wikipedias of 104 languages [Model Card](https://github.com/google-research/bert/blob/master/README.md)                                                                                                                                                                           |
| bert_large_en_uncased                  | [BERT](bert)                           | 335.14M    | 24-layer BERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/bert/blob/master/README.md)                                                                                                                                                                            |
| bert_large_en                          | [BERT](bert)                           | 333.58M    | 24-layer BERT model where case is maintained. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/bert/blob/master/README.md)                                                                                                                                                                                 |
| bert_tiny_en_uncased_sst2              | [BERT](bert)                           | 4.39M      | The bert_tiny_en_uncased backbone model fine-tuned on the SST-2 sentiment analysis dataset. [Model Card](https://github.com/google-research/bert/blob/master/README.md)                                                                                                                                                                               |
| bloom_560m_multi                       | [BLOOM](bloom)                         | 559.21M    | 24-layer Bloom model with hidden dimension of 1024. trained on 45 natural languages and 12 programming languages. [Model Card](https://huggingface.co/bigscience/bloom-560m)                                                                                                                                                                          |
| bloom_1.1b_multi                       | [BLOOM](bloom)                         | 1.07B      | 24-layer Bloom model with hidden dimension of 1536. trained on 45 natural languages and 12 programming languages. [Model Card](https://huggingface.co/bigscience/bloom-1b1)                                                                                                                                                                           |
| bloom_1.7b_multi                       | [BLOOM](bloom)                         | 1.72B      | 24-layer Bloom model with hidden dimension of 2048. trained on 45 natural languages and 12 programming languages. [Model Card](https://huggingface.co/bigscience/bloom-1b7)                                                                                                                                                                           |
| bloom_3b_multi                         | [BLOOM](bloom)                         | 3.00B      | 30-layer Bloom model with hidden dimension of 2560. trained on 45 natural languages and 12 programming languages. [Model Card](https://huggingface.co/bigscience/bloom-3b)                                                                                                                                                                            |
| bloomz_560m_multi                      | [BLOOMZ](bloom)                        | 559.21M    | 24-layer Bloom model with hidden dimension of 1024. finetuned on crosslingual task mixture (xP3) dataset. [Model Card](https://huggingface.co/bigscience/bloomz-560m)                                                                                                                                                                                 |
| bloomz_1.1b_multi                      | [BLOOMZ](bloom)                        | 1.07B      | 24-layer Bloom model with hidden dimension of 1536. finetuned on crosslingual task mixture (xP3) dataset. [Model Card](https://huggingface.co/bigscience/bloomz-1b1)                                                                                                                                                                                  |
| bloomz_1.7b_multi                      | [BLOOMZ](bloom)                        | 1.72B      | 24-layer Bloom model with hidden dimension of 2048. finetuned on crosslingual task mixture (xP3) dataset. [Model Card](https://huggingface.co/bigscience/bloomz-1b7)                                                                                                                                                                                  |
| bloomz_3b_multi                        | [BLOOMZ](bloom)                        | 3.00B      | 30-layer Bloom model with hidden dimension of 2560. finetuned on crosslingual task mixture (xP3) dataset. [Model Card](https://huggingface.co/bigscience/bloomz-3b)                                                                                                                                                                                   |
| deberta_v3_extra_small_en              | [DeBERTaV3](deberta_v3)                | 70.68M     | 12-layer DeBERTaV3 model where case is maintained. Trained on English Wikipedia, BookCorpus and OpenWebText. [Model Card](https://huggingface.co/microsoft/deberta-v3-xsmall)                                                                                                                                                                         |
| deberta_v3_small_en                    | [DeBERTaV3](deberta_v3)                | 141.30M    | 6-layer DeBERTaV3 model where case is maintained. Trained on English Wikipedia, BookCorpus and OpenWebText. [Model Card](https://huggingface.co/microsoft/deberta-v3-small)                                                                                                                                                                           |
| deberta_v3_base_en                     | [DeBERTaV3](deberta_v3)                | 183.83M    | 12-layer DeBERTaV3 model where case is maintained. Trained on English Wikipedia, BookCorpus and OpenWebText. [Model Card](https://huggingface.co/microsoft/deberta-v3-base)                                                                                                                                                                           |
| deberta_v3_large_en                    | [DeBERTaV3](deberta_v3)                | 434.01M    | 24-layer DeBERTaV3 model where case is maintained. Trained on English Wikipedia, BookCorpus and OpenWebText. [Model Card](https://huggingface.co/microsoft/deberta-v3-large)                                                                                                                                                                          |
| deberta_v3_base_multi                  | [DeBERTaV3](deberta_v3)                | 278.22M    | 12-layer DeBERTaV3 model where case is maintained. Trained on the 2.5TB multilingual CC100 dataset. [Model Card](https://huggingface.co/microsoft/mdeberta-v3-base)                                                                                                                                                                                   |
| deeplab_v3_plus_resnet50_pascalvoc     | [DeepLabV3](deeplab_v3)                | 39.19M     | DeepLabV3+ model with ResNet50 as image encoder and trained on augmented Pascal VOC dataset by Semantic Boundaries Dataset(SBD)which is having categorical accuracy of 90.01 and 0.63 Mean IoU. [Model Card](https://arxiv.org/abs/1802.02611)                                                                                                        |
| densenet_121_imagenet                  | [DenseNet](densenet)                   | 7.04M      | 121-layer DenseNet model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1608.06993)                                                                                                                                                                                                               |
| densenet_169_imagenet                  | [DenseNet](densenet)                   | 12.64M     | 169-layer DenseNet model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1608.06993)                                                                                                                                                                                                               |
| densenet_201_imagenet                  | [DenseNet](densenet)                   | 18.32M     | 201-layer DenseNet model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1608.06993)                                                                                                                                                                                                               |
| distil_bert_base_en_uncased            | [DistilBERT](distil_bert)              | 66.36M     | 6-layer DistilBERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus using BERT as the teacher model. [Model Card](https://huggingface.co/distilbert-base-uncased)                                                                                                                                                      |
| distil_bert_base_en                    | [DistilBERT](distil_bert)              | 65.19M     | 6-layer DistilBERT model where case is maintained. Trained on English Wikipedia + BooksCorpus using BERT as the teacher model. [Model Card](https://huggingface.co/distilbert-base-cased)                                                                                                                                                             |
| distil_bert_base_multi                 | [DistilBERT](distil_bert)              | 134.73M    | 6-layer DistilBERT model where case is maintained. Trained on Wikipedias of 104 languages [Model Card](https://huggingface.co/distilbert-base-multilingual-cased)                                                                                                                                                                                     |
| electra_small_discriminator_uncased_en | [ELECTRA](electra)                     | 13.55M     | 12-layer small ELECTRA discriminator model. All inputs are lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/electra)                                                                                                                                                                           |
| electra_small_generator_uncased_en     | [ELECTRA](electra)                     | 13.55M     | 12-layer small ELECTRA generator model. All inputs are lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/electra)                                                                                                                                                                               |
| electra_base_discriminator_uncased_en  | [ELECTRA](electra)                     | 109.48M    | 12-layer base ELECTRA discriminator model. All inputs are lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/electra)                                                                                                                                                                            |
| electra_base_generator_uncased_en      | [ELECTRA](electra)                     | 33.58M     | 12-layer base ELECTRA generator model. All inputs are lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/electra)                                                                                                                                                                                |
| electra_large_discriminator_uncased_en | [ELECTRA](electra)                     | 335.14M    | 24-layer large ELECTRA discriminator model. All inputs are lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/electra)                                                                                                                                                                           |
| electra_large_generator_uncased_en     | [ELECTRA](electra)                     | 51.07M     | 24-layer large ELECTRA generator model. All inputs are lowercased. Trained on English Wikipedia + BooksCorpus. [Model Card](https://github.com/google-research/electra)                                                                                                                                                                               |
| f_net_base_en                          | [FNet](f_net)                          | 82.86M     | 12-layer FNet model where case is maintained. Trained on the C4 dataset. [Model Card](https://github.com/google-research/google-research/blob/master/f_net/README.md)                                                                                                                                                                                 |
| f_net_large_en                         | [FNet](f_net)                          | 236.95M    | 24-layer FNet model where case is maintained. Trained on the C4 dataset. [Model Card](https://github.com/google-research/google-research/blob/master/f_net/README.md)                                                                                                                                                                                 |
| falcon_refinedweb_1b_en                | [Falcon](falcon)                       | 1.31B      | 24-layer Falcon model (Falcon with 1B parameters), trained on 350B tokens of RefinedWeb dataset. [Model Card](https://huggingface.co/tiiuae/falcon-rw-1b)                                                                                                                                                                                             |
| resnet_18_imagenet                     | [ResNet](resnet)                       | 11.19M     | 18-layer ResNet model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/2110.00476)                                                                                                                                                                                                                  |
| resnet_50_imagenet                     | [ResNet](resnet)                       | 23.56M     | 50-layer ResNet model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/2110.00476)                                                                                                                                                                                                                  |
| resnet_101_imagenet                    | [ResNet](resnet)                       | 42.61M     | 101-layer ResNet model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/2110.00476)                                                                                                                                                                                                                 |
| resnet_152_imagenet                    | [ResNet](resnet)                       | 58.30M     | 152-layer ResNet model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/2110.00476)                                                                                                                                                                                                                 |
| resnet_v2_50_imagenet                  | [ResNet](resnet)                       | 23.56M     | 50-layer ResNetV2 model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/2110.00476)                                                                                                                                                                                                                |
| resnet_v2_101_imagenet                 | [ResNet](resnet)                       | 42.61M     | 101-layer ResNetV2 model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/2110.00476)                                                                                                                                                                                                               |
| resnet_vd_18_imagenet                  | [ResNet](resnet)                       | 11.72M     | 18-layer ResNetVD (ResNet with bag of tricks) model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1812.01187)                                                                                                                                                                                    |
| resnet_vd_34_imagenet                  | [ResNet](resnet)                       | 21.84M     | 34-layer ResNetVD (ResNet with bag of tricks) model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1812.01187)                                                                                                                                                                                    |
| resnet_vd_50_imagenet                  | [ResNet](resnet)                       | 25.63M     | 50-layer ResNetVD (ResNet with bag of tricks) model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1812.01187)                                                                                                                                                                                    |
| resnet_vd_50_ssld_imagenet             | [ResNet](resnet)                       | 25.63M     | 50-layer ResNetVD (ResNet with bag of tricks) model pre-trained on the ImageNet 1k dataset at a 224x224 resolution with knowledge distillation. [Model Card](https://arxiv.org/abs/1812.01187)                                                                                                                                                        |
| resnet_vd_50_ssld_v2_imagenet          | [ResNet](resnet)                       | 25.63M     | 50-layer ResNetVD (ResNet with bag of tricks) model pre-trained on the ImageNet 1k dataset at a 224x224 resolution with knowledge distillation and AutoAugment. [Model Card](https://arxiv.org/abs/1812.01187)                                                                                                                                        |
| resnet_vd_50_ssld_v2_fix_imagenet      | [ResNet](resnet)                       | 25.63M     | 50-layer ResNetVD (ResNet with bag of tricks) model pre-trained on the ImageNet 1k dataset at a 224x224 resolution with knowledge distillation, AutoAugment and additional fine-tuning of the classification head. [Model Card](https://arxiv.org/abs/1812.01187)                                                                                     |
| resnet_vd_101_imagenet                 | [ResNet](resnet)                       | 44.67M     | 101-layer ResNetVD (ResNet with bag of tricks) model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1812.01187)                                                                                                                                                                                   |
| resnet_vd_101_ssld_imagenet            | [ResNet](resnet)                       | 44.67M     | 101-layer ResNetVD (ResNet with bag of tricks) model pre-trained on the ImageNet 1k dataset at a 224x224 resolution with knowledge distillation. [Model Card](https://arxiv.org/abs/1812.01187)                                                                                                                                                       |
| resnet_vd_152_imagenet                 | [ResNet](resnet)                       | 60.36M     | 152-layer ResNetVD (ResNet with bag of tricks) model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1812.01187)                                                                                                                                                                                   |
| resnet_vd_200_imagenet                 | [ResNet](resnet)                       | 74.93M     | 200-layer ResNetVD (ResNet with bag of tricks) model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1812.01187)                                                                                                                                                                                   |
| mit_b0_ade20k_512                      | [MiT](mit)                             | 3.32M      | MiT (MixTransformer) model with 8 transformer blocks.                                                                                                                                                                                                                                                                                                 |
| mit_b1_ade20k_512                      | [MiT](mit)                             | 13.16M     | MiT (MixTransformer) model with 8 transformer blocks.                                                                                                                                                                                                                                                                                                 |
| mit_b2_ade20k_512                      | [MiT](mit)                             | 24.20M     | MiT (MixTransformer) model with 16 transformer blocks.                                                                                                                                                                                                                                                                                                |
| mit_b3_ade20k_512                      | [MiT](mit)                             | 44.08M     | MiT (MixTransformer) model with 28 transformer blocks.                                                                                                                                                                                                                                                                                                |
| mit_b4_ade20k_512                      | [MiT](mit)                             | 60.85M     | MiT (MixTransformer) model with 41 transformer blocks.                                                                                                                                                                                                                                                                                                |
| mit_b5_ade20k_640                      | [MiT](mit)                             | 81.45M     | MiT (MixTransformer) model with 52 transformer blocks.                                                                                                                                                                                                                                                                                                |
| mit_b0_cityscapes_1024                 | [MiT](mit)                             | 3.32M      | MiT (MixTransformer) model with 8 transformer blocks.                                                                                                                                                                                                                                                                                                 |
| mit_b1_cityscapes_1024                 | [MiT](mit)                             | 13.16M     | MiT (MixTransformer) model with 8 transformer blocks.                                                                                                                                                                                                                                                                                                 |
| mit_b2_cityscapes_1024                 | [MiT](mit)                             | 24.20M     | MiT (MixTransformer) model with 16 transformer blocks.                                                                                                                                                                                                                                                                                                |
| mit_b3_cityscapes_1024                 | [MiT](mit)                             | 44.08M     | MiT (MixTransformer) model with 28 transformer blocks.                                                                                                                                                                                                                                                                                                |
| mit_b4_cityscapes_1024                 | [MiT](mit)                             | 60.85M     | MiT (MixTransformer) model with 41 transformer blocks.                                                                                                                                                                                                                                                                                                |
| mit_b5_cityscapes_1024                 | [MiT](mit)                             | 81.45M     | MiT (MixTransformer) model with 52 transformer blocks.                                                                                                                                                                                                                                                                                                |
| gemma_2b_en                            | [Gemma](gemma)                         | 2.51B      | 2 billion parameter, 18-layer, base Gemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                             |
| gemma_instruct_2b_en                   | [Gemma](gemma)                         | 2.51B      | 2 billion parameter, 18-layer, instruction tuned Gemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                |
| gemma_1.1_instruct_2b_en               | [Gemma](gemma)                         | 2.51B      | 2 billion parameter, 18-layer, instruction tuned Gemma model. The 1.1 update improves model quality. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                         |
| code_gemma_1.1_2b_en                   | [Gemma](gemma)                         | 2.51B      | 2 billion parameter, 18-layer, CodeGemma model. This model has been trained on a fill-in-the-middle (FIM) task for code completion. The 1.1 update improves model quality. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                   |
| code_gemma_2b_en                       | [Gemma](gemma)                         | 2.51B      | 2 billion parameter, 18-layer, CodeGemma model. This model has been trained on a fill-in-the-middle (FIM) task for code completion. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                          |
| gemma_7b_en                            | [Gemma](gemma)                         | 8.54B      | 7 billion parameter, 28-layer, base Gemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                             |
| gemma_instruct_7b_en                   | [Gemma](gemma)                         | 8.54B      | 7 billion parameter, 28-layer, instruction tuned Gemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                |
| gemma_1.1_instruct_7b_en               | [Gemma](gemma)                         | 8.54B      | 7 billion parameter, 28-layer, instruction tuned Gemma model. The 1.1 update improves model quality. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                         |
| code_gemma_7b_en                       | [Gemma](gemma)                         | 8.54B      | 7 billion parameter, 28-layer, CodeGemma model. This model has been trained on a fill-in-the-middle (FIM) task for code completion. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                          |
| code_gemma_instruct_7b_en              | [Gemma](gemma)                         | 8.54B      | 7 billion parameter, 28-layer, instruction tuned CodeGemma model. This model has been trained for chat use cases related to code. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                            |
| code_gemma_1.1_instruct_7b_en          | [Gemma](gemma)                         | 8.54B      | 7 billion parameter, 28-layer, instruction tuned CodeGemma model. This model has been trained for chat use cases related to code. The 1.1 update improves model quality. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                     |
| gemma2_2b_en                           | [Gemma](gemma)                         | 2.61B      | 2 billion parameter, 26-layer, base Gemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                             |
| gemma2_instruct_2b_en                  | [Gemma](gemma)                         | 2.61B      | 2 billion parameter, 26-layer, instruction tuned Gemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                |
| gemma2_9b_en                           | [Gemma](gemma)                         | 9.24B      | 9 billion parameter, 42-layer, base Gemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                             |
| gemma2_instruct_9b_en                  | [Gemma](gemma)                         | 9.24B      | 9 billion parameter, 42-layer, instruction tuned Gemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                |
| gemma2_27b_en                          | [Gemma](gemma)                         | 27.23B     | 27 billion parameter, 42-layer, base Gemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                            |
| gemma2_instruct_27b_en                 | [Gemma](gemma)                         | 27.23B     | 27 billion parameter, 42-layer, instruction tuned Gemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                               |
| shieldgemma_2b_en                      | [Gemma](gemma)                         | 2.61B      | 2 billion parameter, 26-layer, ShieldGemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                            |
| shieldgemma_9b_en                      | [Gemma](gemma)                         | 9.24B      | 9 billion parameter, 42-layer, ShieldGemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                            |
| shieldgemma_27b_en                     | [Gemma](gemma)                         | 27.23B     | 27 billion parameter, 42-layer, ShieldGemma model. [Model Card](https://www.kaggle.com/models/google/gemma)                                                                                                                                                                                                                                           |
| gpt2_base_en                           | [GPT-2](gpt2)                          | 124.44M    | 12-layer GPT-2 model where case is maintained. Trained on WebText. [Model Card](https://github.com/openai/gpt-2/blob/master/model_card.md)                                                                                                                                                                                                            |
| gpt2_medium_en                         | [GPT-2](gpt2)                          | 354.82M    | 24-layer GPT-2 model where case is maintained. Trained on WebText. [Model Card](https://github.com/openai/gpt-2/blob/master/model_card.md)                                                                                                                                                                                                            |
| gpt2_large_en                          | [GPT-2](gpt2)                          | 774.03M    | 36-layer GPT-2 model where case is maintained. Trained on WebText. [Model Card](https://github.com/openai/gpt-2/blob/master/model_card.md)                                                                                                                                                                                                            |
| gpt2_extra_large_en                    | [GPT-2](gpt2)                          | 1.56B      | 48-layer GPT-2 model where case is maintained. Trained on WebText. [Model Card](https://github.com/openai/gpt-2/blob/master/model_card.md)                                                                                                                                                                                                            |
| gpt2_base_en_cnn_dailymail             | [GPT-2](gpt2)                          | 124.44M    | 12-layer GPT-2 model where case is maintained. Finetuned on the CNN/DailyMail summarization dataset.                                                                                                                                                                                                                                                  |
| llama3_8b_en                           | [LLaMA 3](llama3)                      | 8.03B      | 8 billion parameter, 32-layer, base LLaMA 3 model. [Model Card](https://github.com/meta-llama/llama3)                                                                                                                                                                                                                                                 |
| llama3_8b_en_int8                      | [LLaMA 3](llama3)                      | 8.03B      | 8 billion parameter, 32-layer, base LLaMA 3 model with activation and weights quantized to int8. [Model Card](https://github.com/meta-llama/llama3)                                                                                                                                                                                                   |
| llama3_instruct_8b_en                  | [LLaMA 3](llama3)                      | 8.03B      | 8 billion parameter, 32-layer, instruction tuned LLaMA 3 model. [Model Card](https://github.com/meta-llama/llama3)                                                                                                                                                                                                                                    |
| llama3_instruct_8b_en_int8             | [LLaMA 3](llama3)                      | 8.03B      | 8 billion parameter, 32-layer, instruction tuned LLaMA 3 model with activation and weights quantized to int8. [Model Card](https://github.com/meta-llama/llama3)                                                                                                                                                                                      |
| llama2_7b_en                           | [LLaMA 2](llama2)                      | 6.74B      | 7 billion parameter, 32-layer, base LLaMA 2 model. [Model Card](https://github.com/meta-llama/llama)                                                                                                                                                                                                                                                  |
| llama2_7b_en_int8                      | [LLaMA 2](llama2)                      | 6.74B      | 7 billion parameter, 32-layer, base LLaMA 2 model with activation and weights quantized to int8. [Model Card](https://github.com/meta-llama/llama)                                                                                                                                                                                                    |
| llama2_instruct_7b_en                  | [LLaMA 2](llama2)                      | 6.74B      | 7 billion parameter, 32-layer, instruction tuned LLaMA 2 model. [Model Card](https://github.com/meta-llama/llama)                                                                                                                                                                                                                                     |
| llama2_instruct_7b_en_int8             | [LLaMA 2](llama2)                      | 6.74B      | 7 billion parameter, 32-layer, instruction tuned LLaMA 2 model with activation and weights quantized to int8. [Model Card](https://github.com/meta-llama/llama)                                                                                                                                                                                       |
| vicuna_1.5_7b_en                       | [Vicuna](vicuna)                       | 6.74B      | 7 billion parameter, 32-layer, instruction tuned Vicuna v1.5 model. [Model Card](https://github.com/lm-sys/FastChat)                                                                                                                                                                                                                                  |
| mistral_7b_en                          | [Mistral](mistral)                     | 7.24B      | Mistral 7B base model [Model Card](https://github.com/mistralai/mistral-src/blob/main/README.md)                                                                                                                                                                                                                                                      |
| mistral_instruct_7b_en                 | [Mistral](mistral)                     | 7.24B      | Mistral 7B instruct model [Model Card](https://github.com/mistralai/mistral-src/blob/main/README.md)                                                                                                                                                                                                                                                  |
| mistral_0.2_instruct_7b_en             | [Mistral](mistral)                     | 7.24B      | Mistral 7B instruct Version 0.2 model [Model Card](https://github.com/mistralai/mistral-src/blob/main/README.md)                                                                                                                                                                                                                                      |
| opt_125m_en                            | [OPT](opt)                             | 125.24M    | 12-layer OPT model where case in maintained. Trained on BookCorpus, CommonCrawl, Pile, and PushShift.io corpora. [Model Card](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/model_card.md)                                                                                                                                       |
| opt_1.3b_en                            | [OPT](opt)                             | 1.32B      | 24-layer OPT model where case in maintained. Trained on BookCorpus, CommonCrawl, Pile, and PushShift.io corpora. [Model Card](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/model_card.md)                                                                                                                                       |
| opt_2.7b_en                            | [OPT](opt)                             | 2.70B      | 32-layer OPT model where case in maintained. Trained on BookCorpus, CommonCrawl, Pile, and PushShift.io corpora. [Model Card](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/model_card.md)                                                                                                                                       |
| opt_6.7b_en                            | [OPT](opt)                             | 6.70B      | 32-layer OPT model where case in maintained. Trained on BookCorpus, CommonCrawl, Pile, and PushShift.io corpora. [Model Card](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/model_card.md)                                                                                                                                       |
| pali_gemma_3b_mix_224                  | [PaliGemma](pali_gemma)                | 2.92B      | image size 224, mix fine tuned, text sequence length is 256 [Model Card](https://www.kaggle.com/models/google/paligemma)                                                                                                                                                                                                                              |
| pali_gemma_3b_mix_448                  | [PaliGemma](pali_gemma)                | 2.92B      | image size 448, mix fine tuned, text sequence length is 512 [Model Card](https://www.kaggle.com/models/google/paligemma)                                                                                                                                                                                                                              |
| pali_gemma_3b_224                      | [PaliGemma](pali_gemma)                | 2.92B      | image size 224, pre trained, text sequence length is 128 [Model Card](https://www.kaggle.com/models/google/paligemma)                                                                                                                                                                                                                                 |
| pali_gemma_3b_448                      | [PaliGemma](pali_gemma)                | 2.92B      | image size 448, pre trained, text sequence length is 512 [Model Card](https://www.kaggle.com/models/google/paligemma)                                                                                                                                                                                                                                 |
| pali_gemma_3b_896                      | [PaliGemma](pali_gemma)                | 2.93B      | image size 896, pre trained, text sequence length is 512 [Model Card](https://www.kaggle.com/models/google/paligemma)                                                                                                                                                                                                                                 |
| phi3_mini_4k_instruct_en               | [Phi-3](phi3)                          | 3.82B      | 3.8 billion parameters, 32 layers, 4k context length, Phi-3 model. The model was trained using the Phi-3 datasets. This dataset includes both synthetic data and filtered publicly available website data, with an emphasis on high-quality and reasoning-dense properties. [Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)     |
| phi3_mini_128k_instruct_en             | [Phi-3](phi3)                          | 3.82B      | 3.8 billion parameters, 32 layers, 128k context length, Phi-3 model. The model was trained using the Phi-3 datasets. This dataset includes both synthetic data and filtered publicly available website data, with an emphasis on high-quality and reasoning-dense properties. [Model Card](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) |
| roberta_base_en                        | [RoBERTa](roberta)                     | 124.05M    | 12-layer RoBERTa model where case is maintained.Trained on English Wikipedia, BooksCorpus, CommonCraw, and OpenWebText. [Model Card](https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md)                                                                                                                                |
| roberta_large_en                       | [RoBERTa](roberta)                     | 354.31M    | 24-layer RoBERTa model where case is maintained.Trained on English Wikipedia, BooksCorpus, CommonCraw, and OpenWebText. [Model Card](https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md)                                                                                                                                |
| xlm_roberta_base_multi                 | [XLM-RoBERTa](xlm_roberta)             | 277.45M    | 12-layer XLM-RoBERTa model where case is maintained. Trained on CommonCrawl in 100 languages. [Model Card](https://github.com/facebookresearch/fairseq/blob/main/examples/xlmr/README.md)                                                                                                                                                             |
| xlm_roberta_large_multi                | [XLM-RoBERTa](xlm_roberta)             | 558.84M    | 24-layer XLM-RoBERTa model where case is maintained. Trained on CommonCrawl in 100 languages. [Model Card](https://github.com/facebookresearch/fairseq/blob/main/examples/xlmr/README.md)                                                                                                                                                             |
| sam_base_sa1b                          | [SAMImageSegmenter](sam)               | 93.74M     | The base SAM model trained on the SA1B dataset. [Model Card](https://arxiv.org/abs/2304.02643)                                                                                                                                                                                                                                                        |
| sam_large_sa1b                         | [SAMImageSegmenter](sam)               | 641.09M    | The large SAM model trained on the SA1B dataset. [Model Card](https://arxiv.org/abs/2304.02643)                                                                                                                                                                                                                                                       |
| sam_huge_sa1b                          | [SAMImageSegmenter](sam)               | 312.34M    | The huge SAM model trained on the SA1B dataset. [Model Card](https://arxiv.org/abs/2304.02643)                                                                                                                                                                                                                                                        |
| stable_diffusion_3_medium              | [StableDiffusion3](stable_diffusion_3) | 2.99B      | 3 billion parameter, including CLIP L and CLIP G text encoders, MMDiT generative model, and VAE autoencoder. Developed by Stability AI. [Model Card](https://arxiv.org/abs/2110.00476)                                                                                                                                                                |
| t5_small_multi                         | [T5](t5)                               | 0          | 8-layer T5 model. Trained on the Colossal Clean Crawled Corpus (C4). [Model Card](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.md)                                                                                                                                                                           |
| t5_base_multi                          | [T5](t5)                               | 0          | 12-layer T5 model. Trained on the Colossal Clean Crawled Corpus (C4). [Model Card](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.md)                                                                                                                                                                          |
| t5_large_multi                         | [T5](t5)                               | 0          | 24-layer T5 model. Trained on the Colossal Clean Crawled Corpus (C4). [Model Card](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.md)                                                                                                                                                                          |
| flan_small_multi                       | [T5](t5)                               | 0          | 8-layer T5 model. Trained on the Colossal Clean Crawled Corpus (C4). [Model Card](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.md)                                                                                                                                                                           |
| flan_base_multi                        | [T5](t5)                               | 0          | 12-layer T5 model. Trained on the Colossal Clean Crawled Corpus (C4). [Model Card](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.md)                                                                                                                                                                          |
| flan_large_multi                       | [T5](t5)                               | 0          | 24-layer T5 model. Trained on the Colossal Clean Crawled Corpus (C4). [Model Card](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.md)                                                                                                                                                                          |
| vgg_11_imagenet                        | <vgg>                                  | 9.22M      | 11-layer vgg model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1409.1556)                                                                                                                                                                                                                      |
| vgg_13_imagenet                        | <vgg>                                  | 9.40M      | 13-layer vgg model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1409.1556)                                                                                                                                                                                                                      |
| vgg_16_imagenet                        | <vgg>                                  | 14.71M     | 16-layer vgg model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1409.1556)                                                                                                                                                                                                                      |
| vgg_19_imagenet                        | <vgg>                                  | 20.02M     | 19-layer vgg model pre-trained on the ImageNet 1k dataset at a 224x224 resolution. [Model Card](https://arxiv.org/abs/1409.1556)                                                                                                                                                                                                                      |
| whisper_tiny_en                        | [Whisper](whisper)                     | 37.18M     | 4-layer Whisper model. Trained on 438,000 hours of labelled English speech data. [Model Card](https://github.com/openai/whisper/blob/main/model-card.md)                                                                                                                                                                                              |
| whisper_base_en                        | [Whisper](whisper)                     | 124.44M    | 6-layer Whisper model. Trained on 438,000 hours of labelled English speech data. [Model Card](https://github.com/openai/whisper/blob/main/model-card.md)                                                                                                                                                                                              |
| whisper_small_en                       | [Whisper](whisper)                     | 241.73M    | 12-layer Whisper model. Trained on 438,000 hours of labelled English speech data. [Model Card](https://github.com/openai/whisper/blob/main/model-card.md)                                                                                                                                                                                             |
| whisper_medium_en                      | [Whisper](whisper)                     | 763.86M    | 24-layer Whisper model. Trained on 438,000 hours of labelled English speech data. [Model Card](https://github.com/openai/whisper/blob/main/model-card.md)                                                                                                                                                                                             |
| whisper_tiny_multi                     | [Whisper](whisper)                     | 37.76M     | 4-layer Whisper model. Trained on 680,000 hours of labelled multilingual speech data. [Model Card](https://github.com/openai/whisper/blob/main/model-card.md)                                                                                                                                                                                         |
| whisper_base_multi                     | [Whisper](whisper)                     | 72.59M     | 6-layer Whisper model. Trained on 680,000 hours of labelled multilingual speech data. [Model Card](https://github.com/openai/whisper/blob/main/model-card.md)                                                                                                                                                                                         |
| whisper_small_multi                    | [Whisper](whisper)                     | 241.73M    | 12-layer Whisper model. Trained on 680,000 hours of labelled multilingual speech data. [Model Card](https://github.com/openai/whisper/blob/main/model-card.md)                                                                                                                                                                                        |
| whisper_medium_multi                   | [Whisper](whisper)                     | 763.86M    | 24-layer Whisper model. Trained on 680,000 hours of labelled multilingual speech data. [Model Card](https://github.com/openai/whisper/blob/main/model-card.md)                                                                                                                                                                                        |
| whisper_large_multi                    | [Whisper](whisper)                     | 1.54B      | 32-layer Whisper model. Trained on 680,000 hours of labelled multilingual speech data. [Model Card](https://github.com/openai/whisper/blob/main/model-card.md)                                                                                                                                                                                        |
| whisper_large_multi_v2                 | [Whisper](whisper)                     | 1.54B      | 32-layer Whisper model. Trained for 2.5 epochs on 680,000 hours of labelled multilingual speech data. An improved of `whisper_large_multi`. [Model Card](https://github.com/openai/whisper/blob/main/model-card.md)                                                                                                                                   |

**Note**: The links provided will lead to the model card or to the official README,
if no model card has been provided by the author.

## API Documentation

### [Albert]({{< relref "/docs/api/keras_hub/models/albert/" >}})

- [AlbertTokenizer]({{< relref "/docs/api/keras_hub/models/albert/albert_tokenizer" >}})
- [AlbertBackbone model]({{< relref "/docs/api/keras_hub/models/albert/albert_backbone" >}})
- [AlbertTextClassifier model]({{< relref "/docs/api/keras_hub/models/albert/albert_text_classifier" >}})
- [AlbertTextClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/albert/albert_text_classifier_preprocessor" >}})
- [AlbertMaskedLM model]({{< relref "/docs/api/keras_hub/models/albert/albert_masked_lm" >}})
- [AlbertMaskedLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/albert/albert_masked_lm_preprocessor" >}})

### [Bart]({{< relref "/docs/api/keras_hub/models/bart/" >}})

- [BertTokenizer]({{< relref "/docs/api/keras_hub/models/bart/bart_tokenizer" >}})
- [BertBackbone model]({{< relref "/docs/api/keras_hub/models/bart/bart_backbone" >}})
- [BartSeq2SeqLM model]({{< relref "/docs/api/keras_hub/models/bart/bart_seq_2_seq_lm" >}})
- [BartSeq2SeqLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/bart/bart_seq_2_seq_lm_preprocessor" >}})

### [Bert]({{< relref "/docs/api/keras_hub/models/bert/" >}})

- [BertTokenizer]({{< relref "/docs/api/keras_hub/models/bert/bert_tokenizer" >}})
- [BertBackbone model]({{< relref "/docs/api/keras_hub/models/bert/bert_backbone" >}})
- [BertTextClassifier model]({{< relref "/docs/api/keras_hub/models/bert/bert_text_classifier" >}})
- [BertTextClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/bert/bert_text_classifier_preprocessor" >}})
- [BertMaskedLM model]({{< relref "/docs/api/keras_hub/models/bert/bert_masked_lm" >}})
- [BertMaskedLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/bert/bert_masked_lm_preprocessor" >}})

### [Bloom]({{< relref "/docs/api/keras_hub/models/bloom/" >}})

- [BloomTokenizer]({{< relref "/docs/api/keras_hub/models/bloom/bloom_tokenizer" >}})
- [BloomBackbone model]({{< relref "/docs/api/keras_hub/models/bloom/bloom_backbone" >}})
- [BloomCausalLM model]({{< relref "/docs/api/keras_hub/models/bloom/bloom_causal_lm" >}})
- [BloomCausalLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/bloom/bloom_causal_lm_preprocessor" >}})

### [DebertaV3]({{< relref "/docs/api/keras_hub/models/deberta_v3/" >}})

- [DebertaV3Tokenizer]({{< relref "/docs/api/keras_hub/models/deberta_v3/deberta_v3_tokenizer" >}})
- [DebertaV3Backbone model]({{< relref "/docs/api/keras_hub/models/deberta_v3/deberta_v3_backbone" >}})
- [DebertaV3TextClassifier model]({{< relref "/docs/api/keras_hub/models/deberta_v3/deberta_v3_text_classifier" >}})
- [DebertaV3TextClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/deberta_v3/deberta_v3_text_classifier_preprocessor" >}})
- [DebertaV3MaskedLM model]({{< relref "/docs/api/keras_hub/models/deberta_v3/deberta_v3_masked_lm" >}})
- [DebertaV3MaskedLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/deberta_v3/deberta_v3_masked_lm_preprocessor" >}})

### [DeepLabV3 and DeepLabV3Plus]({{< relref "/docs/api/keras_hub/models/deeplab_v3/" >}})

- [DeepLabV3ImageConverter]({{< relref "/docs/api/keras_hub/models/deeplab_v3/deeplab_v3_image_converter" >}})
- [DeepLabV3Backbone model]({{< relref "/docs/api/keras_hub/models/deeplab_v3/deeplab_v3_backbone" >}})
- [DeepLabV3ImageSegmenter model]({{< relref "/docs/api/keras_hub/models/deeplab_v3/deeplab_v3_image_segmenter" >}})
- [DeepLabV3ImageSegmenterPreprocessor layer]({{< relref "/docs/api/keras_hub/models/deeplab_v3/deeplab_v3_image_segmenter_preprocessor" >}})

### [DenseNet]({{< relref "/docs/api/keras_hub/models/densenet/" >}})

- [DenseNetImageConverter]({{< relref "/docs/api/keras_hub/models/densenet/densenet_image_converter" >}})
- [DensNetBackbone model]({{< relref "/docs/api/keras_hub/models/densenet/densenet_backbone" >}})
- [DenseNetImageClassifier model]({{< relref "/docs/api/keras_hub/models/densenet/densenet_image_classifier" >}})
- [DenseNetImageClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/densenet/densenet_image_classifier_preprocessor" >}})

### [DistilBert]({{< relref "/docs/api/keras_hub/models/distil_bert/" >}})

- [DistilBertTokenizer]({{< relref "/docs/api/keras_hub/models/distil_bert/distil_bert_tokenizer" >}})
- [DistilBertBackbone model]({{< relref "/docs/api/keras_hub/models/distil_bert/distil_bert_backbone" >}})
- [DistilBertTextClassifier model]({{< relref "/docs/api/keras_hub/models/distil_bert/distil_bert_text_classifier" >}})
- [DistilBertTextClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/distil_bert/distil_bert_text_classifier_preprocessor" >}})
- [DistilBertMaskedLM model]({{< relref "/docs/api/keras_hub/models/distil_bert/distil_bert_masked_lm" >}})
- [DistilBertMaskedLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/distil_bert/distil_bert_masked_lm_preprocessor" >}})

### [Electra]({{< relref "/docs/api/keras_hub/models/electra/" >}})

- [ElectraTokenizer]({{< relref "/docs/api/keras_hub/models/electra/electra_tokenizer" >}})
- [ElectraBackbone model]({{< relref "/docs/api/keras_hub/models/electra/electra_backbone" >}})

### [Falcon]({{< relref "/docs/api/keras_hub/models/falcon/" >}})

- [FalconTokenizer]({{< relref "/docs/api/keras_hub/models/falcon/falcon_tokenizer" >}})
- [FalconBackbone model]({{< relref "/docs/api/keras_hub/models/falcon/falcon_backbone" >}})
- [FalconCausalLM model]({{< relref "/docs/api/keras_hub/models/falcon/falcon_causal_lm" >}})
- [FalconCausalLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/falcon/falcon_causal_lm_preprocessor" >}})

### [FNet]({{< relref "/docs/api/keras_hub/models/f_net/" >}})

- [FNetTokenizer]({{< relref "/docs/api/keras_hub/models/f_net/f_net_tokenizer" >}})
- [FNetBackbone model]({{< relref "/docs/api/keras_hub/models/f_net/f_net_backbone" >}})
- [FNetTextClassifier model]({{< relref "/docs/api/keras_hub/models/f_net/f_net_text_classifier" >}})
- [FNetTextClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/f_net/f_net_text_classifier_preprocessor" >}})
- [FNetMaskedLM model]({{< relref "/docs/api/keras_hub/models/f_net/f_net_masked_lm" >}})
- [FNetMaskedLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/f_net/f_net_masked_lm_preprocessor" >}})

### [Gemma]({{< relref "/docs/api/keras_hub/models/gemma/" >}})

- [GemmaTokenizer]({{< relref "/docs/api/keras_hub/models/gemma/gemma_tokenizer" >}})
- [GemmaBackbone model]({{< relref "/docs/api/keras_hub/models/gemma/gemma_backbone" >}})
- [GemmaCausalLM model]({{< relref "/docs/api/keras_hub/models/gemma/gemma_causal_lm" >}})
- [GemmaCausalLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/gemma/gemma_causal_lm_preprocessor" >}})

### [GPT2]({{< relref "/docs/api/keras_hub/models/gpt2/" >}})

- [GPT2Tokenizer]({{< relref "/docs/api/keras_hub/models/gpt2/gpt2_tokenizer" >}})
- [GPT2Backbone model]({{< relref "/docs/api/keras_hub/models/gpt2/gpt2_backbone" >}})
- [GPT2CausalLM model]({{< relref "/docs/api/keras_hub/models/gpt2/gpt2_causal_lm" >}})
- [GPT2CausalLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/gpt2/gpt2_causal_lm_preprocessor" >}})

### [Llama]({{< relref "/docs/api/keras_hub/models/llama/" >}})

- [LlamaTokenizer]({{< relref "/docs/api/keras_hub/models/llama/llama_tokenizer" >}})
- [LlamaBackbone model]({{< relref "/docs/api/keras_hub/models/llama/llama_backbone" >}})
- [LlamaCausalLM model]({{< relref "/docs/api/keras_hub/models/llama/llama_causal_lm" >}})
- [LlamaCausalLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/llama/llama_causal_lm_preprocessor" >}})

### [Llama3]({{< relref "/docs/api/keras_hub/models/llama3/" >}})

- [Llama3Tokenizer]({{< relref "/docs/api/keras_hub/models/llama3/llama3_tokenizer" >}})
- [Llama3Backbone model]({{< relref "/docs/api/keras_hub/models/llama3/llama3_backbone" >}})
- [Llama3CausalLM model]({{< relref "/docs/api/keras_hub/models/llama3/llama3_causal_lm" >}})
- [Llama3CausalLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/llama3/llama3_causal_lm_preprocessor" >}})

### [Mistral]({{< relref "/docs/api/keras_hub/models/mistral/" >}})

- [MistralTokenizer]({{< relref "/docs/api/keras_hub/models/mistral/mistral_tokenizer" >}})
- [MistralBackbone model]({{< relref "/docs/api/keras_hub/models/mistral/mistral_backbone" >}})
- [MistralCausalLM model]({{< relref "/docs/api/keras_hub/models/mistral/mistral_causal_lm" >}})
- [MistralCausalLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/mistral/mistral_causal_lm_preprocessor" >}})

### [MiT]({{< relref "/docs/api/keras_hub/models/mit/" >}})

- [MiTImageConverter]({{< relref "/docs/api/keras_hub/models/mit/mit_image_converter" >}})
- [MiTBackbone model]({{< relref "/docs/api/keras_hub/models/mit/mit_backbone" >}})
- [MiTImageClassifier model]({{< relref "/docs/api/keras_hub/models/mit/mit_image_classifier" >}})
- [MiTImageClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/mit/mit_image_classifier_preprocessor" >}})

### [OPT]({{< relref "/docs/api/keras_hub/models/opt/" >}})

- [OPTTokenizer]({{< relref "/docs/api/keras_hub/models/opt/opt_tokenizer" >}})
- [OPTBackbone model]({{< relref "/docs/api/keras_hub/models/opt/opt_backbone" >}})
- [OPTCausalLM model]({{< relref "/docs/api/keras_hub/models/opt/opt_causal_lm" >}})
- [OPTCausalLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/opt/opt_causal_lm_preprocessor" >}})

### [PaliGemma]({{< relref "/docs/api/keras_hub/models/pali_gemma/" >}})

- [PaliGemmaTokenizer]({{< relref "/docs/api/keras_hub/models/pali_gemma/pali_gemma_tokenizer" >}})
- [PaliGemmaBackbone model]({{< relref "/docs/api/keras_hub/models/pali_gemma/pali_gemma_backbone" >}})
- [PaliGemmaCausalLM model]({{< relref "/docs/api/keras_hub/models/pali_gemma/pali_gemma_causal_lm" >}})
- [PaliGemmaCausalLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/pali_gemma/pali_gemma_causal_lm_preprocessor" >}})

### [Phi3]({{< relref "/docs/api/keras_hub/models/phi3/" >}})

- [Phi3Tokenizer]({{< relref "/docs/api/keras_hub/models/phi3/phi3_tokenizer" >}})
- [Phi3Backbone model]({{< relref "/docs/api/keras_hub/models/phi3/phi3_backbone" >}})
- [Phi3CausalLM model]({{< relref "/docs/api/keras_hub/models/phi3/phi3_causal_lm" >}})
- [Phi3CausalLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/phi3/phi3_causal_lm_preprocessor" >}})

### [ResNet]({{< relref "/docs/api/keras_hub/models/resnet/" >}})

- [ResNetImageConverter]({{< relref "/docs/api/keras_hub/models/resnet/resnet_image_converter" >}})
- [ResNetBackbone model]({{< relref "/docs/api/keras_hub/models/resnet/resnet_backbone" >}})
- [ResNetImageClassifier model]({{< relref "/docs/api/keras_hub/models/resnet/resnet_image_classifier" >}})
- [ResNetImageClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/resnet/resnet_image_classifier_preprocessor" >}})

### [Roberta]({{< relref "/docs/api/keras_hub/models/roberta/" >}})

- [RobertaTokenizer]({{< relref "/docs/api/keras_hub/models/roberta/roberta_tokenizer" >}})
- [RobertaBackbone model]({{< relref "/docs/api/keras_hub/models/roberta/roberta_backbone" >}})
- [RobertaTextClassifier model]({{< relref "/docs/api/keras_hub/models/roberta/roberta_text_classifier" >}})
- [RobertaTextClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/roberta/roberta_text_classifier_preprocessor" >}})
- [RobertaMaskedLM model]({{< relref "/docs/api/keras_hub/models/roberta/roberta_masked_lm" >}})
- [RobertaMaskedLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/roberta/roberta_masked_lm_preprocessor" >}})

### [Segment Anything Model]({{< relref "/docs/api/keras_hub/models/sam/" >}})

- [SAMImageConverter]({{< relref "/docs/api/keras_hub/models/sam/sam_image_converter" >}})
- [SAMBackbone model]({{< relref "/docs/api/keras_hub/models/sam/sam_backbone" >}})
- [SAMImageSegmenter model]({{< relref "/docs/api/keras_hub/models/sam/sam_image_segmenter" >}})
- [SAMImageSegmenterPreprocessor layer]({{< relref "/docs/api/keras_hub/models/sam/sam_image_segmenter_preprocessor" >}})
- [SAMPromptEncoder layer]({{< relref "/docs/api/keras_hub/models/sam/sam_prompt_encoder" >}})
- [SAMMaskDecoder layer]({{< relref "/docs/api/keras_hub/models/sam/sam_mask_decoder" >}})

### [Stable Diffusion 3]({{< relref "/docs/api/keras_hub/models/stable_diffusion_3/" >}})

- [SAMImageConverter]({{< relref "/docs/api/keras_hub/models/stable_diffusion_3/sam_image_converter" >}})
- [StableDiffusion3Backbone model]({{< relref "/docs/api/keras_hub/models/stable_diffusion_3/stable_diffusion_3_backbone" >}})
- [StableDiffusion3TextToImage model]({{< relref "/docs/api/keras_hub/models/stable_diffusion_3/stable_diffusion_3_text_to_image" >}})
- [StableDiffusion3TextToImagePreprocessor layer]({{< relref "/docs/api/keras_hub/models/stable_diffusion_3/stable_diffusion_3_text_to_image_preprocessor" >}})
- [StableDiffusion3ImageToImage model]({{< relref "/docs/api/keras_hub/models/stable_diffusion_3/stable_diffusion_3_image_to_image" >}})
- [StableDiffusion3Inpaint model]({{< relref "/docs/api/keras_hub/models/stable_diffusion_3/stable_diffusion_3_inpaint" >}})

### [T5]({{< relref "/docs/api/keras_hub/models/t5/" >}})

- [T5Tokenizer]({{< relref "/docs/api/keras_hub/models/t5/t5_tokenizer" >}})
- [T5Backbone model]({{< relref "/docs/api/keras_hub/models/t5/t5_backbone" >}})
- [T5Preprocessor layer]({{< relref "/docs/api/keras_hub/models/t5/t5_preprocessor" >}})

### [VGG]({{< relref "/docs/api/keras_hub/models/vgg/" >}})

- [VGGImageConverter]({{< relref "/docs/api/keras_hub/models/vgg/vgg_image_converter" >}})
- [VGGBackbone model]({{< relref "/docs/api/keras_hub/models/vgg/vgg_backbone" >}})
- [VGGImageClassifier model]({{< relref "/docs/api/keras_hub/models/vgg/vgg_image_classifier" >}})
- [VGGImageClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/vgg/vgg_image_classifier_preprocessor" >}})

### [ViTDet]({{< relref "/docs/api/keras_hub/models/vit_det/" >}})

- [VitDet model]({{< relref "/docs/api/keras_hub/models/vit_det/ViTDetBackbone" >}})

### [Whisper]({{< relref "/docs/api/keras_hub/models/whisper/" >}})

- [WhisperTokenizer]({{< relref "/docs/api/keras_hub/models/whisper/whisper_tokenizer" >}})
- [WhisperAudioConverter]({{< relref "/docs/api/keras_hub/models/whisper/whisper_audio_converter" >}})
- [WhisperBackbone model]({{< relref "/docs/api/keras_hub/models/whisper/whisper_backbone" >}})

### [XLMRoberta]({{< relref "/docs/api/keras_hub/models/xlm_roberta/" >}})

- [XLMRobertaTokenizer]({{< relref "/docs/api/keras_hub/models/xlm_roberta/xlm_roberta_tokenizer" >}})
- [XLMRobertaBackbone model]({{< relref "/docs/api/keras_hub/models/xlm_roberta/xlm_roberta_backbone" >}})
- [XLMRobertaTextClassifier model]({{< relref "/docs/api/keras_hub/models/xlm_roberta/xlm_roberta_text_classifier" >}})
- [XLMRobertaTextClassifierPreprocessor layer]({{< relref "/docs/api/keras_hub/models/xlm_roberta/xlm_roberta_text_classifier_preprocessor" >}})
- [XLMRobertaMaskedLM model]({{< relref "/docs/api/keras_hub/models/xlm_roberta/xlm_roberta_masked_lm" >}})
- [XLMRobertaMaskedLMPreprocessor layer]({{< relref "/docs/api/keras_hub/models/xlm_roberta/xlm_roberta_masked_lm_preprocessor" >}})
