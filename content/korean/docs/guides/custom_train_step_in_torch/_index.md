---
title: PyTorchì—ì„œì˜ `fit()` ë™ì‘ì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ
linkTitle: PyTorchì—ì„œ fit() ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ
toc: true
weight: 7
type: docs
---

{{< keras/original checkedAt="2024-11-18" >}}

**{{< t f_author >}}** [fchollet](https://twitter.com/fchollet)  
**{{< t f_date_created >}}** 2023/06/27  
**{{< t f_last_modified >}}** 2024/08/01  
**{{< t f_description >}}** PyTorchì—ì„œ `Model` í´ë˜ìŠ¤ì˜ íŠ¸ë ˆì´ë‹ ìŠ¤í…ì„ ì¬ì •ì˜.

{{< cards cols="2" >}}
{{< card link="https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/custom_train_step_in_torch.ipynb" title="Colab" tag="Colab" tagType="warning">}}
{{< card link="https://github.com/keras-team/keras-io/blob/master/guides/custom_train_step_in_torch.py" title="GitHub" tag="GitHub">}}
{{< /cards >}}

## ì†Œê°œ {#introduction}

ì§€ë„ í•™ìŠµì„ í•  ë•ŒëŠ” `fit()`ì„ ì‚¬ìš©í•˜ë©´, ëª¨ë“  ê²ƒì´ ë§¤ë„ëŸ½ê²Œ ì‘ë™í•©ë‹ˆë‹¤.

í•˜ì§€ë§Œ ëª¨ë“  ì„¸ë¶€ ì‚¬í•­ì„ ì™„ì „íˆ ì œì–´í•´ì•¼ í•  ê²½ìš°,
ì²˜ìŒë¶€í„° ëê¹Œì§€ ì§ì ‘ ë‹¹ì‹ ë§Œì˜ íŠ¸ë ˆì´ë‹ ë£¨í”„ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ë ‡ì§€ë§Œ ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë‹ ì•Œê³ ë¦¬ì¦˜ì´ í•„ìš”í•˜ë©´ì„œë„,
ì½œë°±, ë¹ŒíŠ¸ì¸ ë¶„ì‚° ì§€ì›, ìŠ¤í… í“¨ì§•(step fusing)ê³¼ ê°™ì€,
`fit()`ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ì„ ê·¸ëŒ€ë¡œ í™œìš©í•˜ê³  ì‹¶ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?

Kerasì˜ í•µì‹¬ ì›ì¹™ ì¤‘ í•˜ë‚˜ëŠ” **ì ì§„ì ì¸ ë³µì¡ì„± ê³µê°œ**ì…ë‹ˆë‹¤.
í•­ìƒ ì ì§„ì ìœ¼ë¡œ ë” ë‚®ì€ ë ˆë²¨ì˜ ì›Œí¬í”Œë¡œë¡œ ì§„ì…í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
ë†’ì€ ë ˆë²¨ì˜ ê¸°ëŠ¥ì´ ì •í™•íˆ ì‚¬ìš© ì‚¬ë¡€ì— ë§ì§€ ì•Šë”ë¼ë„, ê°‘ì‘ìŠ¤ëŸ½ê²Œ ì–´ë ¤ì›€ì— ë¶€ë”ªí˜€ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
ë†’ì€ ë ˆë²¨ì˜ í¸ë¦¬í•¨ì„ ìœ ì§€í•˜ë©´ì„œ, ì‘ì€ ì„¸ë¶€ ì‚¬í•­ì— ëŒ€í•œ ì œì–´ ê¶Œí•œì„ ë” ë§ì´ ê°€ì§ˆ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

`fit()`ì´ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•´ì•¼ í•  ë•ŒëŠ”,
**`Model` í´ë˜ìŠ¤ì˜ íŠ¸ë ˆì´ë‹ ìŠ¤í… í•¨ìˆ˜ë¥¼ ì¬ì •ì˜í•´ì•¼** í•©ë‹ˆë‹¤.
ì´ í•¨ìˆ˜ëŠ” `fit()`ì´ ê° ë°ì´í„° ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
ì´ë ‡ê²Œ í•˜ë©´, í‰ì†Œì™€ ê°™ì´ `fit()`ì„ í˜¸ì¶œí•  ìˆ˜ ìˆìœ¼ë©°,
ê·¸ ì•ˆì—ì„œ ì‚¬ìš©ìê°€ ì •ì˜í•œ íŠ¸ë ˆì´ë‹ ì•Œê³ ë¦¬ì¦˜ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.

ì´ íŒ¨í„´ì€ í•¨ìˆ˜í˜• APIë¡œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì„ ë°©í•´í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì— ì£¼ì˜í•˜ì„¸ìš”.
`Sequential` ëª¨ë¸, Functional API ëª¨ë¸,
ë˜ëŠ” ì„œë¸Œí´ë˜ì‹±í•œ ëª¨ë¸ì„ ë§Œë“¤ ë•Œë„ ì´ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì œ ê·¸ ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## ì…‹ì—… {#setup}

```python
import os

# ì´ ê°€ì´ë“œëŠ” torch ë°±ì—”ë“œì—ì„œë§Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
os.environ["KERAS_BACKEND"] = "torch"

import torch
import keras
from keras import layers
import numpy as np
```

## ì²« ë²ˆì§¸ ê°„ë‹¨í•œ ì˜ˆì œ {#a-first-simple-example}

ê°„ë‹¨í•œ ì˜ˆì œë¶€í„° ì‹œì‘í•´ë´…ì‹œë‹¤:

- [`keras.Model`]({{< relref "/docs/api/models/model#model-class" >}})ë¥¼ ì„œë¸Œí´ë˜ì‹±í•˜ëŠ” ìƒˆë¡œìš´ í´ë˜ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ë©”ì„œë“œ `train_step(self, data)`ë§Œ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.
- ë©”íŠ¸ë¦­ ì´ë¦„(ì†ì‹¤ì„ í¬í•¨í•˜ì—¬)ì„ í˜„ì¬ ê°’ì— ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

ì…ë ¥ ì¸ì `data`ëŠ” íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¡œì„œ `fit`ì— ì „ë‹¬ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

- `fit(x, y, ...)`ë¥¼ í˜¸ì¶œí•˜ì—¬ NumPy ë°°ì—´ì„ ì „ë‹¬í•˜ë©´,
  `data`ëŠ” íŠœí”Œ `(x, y)`ê°€ ë©ë‹ˆë‹¤.
- `fit(dataset, ...)`ë¥¼ í˜¸ì¶œí•˜ì—¬ `torch.utils.data.DataLoader` ë˜ëŠ” [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)ë¥¼ ì „ë‹¬í•˜ë©´,
  `data`ëŠ” ê° ë°°ì¹˜ë§ˆë‹¤ `dataset`ì— ì˜í•´ ìƒì„±(yielded)ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

`train_step()` ë©”ì„œë“œì˜ ë³¸ë¬¸ì—ì„œ, ì—¬ëŸ¬ë¶„ì´ ì´ë¯¸ ìµìˆ™í•œ ì¼ë°˜ì ì¸ íŠ¸ë ˆì´ë‹ ì—…ë°ì´íŠ¸ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
ì¤‘ìš”í•œ ì ì€, **`self.compute_loss()`ë¥¼ í†µí•´ ì†ì‹¤ì„ ê³„ì‚°í•œë‹¤ëŠ” ê²ƒ**ì¸ë°,
ì´ëŠ” `compile()`ì— ì „ë‹¬ëœ ì†ì‹¤ í•¨ìˆ˜ë“¤ì„ ë˜í•‘í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë§ˆì°¬ê°€ì§€ë¡œ, `self.metrics`ë¡œë¶€í„°ì˜ ë©”íŠ¸ë¦­ì— ëŒ€í•´ `metric.update_state(y, y_pred)`ë¥¼ í˜¸ì¶œí•˜ì—¬,
`compile()`ì— ì „ë‹¬ëœ ë©”íŠ¸ë¦­ì˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ ,
ë§ˆì§€ë§‰ì— `self.metrics`ì—ì„œ ê²°ê³¼ë¥¼ ì¡°íšŒí•˜ì—¬ í˜„ì¬ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

```python
class CustomModel(keras.Model):
    def train_step(self, data):
        # ë°ì´í„°ë¥¼ ì–¸íŒ©í•©ë‹ˆë‹¤.
        # ê·¸ êµ¬ì¡°ëŠ” ëª¨ë¸ê³¼ `fit()`ì— ì „ë‹¬í•œ ê²ƒì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
        x, y = data

        # ì´ì „ íŠ¸ë ˆì´ë‹ ìŠ¤í…ì—ì„œ ë‚¨ì€ ê°€ì¤‘ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì§€ìš°ê¸° ìœ„í•´
        # torch.nn.Module.zero_grad()ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        self.zero_grad()

        # ì†ì‹¤ ê³„ì‚°
        y_pred = self(x, training=True)  # Forward pass
        loss = self.compute_loss(y=y, y_pred=y_pred)

        # ì†ì‹¤ì— ëŒ€í•´ torch.Tensor.backward()ë¥¼ í˜¸ì¶œí•˜ì—¬
        # ê°€ì¤‘ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        loss.backward()

        trainable_weights = [v for v in self.trainable_weights]
        gradients = [v.value.grad for v in trainable_weights]

        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        with torch.no_grad():
            self.optimizer.apply(gradients, trainable_weights)

        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (ì†ì‹¤ì„ ì¶”ì í•˜ëŠ” ë©”íŠ¸ë¦­ í¬í•¨)
        for metric in self.metrics:
            if metric.name == "loss":
                metric.update_state(loss)
            else:
                metric.update_state(y, y_pred)

        # ë©”íŠ¸ë¦­ ì´ë¦„ì„ í˜„ì¬ ê°’ì— ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        # ì´ëŠ” ì†ì‹¤ì„ í¬í•¨í•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”. (self.metricsì—ì„œ ì¶”ì ë¨)
        return {m.name: m.result() for m in self.metrics}
```

ì´ê²ƒì„ ì‹œë„í•´ë´…ì‹œë‹¤:

```python
# CustomModelì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤.
inputs = keras.Input(shape=(32,))
outputs = keras.layers.Dense(1)(inputs)
model = CustomModel(inputs, outputs)
model.compile(optimizer="adam", loss="mse", metrics=["mae"])

# í‰ì†Œì²˜ëŸ¼ `fit`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
x = np.random.random((1000, 32))
y = np.random.random((1000, 1))
model.fit(x, y, epochs=3)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Epoch 1/3
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step - mae: 0.3410 - loss: 0.1772
Epoch 2/3
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step - mae: 0.3336 - loss: 0.1695
Epoch 3/3
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - mae: 0.3170 - loss: 0.1511

<keras.src.callbacks.history.History at 0x7f48a3255710>
```

{{% /details %}}

## ë” ë‚®ì€ ë ˆë²¨ë¡œ ë‚´ë ¤ê°€ê¸° {#going-lower-level}

ë‹¹ì—°íˆ, `compile()`ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ì§€ ì•Šê³ ,
ëŒ€ì‹  `train_step`ì—ì„œ ëª¨ë“  ì‘ì—…ì„ _ìˆ˜ë™ìœ¼ë¡œ_ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë©”íŠ¸ë¦­ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •ë§Œì„ ìœ„í•´ `compile()`ì„ ì‚¬ìš©í•˜ëŠ”, ë” ë‚®ì€ ë ˆë²¨ì˜ ì˜ˆì œì…ë‹ˆë‹¤:

- `__init__()`ì—ì„œ, ì†ì‹¤ê³¼ MAE ì ìˆ˜ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ `Metric` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.
- ì´ ë©”íŠ¸ë¦­ë“¤ì˜ ìƒíƒœë¥¼ (ë©”íŠ¸ë¦­ì— ëŒ€í•´ `update_state()` í˜¸ì¶œí•¨ìœ¼ë¡œì¨) ì—…ë°ì´íŠ¸í•˜ëŠ”,
  ì»¤ìŠ¤í…€ `train_step()`ì„ êµ¬í˜„í•˜ê³ ,
  ê·¸ëŸ° ë‹¤ìŒ ì§„í–‰ë¥  í‘œì‹œì¤„ì— í‘œì‹œí•˜ê±°ë‚˜ ì½œë°±ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´,
  í˜„ì¬ í‰ê·  ê°’ì„ ë°˜í™˜í•˜ë„ë¡ `result()`ë¥¼ í†µí•´ ì¡°íšŒí•©ë‹ˆë‹¤.
- ê° ì—í¬í¬ ì‚¬ì´ì— ë©”íŠ¸ë¦­ì— ëŒ€í•´ `reset_states()`ë¥¼ í˜¸ì¶œí•´ì•¼ í•œë‹¤ëŠ” ì ì„ ìœ ì˜í•˜ì„¸ìš”!
  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´, `result()`ë¥¼ í˜¸ì¶œí•˜ë©´ íŠ¸ë ˆì´ë‹ ì‹œì‘ ì´í›„ì˜ í‰ê· ì´ ë°˜í™˜ë˜ëŠ”ë°,
  ìš°ë¦¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì—í¬í¬ë³„ í‰ê· ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
  ë‹¤í–‰íˆë„ í”„ë ˆì„ì›Œí¬ëŠ” ì´ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•´ì¤ë‹ˆë‹¤:
  ëª¨ë¸ì˜ `metrics` ì†ì„±ì— ì´ˆê¸°í™”í•˜ë ¤ëŠ” ë©”íŠ¸ë¦­ì„ ë‚˜ì—´í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.
  ëª¨ë¸ì€ ê° `fit()` ì—í¬í¬ì˜ ì‹œì‘ ì‹œ ë˜ëŠ” `evaluate()` í˜¸ì¶œì˜ ì‹œì‘ ì‹œì—,
  ì—¬ê¸°ì— ë‚˜ì—´ëœ ëª¨ë“  ê°ì²´ì— ëŒ€í•´ `reset_states()`ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.

```python
class CustomModel(keras.Model):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.mae_metric = keras.metrics.MeanAbsoluteError(name="mae")
        self.loss_fn = keras.losses.MeanSquaredError()

    def train_step(self, data):
        x, y = data

        # ì´ì „ íŠ¸ë ˆì´ë‹ ìŠ¤í…ì—ì„œ ë‚¨ì€ ê°€ì¤‘ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì§€ìš°ê¸° ìœ„í•´
        # torch.nn.Module.zero_grad()ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        self.zero_grad()

        # ì†ì‹¤ ê³„ì‚°
        y_pred = self(x, training=True)  # ìˆœì „íŒŒ
        loss = self.loss_fn(y, y_pred)

        # ì†ì‹¤ì— ëŒ€í•´ torch.Tensor.backward()ë¥¼ í˜¸ì¶œí•˜ì—¬ ê°€ì¤‘ì¹˜ì˜
        # ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        loss.backward()

        trainable_weights = [v for v in self.trainable_weights]
        gradients = [v.value.grad for v in trainable_weights]

        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        with torch.no_grad():
            self.optimizer.apply(gradients, trainable_weights)

        # ìì²´ ë©”íŠ¸ë¦­ ê³„ì‚°
        self.loss_tracker.update_state(loss)
        self.mae_metric.update_state(y, y_pred)
        return {
            "loss": self.loss_tracker.result(),
            "mae": self.mae_metric.result(),
        }

    @property
    def metrics(self):
        # ì—¬ê¸°ì— `Metric` ê°ì²´ë¥¼ ë‚˜ì—´í•˜ì—¬,
        # ê° ì—í¬í¬ì˜ ì‹œì‘ ì‹œ ë˜ëŠ” `evaluate()`ì˜ ì‹œì‘ ì‹œ,
        # ìë™ìœ¼ë¡œ `reset_states()`ê°€ í˜¸ì¶œë˜ë„ë¡ í•©ë‹ˆë‹¤.
        return [self.loss_tracker, self.mae_metric]


# CustomModelì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
inputs = keras.Input(shape=(32,))
outputs = keras.layers.Dense(1)(inputs)
model = CustomModel(inputs, outputs)

# ì—¬ê¸°ì„œ ì†ì‹¤ì´ë‚˜ ë©”íŠ¸ë¦­ì„ ì „ë‹¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
model.compile(optimizer="adam")

# í‰ì†Œì²˜ëŸ¼ `fit`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ -- ì½œë°± ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
x = np.random.random((1000, 32))
y = np.random.random((1000, 1))
model.fit(x, y, epochs=5)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Epoch 1/5
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step - loss: 0.6173 - mae: 0.6607
Epoch 2/5
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step - loss: 0.2340 - mae: 0.3883
Epoch 3/5
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step - loss: 0.1922 - mae: 0.3517
Epoch 4/5
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step - loss: 0.1802 - mae: 0.3411
Epoch 5/5
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 0.1862 - mae: 0.3505

<keras.src.callbacks.history.History at 0x7f48975ccbd0>
```

{{% /details %}}

## `sample_weight` & `class_weight` ì§€ì› {#supporting-sampleweight-and-classweight}

ì²« ë²ˆì§¸ ê¸°ë³¸ ì˜ˆì œì—ì„œ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ì— ëŒ€í•´ ì–¸ê¸‰í•˜ì§€ ì•Šì€ ê²ƒì„ ëˆˆì¹˜ì±„ì…¨ì„ ê²ë‹ˆë‹¤.
`sample_weight`ì™€ `class_weight`ë¥¼ `fit()` ì¸ìë¡œ ì§€ì›í•˜ë ¤ë©´,
ê°„ë‹¨íˆ ë‹¤ìŒê³¼ ê°™ì´ í•˜ë©´ ë©ë‹ˆë‹¤:

- `data` ì¸ìì—ì„œ `sample_weight`ë¥¼ ì–¸íŒ©í•©ë‹ˆë‹¤.
- `compute_loss`ì™€ `update_state`ì— ì´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
  (ë¬¼ë¡ , ì†ì‹¤ ë° ë©”íŠ¸ë¦­ì— ëŒ€í•´ `compile()`ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ìˆ˜ë™ìœ¼ë¡œ ì ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.)
- ëì…ë‹ˆë‹¤.

```python
class CustomModel(keras.Model):
    def train_step(self, data):
        # ë°ì´í„°ë¥¼ ì–¸íŒ©í•©ë‹ˆë‹¤. ê·¸ êµ¬ì¡°ëŠ” ëª¨ë¸ê³¼
        # `fit()`ì— ì „ë‹¬í•œ ê²ƒì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
        if len(data) == 3:
            x, y, sample_weight = data
        else:
            sample_weight = None
            x, y = data

        # ì´ì „ íŠ¸ë ˆì´ë‹ ìŠ¤í…ì—ì„œ ë‚¨ì€ ê°€ì¤‘ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì§€ìš°ê¸° ìœ„í•´
        # torch.nn.Module.zero_grad()ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        self.zero_grad()

        # ì†ì‹¤ ê³„ì‚°
        y_pred = self(x, training=True)  # ìˆœì „íŒŒ
        loss = self.compute_loss(
            y=y,
            y_pred=y_pred,
            sample_weight=sample_weight,
        )

        # ì†ì‹¤ì— ëŒ€í•´ torch.Tensor.backward()ë¥¼ í˜¸ì¶œí•˜ì—¬,
        # ê°€ì¤‘ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        loss.backward()

        trainable_weights = [v for v in self.trainable_weights]
        gradients = [v.value.grad for v in trainable_weights]

        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        with torch.no_grad():
            self.optimizer.apply(gradients, trainable_weights)

        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (ì†ì‹¤ì„ ì¶”ì í•˜ëŠ” ë©”íŠ¸ë¦­ í¬í•¨)
        for metric in self.metrics:
            if metric.name == "loss":
                metric.update_state(loss)
            else:
                metric.update_state(y, y_pred, sample_weight=sample_weight)

        # ë©”íŠ¸ë¦­ ì´ë¦„ì„ í˜„ì¬ ê°’ì— ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        # ì´ëŠ” ì†ì‹¤ì„ í¬í•¨í•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”. (self.metricsì—ì„œ ì¶”ì ë¨)
        return {m.name: m.result() for m in self.metrics}


# CustomModelì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤
inputs = keras.Input(shape=(32,))
outputs = keras.layers.Dense(1)(inputs)
model = CustomModel(inputs, outputs)
model.compile(optimizer="adam", loss="mse", metrics=["mae"])

# ì´ì œ sample_weight ì¸ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
x = np.random.random((1000, 32))
y = np.random.random((1000, 1))
sw = np.random.random((1000, 1))
model.fit(x, y, sample_weight=sw, epochs=3)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Epoch 1/3
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step - mae: 0.3216 - loss: 0.0827
Epoch 2/3
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step - mae: 0.3156 - loss: 0.0803
Epoch 3/3
 32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step - mae: 0.3085 - loss: 0.0760

<keras.src.callbacks.history.History at 0x7f48975d7bd0>
```

{{% /details %}}

## ë‹¹ì‹ ë§Œì˜ í‰ê°€ ìŠ¤í… ì œê³µ {#providing-your-own-evaluation-step}

`model.evaluate()` í˜¸ì¶œì— ëŒ€í•´ì„œë„ ë™ì¼í•œ ì‘ì—…ì„ í•˜ê³  ì‹¶ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?
ê·¸ëŸ° ê²½ìš°, `test_step`ì„ ì •í™•íˆ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œí•˜ë©´ ë©ë‹ˆë‹¤.
ë‹¤ìŒì€ ê·¸ ì˜ˆì‹œì…ë‹ˆë‹¤:

```python
class CustomModel(keras.Model):
    def test_step(self, data):
        # ë°ì´í„°ë¥¼ ì–¸íŒ©í•©ë‹ˆë‹¤
        x, y = data
        # ì˜ˆì¸¡ê°’ ê³„ì‚°
        y_pred = self(x, training=False)
        # ì†ì‹¤ì„ ì¶”ì í•˜ëŠ” ë©”íŠ¸ë¦­ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        loss = self.compute_loss(y=y, y_pred=y_pred)
        # ë©”íŠ¸ë¦­ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        for metric in self.metrics:
            if metric.name == "loss":
                metric.update_state(loss)
            else:
                metric.update_state(y, y_pred)
        # ë©”íŠ¸ë¦­ ì´ë¦„ì„ í˜„ì¬ ê°’ì— ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        # ì´ëŠ” ì†ì‹¤ì„ í¬í•¨í•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”. (self.metricsì—ì„œ ì¶”ì ë¨)
        return {m.name: m.result() for m in self.metrics}


# CustomModelì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
inputs = keras.Input(shape=(32,))
outputs = keras.layers.Dense(1)(inputs)
model = CustomModel(inputs, outputs)
model.compile(loss="mse", metrics=["mae"])

# ìš°ë¦¬ì˜ ì»¤ìŠ¤í…€ test_stepìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
x = np.random.random((1000, 32))
y = np.random.random((1000, 1))
model.evaluate(x, y)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain

1/32 [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 2ms/step - mae: 0.8706 - loss: 0.9344

32/32 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 1ms/step - mae: 0.8959 - loss: 0.9952

[1.0077838897705078, 0.8984771370887756]
```

{{% /details %}}

## ë§ˆë¬´ë¦¬: ì—”ë“œíˆ¬ì—”ë“œ GAN ì˜ˆì œ {#wrapping-up-an-end-to-end-gan-example}

ì—”ë“œíˆ¬ì—”ë“œ ì˜ˆì œë¥¼ í†µí•´ ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ëª¨ë“  ê²ƒì„ í™œìš©í•´ ë´…ì‹œë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì„± ìš”ì†Œë¥¼ ê³ ë ¤í•˜ê² ìŠµë‹ˆë‹¤:

- 28x28x1 ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ìƒì„±ì ë„¤íŠ¸ì›Œí¬
- 28x28x1 ì´ë¯¸ì§€ë¥¼ ë‘ ê°œì˜ í´ë˜ìŠ¤("ê°€ì§œ"ì™€ "ì§„ì§œ")ë¡œ ë¶„ë¥˜í•˜ëŠ” íŒë³„ì ë„¤íŠ¸ì›Œí¬
- ê°ê°ì— ëŒ€í•œ ì˜µí‹°ë§ˆì´ì € í•˜ë‚˜
- íŒë³„ìë¥¼ íŠ¸ë ˆì´ë‹í•˜ê¸° ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜

```python
# íŒë³„ì ìƒì„±
discriminator = keras.Sequential(
    [
        keras.Input(shape=(28, 28, 1)),
        layers.Conv2D(64, (3, 3), strides=(2, 2), padding="same"),
        layers.LeakyReLU(negative_slope=0.2),
        layers.Conv2D(128, (3, 3), strides=(2, 2), padding="same"),
        layers.LeakyReLU(negative_slope=0.2),
        layers.GlobalMaxPooling2D(),
        layers.Dense(1),
    ],
    name="discriminator",
)

# ìƒì„±ì ìƒì„±
latent_dim = 128
generator = keras.Sequential(
    [
        keras.Input(shape=(latent_dim,)),
        # 7x7x128 ë§µìœ¼ë¡œ reshape í•˜ê¸° ìœ„í•´ 128ê°œì˜ ê³„ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        layers.Dense(7 * 7 * 128),
        layers.LeakyReLU(negative_slope=0.2),
        layers.Reshape((7, 7, 128)),
        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding="same"),
        layers.LeakyReLU(negative_slope=0.2),
        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding="same"),
        layers.LeakyReLU(negative_slope=0.2),
        layers.Conv2D(1, (7, 7), padding="same", activation="sigmoid"),
    ],
    name="generator",
)
```

ì—¬ê¸°ì—ëŠ” `compile()`ì„ ìì‹ ì˜ ì‹œê·¸ë‹ˆì²˜ë¡œ ì‚¬ìš©í•˜ê³ ,
`train_step`ì—ì„œ ì „ì²´ GAN ì•Œê³ ë¦¬ì¦˜ì„ 17ì¤„ë¡œ êµ¬í˜„í•œ,
ê¸°ëŠ¥ì´ ì™„ì „í•œ(feature-complete) GAN í´ë˜ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤:

```python
class GAN(keras.Model):
    def __init__(self, discriminator, generator, latent_dim):
        super().__init__()
        self.discriminator = discriminator
        self.generator = generator
        self.latent_dim = latent_dim
        self.d_loss_tracker = keras.metrics.Mean(name="d_loss")
        self.g_loss_tracker = keras.metrics.Mean(name="g_loss")
        self.seed_generator = keras.random.SeedGenerator(1337)
        self.built = True

    @property
    def metrics(self):
        return [self.d_loss_tracker, self.g_loss_tracker]

    def compile(self, d_optimizer, g_optimizer, loss_fn):
        super().compile()
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.loss_fn = loss_fn

    def train_step(self, real_images):
        device = "cuda" if torch.cuda.is_available() else "cpu"
        if isinstance(real_images, tuple) or isinstance(real_images, list):
            real_images = real_images[0]
        # ì ì¬ ê³µê°„ì—ì„œ ëœë¤ í¬ì¸íŠ¸ ìƒ˜í”Œë§
        batch_size = real_images.shape[0]
        random_latent_vectors = keras.random.normal(
            shape=(batch_size, self.latent_dim), seed=self.seed_generator
        )

        # ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        generated_images = self.generator(random_latent_vectors)

        # ì´ë¥¼ ì§„ì§œ ì´ë¯¸ì§€ì™€ ê²°í•©í•©ë‹ˆë‹¤.
        real_images = torch.tensor(real_images, device=device)
        combined_images = torch.concat([generated_images, real_images], axis=0)

        # ì§„ì§œì™€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” ë ˆì´ë¸”ì„ ì¡°í•©í•©ë‹ˆë‹¤.
        labels = torch.concat(
            [
                torch.ones((batch_size, 1), device=device),
                torch.zeros((batch_size, 1), device=device),
            ],
            axis=0,
        )
        # ë ˆì´ë¸”ì— ëœë¤ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. - ì¤‘ìš”í•œ íŠ¸ë¦­ì…ë‹ˆë‹¤!
        labels += 0.05 * keras.random.uniform(labels.shape, seed=self.seed_generator)

        # íŒë³„ìë¥¼ íŠ¸ë ˆì´ë‹í•©ë‹ˆë‹¤.
        self.zero_grad()
        predictions = self.discriminator(combined_images)
        d_loss = self.loss_fn(labels, predictions)
        d_loss.backward()
        grads = [v.value.grad for v in self.discriminator.trainable_weights]
        with torch.no_grad():
            self.d_optimizer.apply(grads, self.discriminator.trainable_weights)

        # ì ì¬ ê³µê°„ì—ì„œ ëœë¤ í¬ì¸íŠ¸ ìƒ˜í”Œë§
        random_latent_vectors = keras.random.normal(
            shape=(batch_size, self.latent_dim), seed=self.seed_generator
        )

        # "ëª¨ë“  ì§„ì§œ ì´ë¯¸ì§€ (all real images)"ë¼ê³  ë§í•˜ëŠ” ë ˆì´ë¸”ì„ ì¡°í•©í•©ë‹ˆë‹¤.
        misleading_labels = torch.zeros((batch_size, 1), device=device)

        # ìƒì„±ìë¥¼ íŠ¸ë ˆì´ë‹í•©ë‹ˆë‹¤.
        # (íŒë³„ìì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ë©´ ì•ˆë©ë‹ˆë‹¤)
        self.zero_grad()
        predictions = self.discriminator(self.generator(random_latent_vectors))
        g_loss = self.loss_fn(misleading_labels, predictions)
        grads = g_loss.backward()
        grads = [v.value.grad for v in self.generator.trainable_weights]
        with torch.no_grad():
            self.g_optimizer.apply(grads, self.generator.trainable_weights)

        # ë©”íŠ¸ë¦­ì„ ì—…ë°ì´íŠ¸í•˜ê³  ê·¸ ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        self.d_loss_tracker.update_state(d_loss)
        self.g_loss_tracker.update_state(g_loss)
        return {
            "d_loss": self.d_loss_tracker.result(),
            "g_loss": self.g_loss_tracker.result(),
        }
```

ì´ë¥¼ í…ŒìŠ¤íŠ¸í•´ë´…ì‹œë‹¤:

```python
# ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” MNIST ìˆ«ìì˜ íŠ¸ë ˆì´ë‹ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
batch_size = 64
(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()
all_digits = np.concatenate([x_train, x_test])
all_digits = all_digits.astype("float32") / 255.0
all_digits = np.reshape(all_digits, (-1, 28, 28, 1))

# TensorDataset ìƒì„±
dataset = torch.utils.data.TensorDataset(
    torch.from_numpy(all_digits), torch.from_numpy(all_digits)
)
# DataLoader ìƒì„±
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)
gan.compile(
    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),
    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),
    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),
)

gan.fit(dataloader, epochs=1)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
 1094/1094 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 394s 360ms/step - d_loss: 0.2436 - g_loss: 4.7259
<keras.src.callbacks.history.History at 0x7f489760a490>
```

{{% /details %}}

ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì€ ê°„ë‹¨í•œë°, ì™œ ê·¸ êµ¬í˜„ì€ ê³ í†µìŠ¤ëŸ¬ì›Œì•¼ í• ê¹Œìš”?
