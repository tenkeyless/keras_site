---
title: ì „ì´ í•™ìŠµ ë° ë¯¸ì„¸ ì¡°ì •
linkTitle: ì „ì´ í•™ìŠµ ë° ë¯¸ì„¸ ì¡°ì •
toc: true
weight: 14
type: docs
---

{{< keras/original checkedAt="2024-11-18" >}}

**{{< t f_author >}}** [fchollet](https://twitter.com/fchollet)  
**{{< t f_date_created >}}** 2020/04/15  
**{{< t f_last_modified >}}** 2023/06/25  
**{{< t f_description >}}** Kerasì˜ ì „ì´ í•™ìŠµ ë° ë¯¸ì„¸ ì¡°ì •ì— ëŒ€í•œ ì™„ë²½ ê°€ì´ë“œ.

{{< cards cols="2" >}}
{{< card link="https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/transfer_learning.ipynb" title="Colab" tag="Colab" tagType="warning">}}
{{< card link="https://github.com/keras-team/keras-io/blob/master/guides/transfer_learning.py" title="GitHub" tag="GitHub">}}
{{< /cards >}}

## ì…‹ì—… {#setup}

```python
import numpy as np
import keras
from keras import layers
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
```

## ì†Œê°œ {#introduction}

**ì „ì´ í•™ìŠµ(Transfer learning)** ì€ í•œ ë¬¸ì œì—ì„œ í•™ìŠµëœ íŠ¹ì„±ì„ ìƒˆë¡œìš´, ìœ ì‚¬í•œ ë¬¸ì œì— í™œìš©í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ë¼ì¿¤ì„ ì‹ë³„í•˜ëŠ” ëª¨ë¸ì—ì„œ í•™ìŠµëœ íŠ¹ì„±ì€ ë„ˆêµ¬ë¦¬(tanuki)ë¥¼ ì‹ë³„í•˜ë ¤ëŠ” ëª¨ë¸ì„ ì‹œì‘í•˜ëŠ” ë° ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì „ì´ í•™ìŠµì€ ë³´í†µ ë°ì´í„°ì…‹ì´ ë¶€ì¡±í•˜ì—¬, ì²˜ìŒë¶€í„° ì™„ì „í•œ ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•˜ê¸° ì–´ë ¤ìš´ ì‘ì—…ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

ë”¥ëŸ¬ë‹ì—ì„œ ì „ì´ í•™ìŠµì˜ ê°€ì¥ ì¼ë°˜ì ì¸ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì›Œí¬í”Œë¡œì…ë‹ˆë‹¤:

1.  ì´ì „ì— íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸ì˜ ë ˆì´ì–´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
2.  ê·¸ ë ˆì´ì–´ë“¤ì„ ë™ê²°(freeze)í•˜ì—¬, ì´í›„ íŠ¸ë ˆì´ë‹ ë¼ìš´ë“œì—ì„œ í•´ë‹¹ ì •ë³´ê°€ íŒŒê´´ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
3.  ë™ê²°ëœ ë ˆì´ì–´ ìœ„ì— ìƒˆë¡œìš´ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    ì´ ë ˆì´ì–´ë“¤ì€ ê¸°ì¡´ì˜ íŠ¹ì§•ì„ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ í•™ìŠµí•©ë‹ˆë‹¤.
4.  ìƒˆë¡œìš´ ë ˆì´ì–´ë“¤ì„ ë°ì´í„°ì…‹ì— ë§ê²Œ íŠ¸ë ˆì´ë‹í•©ë‹ˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ ì„ íƒì ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” ë‹¨ê³„ëŠ” **ë¯¸ì„¸ ì¡°ì •(fine-tuning)** ì…ë‹ˆë‹¤.
ì´ëŠ” ìœ„ì—ì„œ ì–»ì€ ì „ì²´ ëª¨ë¸(ë˜ëŠ” ê·¸ ì¼ë¶€)ì„ ë™ê²° í•´ì œ(unfreeze)í•˜ê³ ,
ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ë‹¤ì‹œ íŠ¸ë ˆì´ë‹í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.
ì´ë ‡ê²Œ í•˜ë©´ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ íŠ¹ì§•ì„ ìƒˆë¡œìš´ ë°ì´í„°ì— ë§ê²Œ ì ì§„ì ìœ¼ë¡œ ì ì‘ì‹œì¼œ ìœ ì˜ë¯¸í•œ ê°œì„ ì„ ì´ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¨¼ì €, ëŒ€ë¶€ë¶„ì˜ ì „ì´ í•™ìŠµ ë° ë¯¸ì„¸ ì¡°ì • ì›Œí¬í”Œë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ”
Kerasì˜ `trainable` APIë¥¼ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ, ImageNet ë°ì´í„°ì…‹ì—ì„œ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸ì„ ê°€ì ¸ì™€,
Kaggleì˜ "cats vs dogs" ë¶„ë¥˜ ë°ì´í„°ì…‹ì—ì„œ ë‹¤ì‹œ íŠ¸ë ˆì´ë‹í•˜ëŠ” ì¼ë°˜ì ì¸ ì›Œí¬í”Œë¡œë¥¼ ì‹œì—°í•˜ê² ìŠµë‹ˆë‹¤.

ì´ ë‚´ìš©ì€ [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) ë°
2016ë…„ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ ["ì•„ì£¼ ì ì€ ë°ì´í„°ë¡œ ê°•ë ¥í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ë§Œë“¤ê¸°"](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)ì—ì„œ ê°ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.

## ë ˆì´ì–´ ë™ê²°: `trainable` ì†ì„± ì´í•´í•˜ê¸° {#freezing-layers-understanding-the-trainable-attribute}

ë ˆì´ì–´ ë° ëª¨ë¸ì—ëŠ” ì„¸ ê°€ì§€ ê°€ì¤‘ì¹˜ ì†ì„±ì´ ìˆìŠµë‹ˆë‹¤:

- `weights`ëŠ” ë ˆì´ì–´ì˜ ëª¨ë“  ê°€ì¤‘ì¹˜ ë³€ìˆ˜ë¥¼ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
- `trainable_weights`ëŠ” íŠ¸ë ˆì´ë‹ ì¤‘ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´,
  (ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•´) ì—…ë°ì´íŠ¸ë  ê°€ì¤‘ì¹˜ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
- `non_trainable_weights`ëŠ” íŠ¸ë ˆì´ë‹ë˜ì§€ ì•Šë„ë¡ ì„¤ì •ëœ ê°€ì¤‘ì¹˜ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
  ì¼ë°˜ì ìœ¼ë¡œ ì´ëŠ” ëª¨ë¸ì´ ìˆœë°©í–¥ íŒ¨ìŠ¤(forward pass)ë¥¼ ìˆ˜í–‰í•  ë•Œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.

**ì˜ˆì‹œ: `Dense` ë ˆì´ì–´ëŠ” 2ê°œì˜ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜(kernel ë° bias)ë¥¼ ê°€ì§‘ë‹ˆë‹¤.**

```python
layer = keras.layers.Dense(3)
layer.build((None, 4))  # ê°€ì¤‘ì¹˜ ìƒì„±

print("weights:", len(layer.weights))
print("trainable_weights:", len(layer.trainable_weights))
print("non_trainable_weights:", len(layer.non_trainable_weights))
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
weights: 2
trainable_weights: 2
non_trainable_weights: 0
```

{{% /details %}}

ì¼ë°˜ì ìœ¼ë¡œ, ëª¨ë“  ê°€ì¤‘ì¹˜ëŠ” íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ì…ë‹ˆë‹¤.
ìœ ì¼í•˜ê²Œ ë¹ŒíŠ¸ì¸ ë ˆì´ì–´ ì¤‘ íŠ¸ë ˆì´ë‹ ë¶ˆê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ ë ˆì´ì–´ëŠ” `BatchNormalization` ë ˆì´ì–´ì…ë‹ˆë‹¤.
ì´ ë ˆì´ì–´ëŠ” íŠ¸ë ˆì´ë‹ ì¤‘ ì…ë ¥ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì¶”ì í•˜ê¸° ìœ„í•´, íŠ¸ë ˆì´ë‹ ë¶ˆê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ì»¤ìŠ¤í…€ ë ˆì´ì–´ì—ì„œ íŠ¸ë ˆì´ë‹ ë¶ˆê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë°°ìš°ë ¤ë©´,
{{< titledRelref "/docs/guides/making_new_layers_and_models_via_subclassing" >}} ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

**ì˜ˆì‹œ: `BatchNormalization` ë ˆì´ì–´ëŠ” 2ê°œì˜ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ì™€ 2ê°œì˜ íŠ¸ë ˆì´ë‹ ë¶ˆê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤.**

```python
layer = keras.layers.BatchNormalization()
layer.build((None, 4))  # ê°€ì¤‘ì¹˜ ìƒì„±

print("weights:", len(layer.weights))
print("trainable_weights:", len(layer.trainable_weights))
print("non_trainable_weights:", len(layer.non_trainable_weights))
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
weights: 4
trainable_weights: 2
non_trainable_weights: 2
```

{{% /details %}}

ë ˆì´ì–´ì™€ ëª¨ë¸ì—ëŠ” `trainable`ì´ë¼ëŠ” boolean ì†ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì´ ê°’ì€ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
`layer.trainable`ì„ `False`ë¡œ ì„¤ì •í•˜ë©´,
í•´ë‹¹ ë ˆì´ì–´ì˜ ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ìƒíƒœì—ì„œ íŠ¸ë ˆì´ë‹ ë¶ˆê°€ëŠ¥í•œ ìƒíƒœë¡œ ì´ë™í•©ë‹ˆë‹¤.
ì´ë¥¼ "ë ˆì´ì–´ ë™ê²°(freezing)"ì´ë¼ê³  í•˜ë©°, ë™ê²°ëœ ë ˆì´ì–´ì˜ ìƒíƒœëŠ” íŠ¸ë ˆì´ë‹ ì¤‘ì— ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
(`fit()`ì„ ì‚¬ìš©í•œ íŠ¸ë ˆì´ë‹ì´ë“ ,
`trainable_weights`ì— ì˜ì¡´í•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ë¥¼ ì ìš©í•˜ëŠ” ì»¤ìŠ¤í…€ ë£¨í”„ì—ì„œë“  ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤)

**ì˜ˆì‹œ: `trainable`ì„ `False`ë¡œ ì„¤ì •**

```python
layer = keras.layers.Dense(3)
layer.build((None, 4))  # ê°€ì¤‘ì¹˜ ìƒì„±
layer.trainable = False  # ë ˆì´ì–´ ë™ê²°

print("weights:", len(layer.weights))
print("trainable_weights:", len(layer.trainable_weights))
print("non_trainable_weights:", len(layer.non_trainable_weights))
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
weights: 2
trainable_weights: 0
non_trainable_weights: 2
```

{{% /details %}}

íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ê°€ íŠ¸ë ˆì´ë‹ ë¶ˆê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¡œ ë°”ë€Œë©´,
í•´ë‹¹ ê°€ì¤‘ì¹˜ì˜ ê°’ì€ ë” ì´ìƒ íŠ¸ë ˆì´ë‹ ì¤‘ì— ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

```python
# ë‘ ê°œì˜ ë ˆì´ì–´ë¡œ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.
layer1 = keras.layers.Dense(3, activation="relu")
layer2 = keras.layers.Dense(3, activation="sigmoid")
model = keras.Sequential([keras.Input(shape=(3,)), layer1, layer2])

# ì²« ë²ˆì§¸ ë ˆì´ì–´ë¥¼ ë™ê²°í•©ë‹ˆë‹¤.
layer1.trainable = False

# ë‚˜ì¤‘ì— ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ `layer1`ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³µì‚¬í•´ë‘¡ë‹ˆë‹¤.
initial_layer1_weights_values = layer1.get_weights()

# ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•©ë‹ˆë‹¤.
model.compile(optimizer="adam", loss="mse")
model.fit(np.random.random((2, 3)), np.random.random((2, 3)))

# íŠ¸ë ˆì´ë‹ ì¤‘ì— `layer1`ì˜ ê°€ì¤‘ì¹˜ê°€ ë³€ê²½ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
final_layer1_weights_values = layer1.get_weights()
np.testing.assert_allclose(
    initial_layer1_weights_values[0], final_layer1_weights_values[0]
)
np.testing.assert_allclose(
    initial_layer1_weights_values[1], final_layer1_weights_values[1]
)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
 1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 766ms/step - loss: 0.0615
```

{{% /details %}}

`layer.trainable` ì†ì„±ê³¼ `layer.__call__()`ì˜ ì¸ì `training`ì„ í˜¼ë™í•˜ì§€ ë§ˆì„¸ìš”.
(`training` ì¸ìëŠ” ë ˆì´ì–´ê°€ ì¶”ë¡  ëª¨ë“œ ë˜ëŠ” íŠ¸ë ˆì´ë‹ ëª¨ë“œì—ì„œ ìˆœë°©í–¥ íŒ¨ìŠ¤ë¥¼ ì‹¤í–‰í• ì§€ ì—¬ë¶€ë¥¼ ì œì–´í•©ë‹ˆë‹¤.)
ìì„¸í•œ ë‚´ìš©ì€ [Keras FAQ]({{< relref "/docs/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute" >}})ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

## `trainable` ì†ì„±ì˜ ì¬ê·€ì  ì„¤ì • {#recursive-setting-of-the-trainable-attribute}

ëª¨ë¸ì´ë‚˜ ì„œë¸Œë ˆì´ì–´ë¥¼ ê°€ì§„ ë ˆì´ì–´ì—ì„œ `trainable = False`ë¡œ ì„¤ì •í•˜ë©´,
ëª¨ë“  í•˜ìœ„ ë ˆì´ì–´ë„ íŠ¸ë ˆì´ë‹ ë¶ˆê°€ëŠ¥í•œ ìƒíƒœê°€ ë©ë‹ˆë‹¤.

**ì˜ˆì‹œ:**

```python
inner_model = keras.Sequential(
    [
        keras.Input(shape=(3,)),
        keras.layers.Dense(3, activation="relu"),
        keras.layers.Dense(3, activation="relu"),
    ]
)

model = keras.Sequential(
    [
        keras.Input(shape=(3,)),
        inner_model,
        keras.layers.Dense(3, activation="sigmoid"),
    ]
)

model.trainable = False  # ë°”ê¹¥ ëª¨ë¸ì„ ë™ê²°í•©ë‹ˆë‹¤.

assert inner_model.trainable == False  # ì´ì œ `model`ì˜ ëª¨ë“  ë ˆì´ì–´ê°€ ë™ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.
assert inner_model.layers[0].trainable == False  # `trainable`ì´ ì¬ê·€ì ìœ¼ë¡œ ì „íŒŒë©ë‹ˆë‹¤.
```

## ì¼ë°˜ì ì¸ ì „ì´ í•™ìŠµ ì›Œí¬í”Œë¡œ {#the-typical-transfer-learning-workflow}

ì´ì œ Kerasì—ì„œ ì „ì´ í•™ìŠµ ì›Œí¬í”Œë¡œë¥¼ êµ¬í˜„í•˜ëŠ” ì¼ë°˜ì ì¸ ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤:

1.  ë² ì´ìŠ¤ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³ , ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
2.  `trainable = False`ë¡œ ì„¤ì •í•˜ì—¬, ë² ì´ìŠ¤ ëª¨ë¸ì˜ ëª¨ë“  ë ˆì´ì–´ë¥¼ ë™ê²°í•©ë‹ˆë‹¤.
3.  ë² ì´ìŠ¤ ëª¨ë¸ ìœ„ì— í•œ ê°œ ì´ìƒì˜ ë ˆì´ì–´ ì¶œë ¥ì„ ë§Œë“¤ì–´ ìƒˆë¡œìš´ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.
4.  ìƒˆë¡œìš´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìƒˆ ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•©ë‹ˆë‹¤.

ë˜í•œ, ë” ê°€ë²¼ìš´ ì›Œí¬í”Œë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

1.  ë² ì´ìŠ¤ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³ , ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
2.  ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ í†µí•´ ë² ì´ìŠ¤ ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³ , ë² ì´ìŠ¤ ëª¨ë¸ì˜ í•œ ê°œ ì´ìƒì˜ ë ˆì´ì–´ ì¶œë ¥ì„ ê¸°ë¡í•©ë‹ˆë‹¤.
    ì´ë¥¼ **íŠ¹ì„± ì¶”ì¶œ**ì´ë¼ê³  í•©ë‹ˆë‹¤.
3.  ê·¸ ì¶œë ¥ì„ ìƒˆë¡œìš´, ë” ì‘ì€ ëª¨ë¸ì˜ ì…ë ¥ ë°ì´í„°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì´ ë‘ ë²ˆì§¸ ì›Œí¬í”Œë¡œì˜ ì£¼ìš” ì¥ì ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ë•Œ ë² ì´ìŠ¤ ëª¨ë¸ì„ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ë©´ ë˜ë¯€ë¡œ,
ì—í¬í¬ë§ˆë‹¤ ë² ì´ìŠ¤ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì§€ ì•Šì•„ë„ ë˜ê¸° ë•Œë¬¸ì—, í›¨ì”¬ ë¹ ë¥´ê³  ë¹„ìš©ì´ ì €ë ´í•˜ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ë‘ ë²ˆì§¸ ì›Œí¬í”Œë¡œì˜ ë‹¨ì ì€, íŠ¸ë ˆì´ë‹ ì¤‘ì— ìƒˆ ëª¨ë¸ì˜ ì…ë ¥ ë°ì´í„°ë¥¼ ë™ì ìœ¼ë¡œ ìˆ˜ì •í•  ìˆ˜ ì—†ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ë°ì´í„° ë³´ê°•ì„ í•  ë•ŒëŠ” ì´ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤.
ì „ì´ í•™ìŠµì€ ì¼ë°˜ì ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì´ ë¶€ì¡±í•˜ì—¬,
ì²˜ìŒë¶€í„° ëŒ€ê·œëª¨ ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•  ìˆ˜ ì—†ì„ ë•Œ ì‚¬ìš©ë˜ë©°,
ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ë°ì´í„° ë³´ê°•ì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.
ë”°ë¼ì„œ, ì•„ë˜ì—ì„œëŠ” ì²« ë²ˆì§¸ ì›Œí¬í”Œë¡œì— ì´ˆì ì„ ë§ì¶”ê² ìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ Kerasì—ì„œ ì²« ë²ˆì§¸ ì›Œí¬í”Œë¡œê°€ ì–´ë–»ê²Œ ë³´ì´ëŠ”ì§€ì…ë‹ˆë‹¤:

ë¨¼ì €, ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ê°€ì¤‘ì¹˜ë¡œ ê¸°ë³¸ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.

```python
base_model = keras.applications.Xception(
    weights='imagenet',  # ImageNetì—ì„œ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ.
    input_shape=(150, 150, 3),
    include_top=False)  # ImageNet ë¶„ë¥˜ê¸°ë¥¼ ìƒë‹¨ì— í¬í•¨í•˜ì§€ ì•ŠìŒ.
```

ê·¸ëŸ° ë‹¤ìŒ, ë² ì´ìŠ¤ ëª¨ë¸ì„ ë™ê²°í•©ë‹ˆë‹¤.

```python
base_model.trainable = False
```

ìƒë‹¨ì— ìƒˆë¡œìš´ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

```python
inputs = keras.Input(shape=(150, 150, 3))
# ì—¬ê¸°ì„œ ë² ì´ìŠ¤ ëª¨ë¸ì´ ì¶”ë¡  ëª¨ë“œë¡œ ì‹¤í–‰ë˜ë„ë¡, `training=False`ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
# ì´ëŠ” ë¯¸ì„¸ ì¡°ì •ì„ í•  ë•Œ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì€ ì´í›„ì— ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤.
x = base_model(inputs, training=False)
# `base_model.output_shape[1:]` í˜•íƒœì˜ íŠ¹ì„±ì„ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
x = keras.layers.GlobalAveragePooling2D()(x)
# ë‹¨ì¼ ìœ ë‹›ì„ ê°€ì§„ Dense ë¶„ë¥˜ê¸° (ì´ì§„ ë¶„ë¥˜)
outputs = keras.layers.Dense(1)(x)
model = keras.Model(inputs, outputs)
```

ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•©ë‹ˆë‹¤.

```python
model.compile(optimizer=keras.optimizers.Adam(),
              loss=keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=[keras.metrics.BinaryAccuracy()])
model.fit(new_dataset, epochs=20, callbacks=..., validation_data=...)
```

## ë¯¸ì„¸ ì¡°ì • {#fine-tuning}

ëª¨ë¸ì´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ìˆ˜ë ´í•œ í›„, ë² ì´ìŠ¤ ëª¨ë¸ì˜ ì „ì²´ ë˜ëŠ” ì¼ë¶€ë¥¼ ë™ê²° í•´ì œí•˜ê³ ,
ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ ë¡œ ì „ì²´ ëª¨ë¸ì„ ì—”ë“œíˆ¬ì—”ë“œë¡œ ë‹¤ì‹œ íŠ¸ë ˆì´ë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ë‹¨ê³„ëŠ” ì„ íƒì ì¸ ë§ˆì§€ë§‰ ë‹¨ê³„ë¡œ, ì ì§„ì ì¸ ê°œì„ ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ, ê³¼ì í•©ì´ ë¹ ë¥´ê²Œ ë°œìƒí•  ìˆ˜ë„ ìˆë‹¤ëŠ” ì ì„ ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤.

ëª¨ë¸ì˜ ë™ê²°ëœ ë ˆì´ì–´ê°€ ìˆ˜ë ´ëœ _í›„ì—ë§Œ_ ì´ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ë ˆì´ì–´ì™€ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ íŠ¹ì„±ì„ ë³´ìœ í•œ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ ë ˆì´ì–´ë¥¼ ì„ìœ¼ë©´,
ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ë ˆì´ì–´ê°€ íŠ¸ë ˆì´ë‹ ì¤‘ì— ë§¤ìš° í° ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ë¥¼ ìœ ë°œí•´,
ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ íŠ¹ì„±ì„ íŒŒê´´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë˜í•œ, ì´ ë‹¨ê³„ì—ì„œ ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
ì´ëŠ” ì²« ë²ˆì§¸ ë¼ìš´ë“œì˜ íŠ¸ë ˆì´ë‹ë³´ë‹¤ í›¨ì”¬ í° ëª¨ë¸ì„, ì¼ë°˜ì ìœ¼ë¡œ ë§¤ìš° ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ íŠ¸ë ˆì´ë‹í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
ê·¸ ê²°ê³¼, í° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ì ìš©í•˜ë©´ ë§¤ìš° ë¹ ë¥´ê²Œ ê³¼ì í•©ë  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.
ì—¬ê¸°ì„œëŠ” ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ê°€ì¤‘ì¹˜ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¬ì ì‘ì‹œí‚¤ê³ ì í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ì „ì²´ ë² ì´ìŠ¤ ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì •ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤:

```python
# ë² ì´ìŠ¤ ëª¨ë¸ì˜ ë™ê²° í•´ì œ
base_model.trainable = True

# `trainable` ì†ì„±ì„ ë³€ê²½í•œ í›„ì—ëŠ” ëª¨ë¸ì„ ë‹¤ì‹œ ì»´íŒŒì¼í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•´ì•¼ ë³€ê²½ ì‚¬í•­ì´ ë°˜ì˜ë©ë‹ˆë‹¤.
model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate
              loss=keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=[keras.metrics.BinaryAccuracy()])

# ì—”ë“œíˆ¬ì—”ë“œë¡œ íŠ¸ë ˆì´ë‹. ê³¼ì í•©ì´ ë°œìƒí•˜ê¸° ì „ì— ì£¼ì˜í•´ì„œ ë©ˆì¶”ì„¸ìš”!
model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...)
```

**`compile()`ê³¼ `trainable`ì— ëŒ€í•œ ì¤‘ìš”í•œ ì°¸ê³  ì‚¬í•­**

ëª¨ë¸ì—ì„œ `compile()`ì„ í˜¸ì¶œí•˜ë©´, ê·¸ ëª¨ë¸ì˜ ë™ì‘ì´ "ë™ê²°(freeze)"ë©ë‹ˆë‹¤.
ì¦‰, ëª¨ë¸ì´ ì»´íŒŒì¼ë  ë•Œì˜ `trainable` ì†ì„± ê°’ì´ ì´í›„,
ë‹¤ì‹œ `compile`ì´ í˜¸ì¶œë˜ê¸° ì „ê¹Œì§€ ìœ ì§€ë©ë‹ˆë‹¤.
ë”°ë¼ì„œ, ì–´ë–¤ `trainable` ê°’ì„ ë³€ê²½í•œ ê²½ìš°,
ë³€ê²½ ì‚¬í•­ì´ ë°˜ì˜ë˜ë„ë¡ ëª¨ë¸ì—ì„œ `compile()`ì„ ë‹¤ì‹œ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.

**`BatchNormalization` ë ˆì´ì–´ì— ëŒ€í•œ ì¤‘ìš”í•œ ì°¸ê³  ì‚¬í•­**

ë§ì€ ì´ë¯¸ì§€ ëª¨ë¸ì—ëŠ” `BatchNormalization` ë ˆì´ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ì´ ë ˆì´ì–´ëŠ” ëª¨ë“  ë©´ì—ì„œ íŠ¹ë³„í•œ ê²½ìš°ì— í•´ë‹¹í•©ë‹ˆë‹¤.
ë‹¤ìŒ ëª‡ ê°€ì§€ ì‚¬í•­ì„ ì—¼ë‘ì— ë‘ì–´ì•¼ í•©ë‹ˆë‹¤.

- `BatchNormalization`ì€ íŠ¸ë ˆì´ë‹ ì¤‘ì— ì—…ë°ì´íŠ¸ë˜ëŠ” 2ê°œì˜ íŠ¸ë ˆì´ë‹ ë¶ˆê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
  ì´ëŠ” ì…ë ¥ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì¶”ì í•˜ëŠ” ë³€ìˆ˜ë“¤ì…ë‹ˆë‹¤.
- `bn_layer.trainable = False`ë¡œ ì„¤ì •í•˜ë©´, `BatchNormalization` ë ˆì´ì–´ëŠ” ì¶”ë¡  ëª¨ë“œì—ì„œ ì‹¤í–‰ë˜ë©°,
  í‰ê· ê³¼ ë¶„ì‚° í†µê³„ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
  ì´ëŠ” ì¼ë°˜ì ì¸ ë‹¤ë¥¸ ë ˆì´ì–´ì—ì„œëŠ” í•´ë‹¹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
  [ê°€ì¤‘ì¹˜ì˜ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥ ì—¬ë¶€ì™€ ì¶”ë¡ /íŠ¸ë ˆì´ë‹ ëª¨ë“œëŠ” ì„œë¡œ ë…ë¦½ì ì¸ ê°œë…]({{< relref "/docs/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute" >}})ì´ì§€ë§Œ,
  `BatchNormalization` ë ˆì´ì–´ì—ì„œëŠ” ì´ ë‘˜ì´ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ `BatchNormalization` ë ˆì´ì–´ê°€ í¬í•¨ëœ ëª¨ë¸ì„ ë™ê²° í•´ì œí•˜ëŠ” ê²½ìš°,
  ë² ì´ìŠ¤ ëª¨ë¸ì„ í˜¸ì¶œí•  ë•Œ `training=False`ë¥¼ ì „ë‹¬í•˜ì—¬,
  `BatchNormalization` ë ˆì´ì–´ê°€ ì¶”ë¡  ëª¨ë“œì—ì„œ ìœ ì§€ë˜ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤.
  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´, íŠ¸ë ˆì´ë‹ ë¶ˆê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ì— ì ìš©ëœ ì—…ë°ì´íŠ¸ê°€ ëª¨ë¸ì´ í•™ìŠµí•œ ë‚´ìš©ì„ ê°‘ìê¸° íŒŒê´´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ê°€ì´ë“œ ëë¶€ë¶„ì˜ ì—”ë“œíˆ¬ì—”ë“œ ì˜ˆì œì—ì„œ, ì´ íŒ¨í„´ì´ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì—”ë“œíˆ¬ì—”ë“œ ì˜ˆì‹œ: cats vs. dogs ë°ì´í„°ì…‹ì— ëŒ€í•´ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • {#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs-dogs-dataset}

ì´ ê°œë…ë“¤ì„ ë” í™•ì‹¤íˆ í•˜ê¸° ìœ„í•´, êµ¬ì²´ì ì¸ ì—”ë“œíˆ¬ì—”ë“œ ì „ì´ í•™ìŠµ ë° ë¯¸ì„¸ ì¡°ì • ì˜ˆì œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.
ìš°ë¦¬ëŠ” ImageNetì—ì„œ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ Xception ëª¨ë¸ì„ ë¡œë“œí•œ í›„,
Kaggleì˜ "cats vs. dogs" ë¶„ë¥˜ ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

### ë°ì´í„° ê°€ì ¸ì˜¤ê¸° {#getting-the-data}

ë¨¼ì €, TFDSë¥¼ ì‚¬ìš©í•˜ì—¬ cats vs. dogs ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜¤ê² ìŠµë‹ˆë‹¤.
ì—¬ëŸ¬ë¶„ì´ ì§ì ‘ ë°ì´í„°ì…‹ì„ ê°€ì§€ê³  ìˆë‹¤ë©´,
[`keras.utils.image_dataset_from_directory`]({{< relref "/docs/api/data_loading/image#image_dataset_from_directory-function" >}}) ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ì—¬,
ë””ìŠ¤í¬ì— í´ë˜ìŠ¤ë³„ í´ë”ë¡œ ë‚˜ëˆ„ì–´ì§„ ì´ë¯¸ì§€ ì§‘í•©ì—ì„œ ìœ ì‚¬í•œ ë¼ë²¨ë§ëœ ë°ì´í„°ì…‹ ê°ì²´ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì „ì´ í•™ìŠµì€ ë§¤ìš° ì‘ì€ ë°ì´í„°ì…‹ì„ ë‹¤ë£° ë•Œ ê°€ì¥ ìœ ìš©í•©ë‹ˆë‹¤.
ë°ì´í„°ì…‹ì„ ì‘ê²Œ ìœ ì§€í•˜ê¸° ìœ„í•´,
ì›ë˜ì˜ íŠ¸ë ˆì´ë‹ ë°ì´í„°(25,000ê°œì˜ ì´ë¯¸ì§€) ì¤‘ 40%ë¥¼ íŠ¸ë ˆì´ë‹ì— ì‚¬ìš©í•˜ê³ ,
10%ëŠ” ê²€ì¦, 10%ëŠ” í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

```python
tfds.disable_progress_bar()

train_ds, validation_ds, test_ds = tfds.load(
    "cats_vs_dogs",
    # 10%ë¥¼ ê²€ì¦, 10%ë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ì˜ˆì•½
    split=["train[:40%]", "train[40%:50%]", "train[50%:60%]"],
    as_supervised=True,  # ë¼ë²¨ í¬í•¨
)

print(f"Number of training samples: {train_ds.cardinality()}")
print(f"Number of validation samples: {validation_ds.cardinality()}")
print(f"Number of test samples: {test_ds.cardinality()}")
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
 Downloading and preparing dataset 786.68 MiB (download: 786.68 MiB, generated: Unknown size, total: 786.68 MiB) to /home/mattdangerw/tensorflow_datasets/cats_vs_dogs/4.0.0...

WARNING:absl:1738 images were corrupted and were skipped

 Dataset cats_vs_dogs downloaded and prepared to /home/mattdangerw/tensorflow_datasets/cats_vs_dogs/4.0.0. Subsequent calls will reuse this data.
Number of training samples: 9305
Number of validation samples: 2326
Number of test samples: 2326
```

{{% /details %}}

ë‹¤ìŒì€ íŠ¸ë ˆì´ë‹ ë°ì´í„°ì…‹ì— ìˆëŠ” ì²« 9ê°œì˜ ì´ë¯¸ì§€ì…ë‹ˆë‹¤. ë³´ì‹œë‹¤ì‹œí”¼, ëª¨ë‘ í¬ê¸°ê°€ ë‹¤ë¦…ë‹ˆë‹¤.

```python
plt.figure(figsize=(10, 10))
for i, (image, label) in enumerate(train_ds.take(9)):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(image)
    plt.title(int(label))
    plt.axis("off")
```

![png](/images/guides/transfer_learning/transfer_learning_21_0.png)

ë˜í•œ, ë¼ë²¨ 1ì€ "ê°œ"ì´ê³  ë¼ë²¨ 0ì€ "ê³ ì–‘ì´"ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë°ì´í„° í‘œì¤€í™” {#standardizing-the-data}

ì›ë³¸ ì´ë¯¸ì§€ë“¤ì€ ë‹¤ì–‘í•œ í¬ê¸°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
ë˜í•œ, ê° í”½ì…€ì€ 0ì—ì„œ 255 ì‚¬ì´ì˜ 3ê°œì˜ ì •ìˆ˜ ê°’ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. (RGB ê°’)
ì´ëŠ” ì‹ ê²½ë§ì— ì í•©í•œ ì…ë ¥ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ ì‘ì—…ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤:

- ê³ ì •ëœ ì´ë¯¸ì§€ í¬ê¸°ë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤. 150x150ì„ ì„ íƒí•©ë‹ˆë‹¤.
- í”½ì…€ ê°’ì„ -1ì—ì„œ 1 ì‚¬ì´ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ `Normalization` ë ˆì´ì–´ë¥¼ ëª¨ë¸ì˜ ì¼ë¶€ë¡œ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ, ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë³´ë‹¤ëŠ”,
raw ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
ê·¸ ì´ìœ ëŠ”, ëª¨ë¸ì´ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ê¸°ëŒ€í•  ê²½ìš°,
ëª¨ë¸ì„ ë‹¤ë¥¸ í™˜ê²½(ì˜ˆ: ì›¹ ë¸Œë¼ìš°ì €, ëª¨ë°”ì¼ ì•±)ì—ì„œ ì‚¬ìš©í•  ë•Œë§ˆë‹¤,
ë™ì¼í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ë‹¤ì‹œ êµ¬í˜„í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
ì´ëŠ” ë§¤ìš° ê¹Œë‹¤ë¡œì›Œì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë”°ë¼ì„œ, ëª¨ë¸ì— ì…ë ¥í•˜ê¸° ì „ì— ê°€ëŠ¥í•œ í•œ ì ì€ ì–‘ì˜ ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì—ì„œ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •ì„ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤.
(ë”¥ëŸ¬ë‹ ì‹ ê²½ë§ì€ ì—°ì†ì ì¸ ë°ì´í„° ë°°ì¹˜ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤)
ê·¸ë¦¬ê³  ì…ë ¥ ê°’ ìŠ¤ì¼€ì¼ë§ì€ ëª¨ë¸ì„ ë§Œë“¤ ë•Œ ëª¨ë¸ì˜ ì¼ë¶€ë¡œ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤.

ì´ë¯¸ì§€ë¥¼ 150x150 í¬ê¸°ë¡œ ì¡°ì •í•´ë´…ì‹œë‹¤:

```python
resize_fn = keras.layers.Resizing(150, 150)

train_ds = train_ds.map(lambda x, y: (resize_fn(x), y))
validation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))
test_ds = test_ds.map(lambda x, y: (resize_fn(x), y))
```

### ëœë¤ ë°ì´í„° ë³´ê°• ì‚¬ìš© {#using-random-data-augmentation}

ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì´ ì—†ì„ ë•Œ,
íŠ¸ë ˆì´ë‹ ì´ë¯¸ì§€ì— ëœë¤í•˜ì§€ë§Œ í˜„ì‹¤ì ì¸ ë³€í™˜ì„ ì ìš©í•˜ì—¬,
ì¸ìœ„ì ìœ¼ë¡œ ìƒ˜í”Œ ë‹¤ì–‘ì„±ì„ ë„ì…í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ëœë¤í•œ ê°€ë¡œ ë’¤ì§‘ê¸°ë‚˜ ì‘ì€ ê°ë„ì˜ ëœë¤ íšŒì „ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ íŠ¸ë ˆì´ë‹ ë°ì´í„°ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì— ë…¸ì¶œë˜ë©´ì„œ ê³¼ì í•©ì„ ëŠ¦ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
augmentation_layers = [
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
]


def data_augmentation(x):
    for layer in augmentation_layers:
        x = layer(x)
    return x


train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))
```

ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ê³ , ë¡œë”© ì†ë„ë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•´ prefetchingì„ ì‚¬ìš©í•´ë´…ì‹œë‹¤.

```python
from tensorflow import data as tf_data

batch_size = 64

train_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()
validation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()
test_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()
```

ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ê°€ ë‹¤ì–‘í•œ ëœë¤ ë³€í™˜ì„ ê±°ì¹œ í›„ ì–´ë–»ê²Œ ë³´ì´ëŠ”ì§€ ì‹œê°í™”í•´ë´…ì‹œë‹¤:

```python
for images, labels in train_ds.take(1):
    plt.figure(figsize=(10, 10))
    first_image = images[0]
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        augmented_image = data_augmentation(np.expand_dims(first_image, 0))
        plt.imshow(np.array(augmented_image[0]).astype("int32"))
        plt.title(int(labels[0]))
        plt.axis("off")
```

![png](/images/guides/transfer_learning/transfer_learning_30_0.png)

## ëª¨ë¸ ë¹Œë“œ {#build-a-model}

ì´ì œ ì´ì „ì— ì„¤ëª…í•œ ì„¤ê³„ë„ë¥¼ ë”°ë¥´ëŠ” ëª¨ë¸ì„ ë¹Œë“œí•´ë´…ì‹œë‹¤.

ì°¸ê³  ì‚¬í•­:

- ì…ë ¥ ê°’ì„ `[0, 255]` ë²”ìœ„ì—ì„œ `[-1, 1]` ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ê¸° ìœ„í•´, `Rescaling` ë ˆì´ì–´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
- ì •ê·œí™”ë¥¼ ìœ„í•´, ë¶„ë¥˜ ë ˆì´ì–´ ì•ì— `Dropout` ë ˆì´ì–´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
- ë² ì´ìŠ¤ ëª¨ë¸ì„ í˜¸ì¶œí•  ë•Œ `training=False`ë¥¼ ì „ë‹¬í•˜ì—¬, ë°°ì¹˜ ì •ê·œí™” í†µê³„ê°€ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šë„ë¡,
  ì¶”ë¡  ëª¨ë“œì—ì„œ ì‹¤í–‰ë˜ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤.
  ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë™ê²° í•´ì œí•œ í›„ì—ë„, ì´ ì„¤ì •ì„ ìœ ì§€í•©ë‹ˆë‹¤.

```python
base_model = keras.applications.Xception(
    weights="imagenet",  # ImageNetì—ì„œ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ.
    input_shape=(150, 150, 3),
    include_top=False,
)  # ìƒë‹¨ì— ImageNet ë¶„ë¥˜ê¸°ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ.

# `base_model` ë™ê²°
base_model.trainable = False

# ìƒë‹¨ì— ìƒˆ ëª¨ë¸ ìƒì„±
inputs = keras.Input(shape=(150, 150, 3))

# ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ Xception ê°€ì¤‘ì¹˜ëŠ” ì…ë ¥ì´ (0, 255) ë²”ìœ„ì—ì„œ
# (-1., +1.) ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
# Rescaling ë ˆì´ì–´ì˜ ì¶œë ¥ì€: `(inputs * scale) + offset`ì…ë‹ˆë‹¤.
scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)
x = scale_layer(inputs)

# ë² ì´ìŠ¤ ëª¨ë¸ì— ë°°ì¹˜ ì •ê·œí™” ë ˆì´ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
# ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë™ê²° í•´ì œí•  ë•Œë„ ì¶”ë¡  ëª¨ë“œì—ì„œ ìœ ì§€ë˜ë„ë¡
# base_modelì´ ì—¬ê¸°ì„œ ì¶”ë¡  ëª¨ë“œë¡œ ì‹¤í–‰ë˜ê²Œ í•©ë‹ˆë‹¤.
x = base_model(x, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
x = keras.layers.Dropout(0.2)(x)  # Dropoutìœ¼ë¡œ ì •ê·œí™”
outputs = keras.layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

model.summary(show_trainable=True)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5
 83683744/83683744 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 0us/step
```

```plain
Model: "functional_4"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                â”ƒ Output Shape             â”ƒ Param # â”ƒ Traiâ€¦ â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ input_layer_4 (InputLayer)  â”‚ (None, 150, 150, 3)      â”‚       0 â”‚   -   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ rescaling (Rescaling)       â”‚ (None, 150, 150, 3)      â”‚       0 â”‚   -   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ xception (Functional)       â”‚ (None, 5, 5, 2048)       â”‚ 20,861â€¦ â”‚   N   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ global_average_pooling2d    â”‚ (None, 2048)             â”‚       0 â”‚   -   â”‚
â”‚ (GlobalAveragePooling2D)    â”‚                          â”‚         â”‚       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout (Dropout)           â”‚ (None, 2048)             â”‚       0 â”‚   -   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_7 (Dense)             â”‚ (None, 1)                â”‚   2,049 â”‚   Y   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 20,863,529 (79.59 MB)
 Trainable params: 2,049 (8.00 KB)
 Non-trainable params: 20,861,480 (79.58 MB)
```

{{% /details %}}

## ìƒë‹¨ ë ˆì´ì–´ íŠ¸ë ˆì´ë‹ {#train-the-top-layer}

```python
model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[keras.metrics.BinaryAccuracy()],
)

epochs = 2
print("Fitting the top layer of the model")
model.fit(train_ds, epochs=epochs, validation_data=validation_ds)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Fitting the top layer of the model
Epoch 1/2
  78/146 â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”  15s 226ms/step - binary_accuracy: 0.7995 - loss: 0.4088

Corrupt JPEG data: 65 extraneous bytes before marker 0xd9

 136/146 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”  2s 231ms/step - binary_accuracy: 0.8430 - loss: 0.3298

Corrupt JPEG data: 239 extraneous bytes before marker 0xd9

 143/146 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”  0s 231ms/step - binary_accuracy: 0.8464 - loss: 0.3235

Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9

 144/146 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”  0s 231ms/step - binary_accuracy: 0.8468 - loss: 0.3226

Corrupt JPEG data: 228 extraneous bytes before marker 0xd9

 146/146 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 260ms/step - binary_accuracy: 0.8478 - loss: 0.3209

Corrupt JPEG data: 2226 extraneous bytes before marker 0xd9

 146/146 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 54s 317ms/step - binary_accuracy: 0.8482 - loss: 0.3200 - val_binary_accuracy: 0.9667 - val_loss: 0.0877
Epoch 2/2
 146/146 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 51ms/step - binary_accuracy: 0.9483 - loss: 0.1232 - val_binary_accuracy: 0.9705 - val_loss: 0.0786

<keras.src.callbacks.history.History at 0x7fc8b7f1db70>
```

{{% /details %}}

## ì „ì²´ ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„ {#do-a-round-of-fine-tuning-of-the-entire-model}

ë§ˆì§€ë§‰ìœ¼ë¡œ, ë² ì´ìŠ¤ ëª¨ë¸ì˜ ë™ê²°ì„ í•´ì œí•˜ê³ , ë‚®ì€ í•™ìŠµë¥ ë¡œ ì—”ë“œíˆ¬ì—”ë“œë¡œ ì „ì²´ ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•©ë‹ˆë‹¤.

ì¤‘ìš”í•œ ì ì€, ë² ì´ìŠ¤ ëª¨ë¸ì´ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•´ì§€ë”ë¼ë„,
ëª¨ë¸ì„ ë¹Œë“œí•  ë•Œ `training=False`ë¥¼ ì „ë‹¬í–ˆê¸° ë•Œë¬¸ì— ì—¬ì „íˆ ì¶”ë¡  ëª¨ë“œë¡œ ì‹¤í–‰ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì´ëŠ” ë‚´ë¶€ì˜ ë°°ì¹˜ ì •ê·œí™” ë ˆì´ì–´ë“¤ì´ ë°°ì¹˜ í†µê³„ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
ë§Œì•½ ì—…ë°ì´íŠ¸ëœë‹¤ë©´, ì§€ê¸ˆê¹Œì§€ ëª¨ë¸ì´ í•™ìŠµí•œ í‘œí˜„ì— í° í˜¼ë€ì„ ì¼ìœ¼í‚¬ ê²ƒì…ë‹ˆë‹¤.

```python
# base_modelì˜ ë™ê²° í•´ì œ.
# ëª¨ë¸ì„ í˜¸ì¶œí•  ë•Œ `training=False`ë¥¼ ì „ë‹¬í–ˆìœ¼ë¯€ë¡œ
# ì—¬ì „íˆ ì¶”ë¡  ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
# ì¦‰, ë°°ì¹˜ ì •ê·œí™” ë ˆì´ì–´ëŠ” ë°°ì¹˜ í†µê³„ë¥¼
# ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šìœ¼ë©°, ì´ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ íŠ¸ë ˆì´ë‹ì„ ë¬´íš¨í™”í•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
base_model.trainable = True
model.summary(show_trainable=True)

model.compile(
    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[keras.metrics.BinaryAccuracy()],
)

epochs = 1
print("Fitting the end-to-end model")
model.fit(train_ds, epochs=epochs, validation_data=validation_ds)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Model: "functional_4"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                â”ƒ Output Shape             â”ƒ Param # â”ƒ Traiâ€¦ â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ input_layer_4 (InputLayer)  â”‚ (None, 150, 150, 3)      â”‚       0 â”‚   -   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ rescaling (Rescaling)       â”‚ (None, 150, 150, 3)      â”‚       0 â”‚   -   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ xception (Functional)       â”‚ (None, 5, 5, 2048)       â”‚ 20,861â€¦ â”‚   Y   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ global_average_pooling2d    â”‚ (None, 2048)             â”‚       0 â”‚   -   â”‚
â”‚ (GlobalAveragePooling2D)    â”‚                          â”‚         â”‚       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout (Dropout)           â”‚ (None, 2048)             â”‚       0 â”‚   -   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_7 (Dense)             â”‚ (None, 1)                â”‚   2,049 â”‚   Y   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 20,867,629 (79.60 MB)
 Trainable params: 20,809,001 (79.38 MB)
 Non-trainable params: 54,528 (213.00 KB)
 Optimizer params: 4,100 (16.02 KB)
```

```plain
Fitting the end-to-end model
 146/146 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 75s 327ms/step - binary_accuracy: 0.8487 - loss: 0.3760 - val_binary_accuracy: 0.9494 - val_loss: 0.1160

<keras.src.callbacks.history.History at 0x7fcd1c755090>
```

{{% /details %}}

10 ì—í¬í¬ í›„, ë¯¸ì„¸ ì¡°ì •ì„ í†µí•´ ì„±ëŠ¥ì´ ìƒë‹¹íˆ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•´ë´…ì‹œë‹¤:

```python
print("Test dataset evaluation")
model.evaluate(test_ds)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Test dataset evaluation
 11/37 â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  1s 52ms/step - binary_accuracy: 0.9407 - loss: 0.1155

Corrupt JPEG data: 99 extraneous bytes before marker 0xd9

 37/37 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 47ms/step - binary_accuracy: 0.9427 - loss: 0.1259

[0.13755160570144653, 0.941300630569458]
```

{{% /details %}}
