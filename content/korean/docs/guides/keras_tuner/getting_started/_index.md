---
title: KerasTuner ì‹œì‘í•˜ê¸°
linkTitle: KerasTuner ì‹œì‘í•˜ê¸°
toc: true
weight: 1
type: docs
---

{{< keras/original checkedAt="2024-11-18" >}}

**{{< t f_author >}}** Luca Invernizzi, James Long, Francois Chollet, Tom O'Malley, Haifeng Jin  
**{{< t f_date_created >}}** 2019/05/31  
**{{< t f_last_modified >}}** 2021/10/27  
**{{< t f_description >}}** ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•œ KerasTuner ì‚¬ìš© ê¸°ë³¸ ì‚¬í•­.

{{< cards cols="2" >}}
{{< card link="https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/keras_tuner/getting_started.ipynb" title="Colab" tag="Colab" tagType="warning">}}
{{< card link="https://github.com/keras-team/keras-io/blob/master/guides/keras_tuner/getting_started.py" title="GitHub" tag="GitHub">}}
{{< /cards >}}

```python
!pip install keras-tuner -q
```

## ì†Œê°œ {#introduction}

KerasTunerëŠ” ë²”ìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
Keras ì›Œí¬í”Œë¡œìš°ì™€ì˜ ê°•ë ¥í•œ í†µí•©ì„ ì œê³µí•˜ì§€ë§Œ, ì´ì— êµ­í•œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
scikit-learn ëª¨ë¸ì„ íŠœë‹í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì‘ì—…ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” KerasTunerë¥¼ ì‚¬ìš©í•˜ì—¬
ëª¨ë¸ ì•„í‚¤í…ì²˜, íŠ¸ë ˆì´ë‹ ê³¼ì • ë° ë°ì´í„° ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ íŠœë‹í•˜ëŠ” ë°©ë²•ì„ ë°°ìš¸ ê²ƒì…ë‹ˆë‹¤.
ê°„ë‹¨í•œ ì˜ˆì œë¶€í„° ì‹œì‘í•´ë´…ì‹œë‹¤.

## ëª¨ë¸ ì•„í‚¤í…ì²˜ íŠœë‹ {#tune-the-model-architecture}

ë¨¼ì €, ì»´íŒŒì¼ëœ Keras ëª¨ë¸ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
ì´ í•¨ìˆ˜ëŠ” ëª¨ë¸ì„ ë¹Œë“œí•  ë•Œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ê¸° ìœ„í•œ `hp` ì¸ìë¥¼ ë°›ìŠµë‹ˆë‹¤.

### ê²€ìƒ‰ ê³µê°„ ì •ì˜ {#define-the-search-space}

ë‹¤ìŒ ì½”ë“œ ì˜ˆì œì—ì„œëŠ”, ë‘ ê°œì˜ `Dense` ë ˆì´ì–´ë¡œ êµ¬ì„±ëœ Keras ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
ì²« ë²ˆì§¸ `Dense` ë ˆì´ì–´ì˜ ìœ ë‹› ìˆ˜ë¥¼ íŠœë‹í•˜ë ¤ê³  í•©ë‹ˆë‹¤.
ì´ë¥¼ ìœ„í•´ `hp.Int('units', min_value=32, max_value=512, step=32)`ë¡œ ì •ìˆ˜í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
ì´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” 32ì—ì„œ 512ê¹Œì§€ì˜ ë²”ìœ„ë¥¼ ê°€ì§€ë©°, ìƒ˜í”Œë§í•  ë•Œ ìµœì†Œ ë‹¨ìœ„ëŠ” 32ì…ë‹ˆë‹¤.

```python
import keras
from keras import layers


def build_model(hp):
    model = keras.Sequential()
    model.add(layers.Flatten())
    model.add(
        layers.Dense(
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
            units=hp.Int("units", min_value=32, max_value=512, step=32),
            activation="relu",
        )
    )
    model.add(layers.Dense(10, activation="softmax"))
    model.compile(
        optimizer="adam",
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model
```

ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¹Œë“œë˜ëŠ”ì§€ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import keras_tuner

build_model(keras_tuner.HyperParameters())
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
<Sequential name=sequential, built=False>
```

{{% /details %}}

ì—¬ëŸ¬ íƒ€ì…ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
ë‹¤ìŒ ì½”ë“œì—ì„œëŠ” `Dropout` ë ˆì´ì–´ë¥¼ ì‚¬ìš©í• ì§€ ì—¬ë¶€ë¥¼ `hp.Boolean()`ìœ¼ë¡œ íŠœë‹í•˜ê³ ,
í™œì„±í™” í•¨ìˆ˜ëŠ” `hp.Choice()`ë¡œ ì„ íƒí•˜ë©°,
ì˜µí‹°ë§ˆì´ì €ì˜ í•™ìŠµë¥ ì€ `hp.Float()`ë¡œ íŠœë‹í•©ë‹ˆë‹¤.

```python
def build_model(hp):
    model = keras.Sequential()
    model.add(layers.Flatten())
    model.add(
        layers.Dense(
            # ìœ ë‹› ìˆ˜ íŠœë‹.
            units=hp.Int("units", min_value=32, max_value=512, step=32),
            # ì‚¬ìš©í•  í™œì„±í™” í•¨ìˆ˜ íŠœë‹.
            activation=hp.Choice("activation", ["relu", "tanh"]),
        )
    )
    # ë“œë¡­ì•„ì›ƒ ì‚¬ìš© ì—¬ë¶€ íŠœë‹.
    if hp.Boolean("dropout"):
        model.add(layers.Dropout(rate=0.25))
    model.add(layers.Dense(10, activation="softmax"))
    # ì˜µí‹°ë§ˆì´ì € í•™ìŠµë¥ ì„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì •ì˜.
    learning_rate = hp.Float("lr", min_value=1e-4, max_value=1e-2, sampling="log")
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model


build_model(keras_tuner.HyperParameters())
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
<Sequential name=sequential_1, built=False>
```

{{% /details %}}

ì•„ë˜ì— ë³´ì´ëŠ” ê²ƒì²˜ëŸ¼, í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì‹¤ì œ ê°’ì…ë‹ˆë‹¤.
ì‚¬ì‹¤, ì´ëŠ” ì‹¤ì œ ê°’ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì¼ ë¿ì…ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, `hp.Int()`ëŠ” `int` ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
ë”°ë¼ì„œ, ì´ë¥¼ ë³€ìˆ˜, for ë£¨í”„, ë˜ëŠ” if ì¡°ê±´ë¬¸ì— ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
hp = keras_tuner.HyperParameters()
print(hp.Int("units", min_value=32, max_value=512, step=32))
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
32
```

{{% /details %}}

í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë¯¸ë¦¬ ì •ì˜í•˜ê³  Keras ì½”ë“œë¥¼ ë³„ë„ì˜ í•¨ìˆ˜ì— ë‘˜ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```python
def call_existing_code(units, activation, dropout, lr):
    model = keras.Sequential()
    model.add(layers.Flatten())
    model.add(layers.Dense(units=units, activation=activation))
    if dropout:
        model.add(layers.Dropout(rate=0.25))
    model.add(layers.Dense(10, activation="softmax"))
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=lr),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model


def build_model(hp):
    units = hp.Int("units", min_value=32, max_value=512, step=32)
    activation = hp.Choice("activation", ["relu", "tanh"])
    dropout = hp.Boolean("dropout")
    lr = hp.Float("lr", min_value=1e-4, max_value=1e-2, sampling="log")
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ëª¨ë¸ ë¹Œë“œ ì½”ë“œë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    model = call_existing_code(
        units=units, activation=activation, dropout=dropout, lr=lr
    )
    return model


build_model(keras_tuner.HyperParameters())
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
<Sequential name=sequential_2, built=False>
```

{{% /details %}}

ê° í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì´ë¦„(ì²« ë²ˆì§¸ ì¸ì)ìœ¼ë¡œ ê³ ìœ í•˜ê²Œ ì‹ë³„ë©ë‹ˆë‹¤.
ì„œë¡œ ë‹¤ë¥¸ `Dense` ë ˆì´ì–´ì—ì„œ ìœ ë‹› ìˆ˜ë¥¼ ë³„ë„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì¡°ì •í•˜ë ¤ë©´,
ê·¸ë“¤ì—ê²Œ `f"units_{i}"`ì™€ ê°™ì´ ë‹¤ë¥¸ ì´ë¦„ì„ ë¶€ì—¬í•˜ë©´ ë©ë‹ˆë‹¤.

ë˜í•œ, ì´ëŠ” ì¡°ê±´ë¶€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•˜ëŠ” ì˜ˆì‹œì´ê¸°ë„ í•©ë‹ˆë‹¤.
`Dense` ë ˆì´ì–´ì—ì„œ ìœ ë‹› ìˆ˜ë¥¼ ì§€ì •í•˜ëŠ” ë§ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì¡´ì¬í•˜ë©°,
ì´ëŸ¬í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ëŠ” ë ˆì´ì–´ ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
ë ˆì´ì–´ ìˆ˜ ìì²´ë„ í•˜ë‚˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì´ê¸° ë•Œë¬¸ì—,
ì „ì²´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” ì‹œë„í•  ë•Œë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì–´ë–¤ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” íŠ¹ì • ì¡°ê±´ì´ ì¶©ì¡±ë  ë•Œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, `units_3`ëŠ” `num_layers`ê°€ 3ë³´ë‹¤ í´ ë•Œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
KerasTunerë¥¼ ì‚¬ìš©í•˜ë©´, ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì´ëŸ¬í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ì‰½ê²Œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
def build_model(hp):
    model = keras.Sequential()
    model.add(layers.Flatten())
    # ë ˆì´ì–´ ìˆ˜ë¥¼ íŠœë‹í•©ë‹ˆë‹¤.
    for i in range(hp.Int("num_layers", 1, 3)):
        model.add(
            layers.Dense(
                # ê° ë ˆì´ì–´ì˜ ìœ ë‹› ìˆ˜ë¥¼ ê°œë³„ì ìœ¼ë¡œ íŠœë‹í•©ë‹ˆë‹¤.
                units=hp.Int(f"units_{i}", min_value=32, max_value=512, step=32),
                activation=hp.Choice("activation", ["relu", "tanh"]),
            )
        )
    if hp.Boolean("dropout"):
        model.add(layers.Dropout(rate=0.25))
    model.add(layers.Dense(10, activation="softmax"))
    learning_rate = hp.Float("lr", min_value=1e-4, max_value=1e-2, sampling="log")
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model


build_model(keras_tuner.HyperParameters())
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
<Sequential name=sequential_3, built=False>
```

{{% /details %}}

### ê²€ìƒ‰ ì‹œì‘í•˜ê¸° {#start-the-search}

íƒìƒ‰ ê³µê°„ì„ ì •ì˜í•œ í›„, íƒìƒ‰ì„ ì‹¤í–‰í•  íŠœë„ˆ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.
`RandomSearch`, `BayesianOptimization`, `Hyperband` ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìœ¼ë©°,
ì´ëŠ” ê°ê° ë‹¤ë¥¸ íŠœë‹ ì•Œê³ ë¦¬ì¦˜ì— í•´ë‹¹í•©ë‹ˆë‹¤.
ì—¬ê¸°ì„œëŠ” `RandomSearch`ë¥¼ ì˜ˆë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

íŠœë„ˆë¥¼ ì´ˆê¸°í™”í•˜ë ¤ë©´, ì´ë‹ˆì…œë¼ì´ì €ì—ì„œ ì—¬ëŸ¬ ì¸ìë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.

- `hypermodel`. ëª¨ë¸ì„ ë¹Œë“œí•˜ëŠ” í•¨ìˆ˜ë¡œ, ì—¬ê¸°ì„œëŠ” `build_model`ì´ í•´ë‹¹ë©ë‹ˆë‹¤.
- `objective`. ìµœì í™”í•  ëª©í‘œì˜ ì´ë¦„ì…ë‹ˆë‹¤. (ë¹ŒíŠ¸ì¸ ì§€í‘œì˜ ê²½ìš°, ìµœì†Œí™” ë˜ëŠ” ìµœëŒ€í™” ì—¬ë¶€ëŠ” ìë™ìœ¼ë¡œ ì¶”ë¡ ë©ë‹ˆë‹¤)
  ì´ íŠœí† ë¦¬ì–¼ í›„ë°˜ë¶€ì—ì„œ ì»¤ìŠ¤í…€ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•  ì˜ˆì •ì…ë‹ˆë‹¤.
- `max_trials`. íƒìƒ‰ ì¤‘ ì‹¤í–‰í•  ì´ ì‹¤í—˜ íšŸìˆ˜ì…ë‹ˆë‹¤.
- `executions_per_trial`. ê° ì‹¤í—˜ì—ì„œ êµ¬ì¶•ë˜ê³  fit ë˜ì–´ì•¼ í•˜ëŠ” ëª¨ë¸ì˜ ìˆ˜ì…ë‹ˆë‹¤.
  ì„œë¡œ ë‹¤ë¥¸ ì‹¤í—˜ì€ ì„œë¡œ ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.
  ë™ì¼í•œ ì‹¤í—˜ ë‚´ì—ì„œëŠ” ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.
  ê° ì‹¤í—˜ì—ì„œ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ëŠ” ëª©ì ì€ ê²°ê³¼ì˜ ë¶„ì‚°ì„ ì¤„ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë” ì •í™•í•˜ê²Œ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
  ë” ë¹ ë¥´ê²Œ ê²°ê³¼ë¥¼ ì–»ê³  ì‹¶ë‹¤ë©´, `executions_per_trial=1`ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  (ê° ëª¨ë¸ êµ¬ì„±ì— ëŒ€í•´ í•œ ë²ˆì˜ íŠ¸ë ˆì´ë‹ ë¼ìš´ë“œë§Œ ì‹¤í–‰)
- `overwrite`. ë™ì¼í•œ ë””ë ‰í† ë¦¬ì—ì„œ ì´ì „ ê²°ê³¼ë¥¼ ë®ì–´ì“¸ì§€ ì•„ë‹ˆë©´ ì´ì „ íƒìƒ‰ì„ ë‹¤ì‹œ ì‹œì‘í• ì§€ë¥¼ ì œì–´í•©ë‹ˆë‹¤.
  ì—¬ê¸°ì„œëŠ” `overwrite=True`ë¡œ ì„¤ì •í•˜ì—¬ ìƒˆ íƒìƒ‰ì„ ì‹œì‘í•˜ê³  ì´ì „ ê²°ê³¼ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.
- `directory`. íƒìƒ‰ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œì…ë‹ˆë‹¤.
- `project_name`. `directory` ë‚´ì— ì €ì¥í•  í•˜ìœ„ ë””ë ‰í† ë¦¬ì˜ ì´ë¦„ì…ë‹ˆë‹¤.

```python
tuner = keras_tuner.RandomSearch(
    hypermodel=build_model,
    objective="val_accuracy",
    max_trials=3,
    executions_per_trial=2,
    overwrite=True,
    directory="my_dir",
    project_name="helloworld",
)
```

íƒìƒ‰ ê³µê°„ ìš”ì•½ì„ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
tuner.search_space_summary()
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Search space summary
Default search space size: 5
num_layers (Int)
{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}
units_0 (Int)
{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}
activation (Choice)
{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}
dropout (Boolean)
{'default': False, 'conditions': []}
lr (Float)
{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}
```

{{% /details %}}

íƒìƒ‰ì„ ì‹œì‘í•˜ê¸° ì „ì—, MNIST ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

```python
import keras
import numpy as np

(x, y), (x_test, y_test) = keras.datasets.mnist.load_data()

x_train = x[:-10000]
x_val = x[-10000:]
y_train = y[:-10000]
y_val = y[-10000:]

x_train = np.expand_dims(x_train, -1).astype("float32") / 255.0
x_val = np.expand_dims(x_val, -1).astype("float32") / 255.0
x_test = np.expand_dims(x_test, -1).astype("float32") / 255.0

num_classes = 10
y_train = keras.utils.to_categorical(y_train, num_classes)
y_val = keras.utils.to_categorical(y_val, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
```

ê·¸ëŸ° ë‹¤ìŒ, ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° êµ¬ì„±ì„ íƒìƒ‰í•©ë‹ˆë‹¤.
`search`ì— ì „ë‹¬ëœ ëª¨ë“  ì¸ìëŠ” ê° ì‹¤í–‰ì—ì„œ `model.fit()`ì— ì „ë‹¬ë©ë‹ˆë‹¤.
ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•´ `validation_data`ë¥¼ ë°˜ë“œì‹œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.

```python
tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Trial 3 Complete [00h 00m 19s]
val_accuracy: 0.9665500223636627
```

```plain
Best val_accuracy So Far: 0.9665500223636627
Total elapsed time: 00h 00m 40s
```

{{% /details %}}

`search`ê°€ ì§„í–‰ë˜ëŠ” ë™ì•ˆ,
ëª¨ë¸ ë¹Œë“œ í•¨ìˆ˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì‹¤í—˜ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ìœ¼ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤.
ê° ì‹¤í—˜ì—ì„œ íŠœë„ˆëŠ” ìƒˆë¡œìš´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ìƒì„±í•˜ì—¬ ëª¨ë¸ì„ ë¹Œë“œí•©ë‹ˆë‹¤.
ê·¸ëŸ° ë‹¤ìŒ ëª¨ë¸ì„ fití•˜ê³  í‰ê°€í•©ë‹ˆë‹¤. ì´ë•Œ ë©”íŠ¸ë¦­ì´ ê¸°ë¡ë©ë‹ˆë‹¤.
íŠœë„ˆëŠ” íƒìƒ‰ ê³µê°„ì„ ì ì§„ì ìœ¼ë¡œ íƒìƒ‰í•˜ë©°, ê²°êµ­ ì¢‹ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.

### ê²°ê³¼ ì¡°íšŒ {#query-the-results}

íƒìƒ‰ì´ ì™„ë£Œë˜ë©´, ìµœì ì˜ ëª¨ë¸ì„ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ëª¨ë¸ì€ `validation_data`ì— ëŒ€í•´ í‰ê°€í•œ ê²°ê³¼ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ì—í¬í¬ì—ì„œ ì €ì¥ë©ë‹ˆë‹¤.

```python
# ìƒìœ„ 2ê°œì˜ ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
models = tuner.get_best_models(num_models=2)
best_model = models[0]
best_model.summary()
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
/usr/local/python/3.10.13/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:388: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables.
  trackable.load_own_variables(weights_store.get(inner_path))
/usr/local/python/3.10.13/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:388: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables.
  trackable.load_own_variables(weights_store.get(inner_path))
```

```plain
Model: "sequential"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                    â”ƒ Output Shape              â”ƒ    Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ flatten (Flatten)               â”‚ (32, 784)                 â”‚          0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense (Dense)                   â”‚ (32, 416)                 â”‚    326,560 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_1 (Dense)                 â”‚ (32, 512)                 â”‚    213,504 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_2 (Dense)                 â”‚ (32, 32)                  â”‚     16,416 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout (Dropout)               â”‚ (32, 32)                  â”‚          0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_3 (Dense)                 â”‚ (32, 10)                  â”‚        330 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 556,810 (2.12 MB)
 Trainable params: 556,810 (2.12 MB)
 Non-trainable params: 0 (0.00 B)
```

{{% /details %}}

íƒìƒ‰ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```python
tuner.results_summary()
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Results summary
Results in my_dir/helloworld
Showing 10 best trials
Objective(name="val_accuracy", direction="max")
```

```plain
Trial 2 summary
Hyperparameters:
num_layers: 3
units_0: 416
activation: relu
dropout: True
lr: 0.0001324166048504802
units_1: 512
units_2: 32
Score: 0.9665500223636627
```

```plain
Trial 0 summary
Hyperparameters:
num_layers: 1
units_0: 128
activation: tanh
dropout: False
lr: 0.001425162921397599
Score: 0.9623999893665314
```

```plain
Trial 1 summary
Hyperparameters:
num_layers: 2
units_0: 512
activation: tanh
dropout: True
lr: 0.0010584293918512798
units_1: 32
Score: 0.9606499969959259
```

{{% /details %}}

`my_dir/helloworld` í´ë”, ì¦‰ `directory/project_name`ì— ìì„¸í•œ ë¡œê·¸, ì²´í¬í¬ì¸íŠ¸ ë“±ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë˜í•œ, TensorBoardì™€ HParams í”ŒëŸ¬ê·¸ì¸ì„ ì‚¬ìš©í•˜ì—¬ íŠœë‹ ê²°ê³¼ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ìì„¸í•œ ë‚´ìš©ì€ {{< titledRelref "/docs/guides/keras_tuner/visualize_tuning" >}} ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

### ëª¨ë¸ ì¬íŠ¸ë ˆì´ë‹ {#retrain-the-model}

ì „ì²´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‹¤ì‹œ íŠ¸ë ˆì´ë‹í•˜ê³  ì‹¶ë‹¤ë©´,
ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê²€ìƒ‰í•´ ìŠ¤ìŠ¤ë¡œ ì§ì ‘ ëª¨ë¸ì„ ì¬íŠ¸ë ˆì´ë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ìµœì ì˜ 2ê°œì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
best_hps = tuner.get_best_hyperparameters(5)
# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ ë¹Œë“œí•©ë‹ˆë‹¤.
model = build_model(best_hps[0])
# ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ íŠ¸ë ˆì´ë‹í•©ë‹ˆë‹¤.
x_all = np.concatenate((x_train, x_val))
y_all = np.concatenate((y_train, y_val))
model.fit(x=x_all, y=y_all, epochs=1)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
1/1875 [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  17:57 575ms/step - accuracy: 0.1250 - loss: 2.3113

29/1875 [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 2ms/step - accuracy: 0.1753 - loss: 2.2296


63/1875 [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 2ms/step - accuracy: 0.2626 - loss: 2.1206


96/1875 â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - accuracy: 0.3252 - loss: 2.0103


130/1875 â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - accuracy: 0.3745 - loss: 1.9041


164/1875 â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - accuracy: 0.4139 - loss: 1.8094


199/1875 â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - accuracy: 0.4470 - loss: 1.7246


235/1875 â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - accuracy: 0.4752 - loss: 1.6493


270/1875 â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - accuracy: 0.4982 - loss: 1.5857


305/1875 â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - accuracy: 0.5182 - loss: 1.5293


339/1875 â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - accuracy: 0.5354 - loss: 1.4800


374/1875 â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 1ms/step - accuracy: 0.5513 - loss: 1.4340


409/1875 â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 1ms/step - accuracy: 0.5656 - loss: 1.3924


444/1875 â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 1ms/step - accuracy: 0.5785 - loss: 1.3545


478/1875 â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 1ms/step - accuracy: 0.5899 - loss: 1.3208


513/1875 â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 1ms/step - accuracy: 0.6006 - loss: 1.2887


548/1875 â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6104 - loss: 1.2592


583/1875 â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6195 - loss: 1.2318


618/1875 â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6279 - loss: 1.2063


653/1875 â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6358 - loss: 1.1823


688/1875 â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6431 - loss: 1.1598


723/1875 â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6500 - loss: 1.1387


758/1875 â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6564 - loss: 1.1189


793/1875 â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6625 - loss: 1.1002


828/1875 â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6682 - loss: 1.0826


863/1875 â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6736 - loss: 1.0658


899/1875 â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6788 - loss: 1.0495


935/1875 â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6838 - loss: 1.0339


970/1875 â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6885 - loss: 1.0195


1005/1875 â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6929 - loss: 1.0058


1041/1875 â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.6972 - loss: 0.9923


1076/1875 â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.7012 - loss: 0.9798


1111/1875 â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.7051 - loss: 0.9677


1146/1875 â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.7088 - loss: 0.9561


1182/1875 â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â” 1s 1ms/step - accuracy: 0.7124 - loss: 0.9446


1218/1875 â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â” 0s 1ms/step - accuracy: 0.7159 - loss: 0.9336


1254/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â” 0s 1ms/step - accuracy: 0.7193 - loss: 0.9230


1289/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â” 0s 1ms/step - accuracy: 0.7225 - loss: 0.9131


1324/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â” 0s 1ms/step - accuracy: 0.7255 - loss: 0.9035


1359/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â” 0s 1ms/step - accuracy: 0.7284 - loss: 0.8943


1394/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â” 0s 1ms/step - accuracy: 0.7313 - loss: 0.8853


1429/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â” 0s 1ms/step - accuracy: 0.7341 - loss: 0.8767


1465/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â” 0s 1ms/step - accuracy: 0.7368 - loss: 0.8680


1500/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â” 0s 1ms/step - accuracy: 0.7394 - loss: 0.8599


1535/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â” 0s 1ms/step - accuracy: 0.7419 - loss: 0.8520


1570/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â” 0s 1ms/step - accuracy: 0.7443 - loss: 0.8444


1605/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â” 0s 1ms/step - accuracy: 0.7467 - loss: 0.8370


1639/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â” 0s 1ms/step - accuracy: 0.7489 - loss: 0.8299


1674/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â” 0s 1ms/step - accuracy: 0.7511 - loss: 0.8229


1707/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â” 0s 1ms/step - accuracy: 0.7532 - loss: 0.8164


1741/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â” 0s 1ms/step - accuracy: 0.7552 - loss: 0.8099


1774/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â” 0s 1ms/step - accuracy: 0.7572 - loss: 0.8038


1809/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ” 0s 1ms/step - accuracy: 0.7592 - loss: 0.7975


1843/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ” 0s 1ms/step - accuracy: 0.7611 - loss: 0.7915


1875/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 1ms/step - accuracy: 0.7629 - loss: 0.7858

<keras.src.callbacks.history.History at 0x7f31883d9e10>
```

{{% /details %}}

## ëª¨ë¸ íŠ¸ë ˆì´ë‹ íŠœë‹ {#tune-model-training}

ëª¨ë¸ ë¹Œë”© í”„ë¡œì„¸ìŠ¤ë¥¼ íŠœë‹í•˜ë ¤ë©´, `HyperModel` í´ë˜ìŠ¤ë¥¼ ì„œë¸Œí´ë˜ì‹±í•´ì•¼ í•©ë‹ˆë‹¤.
ì´ë¥¼ í†µí•´ í•˜ì´í¼ëª¨ë¸ì„ ì‰½ê²Œ ê³µìœ í•˜ê³  ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ ë¹Œë”©ê³¼ íŠ¸ë ˆì´ë‹ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°ê° íŠœë‹í•˜ë ¤ë©´,
`HyperModel.build()`ì™€ `HyperModel.fit()`ì„ ì˜¤ë²„ë¼ì´ë“œí•´ì•¼ í•©ë‹ˆë‹¤.
`HyperModel.build()` ë©”ì„œë“œëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬,
Keras ëª¨ë¸ì„ ìƒì„±í•˜ê³  ë°˜í™˜í•˜ëŠ” ëª¨ë¸ ë¹Œë”© í•¨ìˆ˜ì™€ ë™ì¼í•©ë‹ˆë‹¤.

`HyperModel.fit()`ì—ì„œëŠ”, `HyperModel.build()`ì—ì„œ ë°˜í™˜ëœ ëª¨ë¸, `hp`,
ê·¸ë¦¬ê³  `search()`ì— ì „ë‹¬ëœ ëª¨ë“  ì¸ìì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•œ í›„ íŠ¸ë ˆì´ë‹ íˆìŠ¤í† ë¦¬ë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì½”ë“œì—ì„œëŠ”, `model.fit()`ì—ì„œ `shuffle` ì¸ìë¥¼ íŠœë‹í•©ë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ ì—í¬í¬ ìˆ˜ë¥¼ íŠœë‹í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.
ì™œëƒí•˜ë©´ `model.fit()`ì— ë‚´ì¥ëœ ì½œë°±ì´ ì „ë‹¬ë˜ì–´,
`validation_data`ë¡œ í‰ê°€ëœ ê°€ì¥ ì¢‹ì€ ì—í¬í¬ì—ì„œ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

> **ì°¸ê³ **: `**kwargs`ëŠ” í•­ìƒ `model.fit()`ì— ì „ë‹¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
> ì—¬ê¸°ì—ëŠ” ëª¨ë¸ ì €ì¥ ë° TensorBoard í”ŒëŸ¬ê·¸ì¸ì„ ìœ„í•œ ì½œë°±ì´ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

```python
class MyHyperModel(keras_tuner.HyperModel):
    def build(self, hp):
        model = keras.Sequential()
        model.add(layers.Flatten())
        model.add(
            layers.Dense(
                units=hp.Int("units", min_value=32, max_value=512, step=32),
                activation="relu",
            )
        )
        model.add(layers.Dense(10, activation="softmax"))
        model.compile(
            optimizer="adam",
            loss="categorical_crossentropy",
            metrics=["accuracy"],
        )
        return model

    def fit(self, hp, model, *args, **kwargs):
        return model.fit(
            *args,
            # ê° ì—í¬í¬ì—ì„œ ë°ì´í„°ë¥¼ ì…”í”Œí• ì§€ ì—¬ë¶€ë¥¼ íŠœë‹.
            shuffle=hp.Boolean("shuffle"),
            **kwargs,
        )
```

ë‹¤ì‹œ í•œ ë²ˆ, ì½”ë“œë¥¼ ë¹ ë¥´ê²Œ í™•ì¸í•˜ì—¬ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
hp = keras_tuner.HyperParameters()
hypermodel = MyHyperModel()
model = hypermodel.build(hp)
hypermodel.fit(hp, model, np.random.rand(100, 28, 28), np.random.rand(100, 10))
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
1/4 â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 279ms/step - accuracy: 0.0000e+00 - loss: 12.2230


4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 108ms/step - accuracy: 0.0679 - loss: 11.9568


4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 109ms/step - accuracy: 0.0763 - loss: 11.8941

<keras.src.callbacks.history.History at 0x7f318865c100>
```

{{% /details %}}

## ë°ì´í„° ì „ì²˜ë¦¬ íŠœë‹ {#tune-data-preprocessing}

ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ íŠœë‹í•˜ë ¤ë©´, `HyperModel.fit()`ì—ì„œ,
ì¸ìˆ˜ë¡œë¶€í„° ë°ì´í„°ì…‹ì— ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡, ì¶”ê°€ì ì¸ ë‹¨ê³„ë§Œ ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.
ë‹¤ìŒ ì½”ë“œì—ì„œëŠ”, íŠ¸ë ˆì´ë‹ ì „ì— ë°ì´í„°ë¥¼ ì •ê·œí™”í• ì§€ ì—¬ë¶€ë¥¼ íŠœë‹í•©ë‹ˆë‹¤.
ì´ë²ˆì—ëŠ” `x`ì™€ `y`ë¥¼ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ì— ëª…ì‹œì ìœ¼ë¡œ ë„£ì—ˆëŠ”ë°,
ì´ëŠ” ìš°ë¦¬ê°€ ì´ ê°’ë“¤ì„ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

```python
class MyHyperModel(keras_tuner.HyperModel):
    def build(self, hp):
        model = keras.Sequential()
        model.add(layers.Flatten())
        model.add(
            layers.Dense(
                units=hp.Int("units", min_value=32, max_value=512, step=32),
                activation="relu",
            )
        )
        model.add(layers.Dense(10, activation="softmax"))
        model.compile(
            optimizer="adam",
            loss="categorical_crossentropy",
            metrics=["accuracy"],
        )
        return model

    def fit(self, hp, model, x, y, **kwargs):
        if hp.Boolean("normalize"):
            x = layers.Normalization()(x)
        return model.fit(
            x,
            y,
            # ê° ì—í¬í¬ì—ì„œ ë°ì´í„°ë¥¼ ì…”í”Œí• ì§€ ì—¬ë¶€ë¥¼ íŠœë‹.
            shuffle=hp.Boolean("shuffle"),
            **kwargs,
        )


hp = keras_tuner.HyperParameters()
hypermodel = MyHyperModel()
model = hypermodel.build(hp)
hypermodel.fit(hp, model, np.random.rand(100, 28, 28), np.random.rand(100, 10))
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
1/4 â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 276ms/step - accuracy: 0.1250 - loss: 12.0090


4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 94ms/step - accuracy: 0.0994 - loss: 12.1242


4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 95ms/step - accuracy: 0.0955 - loss: 12.1594

<keras.src.callbacks.history.History at 0x7f31ba836200>
```

{{% /details %}}

í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ `build()`ì™€ `fit()` ëª¨ë‘ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê²½ìš°,
`build()`ì—ì„œ ì •ì˜í•˜ê³  `hp.get(hp_name)`ì„ ì‚¬ìš©í•˜ì—¬,
`fit()`ì—ì„œ í•´ë‹¹ ê°’ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì˜ˆë¡œ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.
ì´ ê°’ì€ `build()`ì—ì„œ ì…ë ¥ í¬ê¸°ë¡œ ì‚¬ìš©ë˜ë©°,
`fit()`ì˜ ë°ì´í„° ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ì´ë¯¸ì§€ë¥¼ ìë¥¼ ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.

```python
class MyHyperModel(keras_tuner.HyperModel):
    def build(self, hp):
        image_size = hp.Int("image_size", 10, 28)
        inputs = keras.Input(shape=(image_size, image_size))
        outputs = layers.Flatten()(inputs)
        outputs = layers.Dense(
            units=hp.Int("units", min_value=32, max_value=512, step=32),
            activation="relu",
        )(outputs)
        outputs = layers.Dense(10, activation="softmax")(outputs)
        model = keras.Model(inputs, outputs)
        model.compile(
            optimizer="adam",
            loss="categorical_crossentropy",
            metrics=["accuracy"],
        )
        return model

    def fit(self, hp, model, x, y, validation_data=None, **kwargs):
        if hp.Boolean("normalize"):
            x = layers.Normalization()(x)
        image_size = hp.get("image_size")
        cropped_x = x[:, :image_size, :image_size, :]
        if validation_data:
            x_val, y_val = validation_data
            cropped_x_val = x_val[:, :image_size, :image_size, :]
            validation_data = (cropped_x_val, y_val)
        return model.fit(
            cropped_x,
            y,
            # ê° ì—í¬í¬ì—ì„œ ë°ì´í„°ë¥¼ ì…”í”Œí• ì§€ ì—¬ë¶€ë¥¼ íŠœë‹.
            shuffle=hp.Boolean("shuffle"),
            validation_data=validation_data,
            **kwargs,
        )


tuner = keras_tuner.RandomSearch(
    MyHyperModel(),
    objective="val_accuracy",
    max_trials=3,
    overwrite=True,
    directory="my_dir",
    project_name="tune_hypermodel",
)

tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Trial 3 Complete [00h 00m 04s]
val_accuracy: 0.9567000269889832
```

```plain
Best val_accuracy So Far: 0.9685999751091003
Total elapsed time: 00h 00m 13s
```

{{% /details %}}

### ëª¨ë¸ ì¬íŠ¸ë ˆì´ë‹ {#retrain-the-model}

`HyperModel`ì„ ì‚¬ìš©í•˜ë©´ ìµœì ì˜ ëª¨ë¸ì„ ì§ì ‘ ì¬íŠ¸ë ˆì´ë‹í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```python
hypermodel = MyHyperModel()
best_hp = tuner.get_best_hyperparameters()[0]
model = hypermodel.build(best_hp)
hypermodel.fit(best_hp, model, x_all, y_all, epochs=1)
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
1/1875 [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  9:00 289ms/step - accuracy: 0.0000e+00 - loss: 2.4352

52/1875 [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 996us/step - accuracy: 0.6035 - loss: 1.3521


110/1875 â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 925us/step - accuracy: 0.7037 - loss: 1.0231


171/1875 â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 890us/step - accuracy: 0.7522 - loss: 0.8572


231/1875 â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 877us/step - accuracy: 0.7804 - loss: 0.7590


291/1875 â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 870us/step - accuracy: 0.7993 - loss: 0.6932


350/1875 â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 867us/step - accuracy: 0.8127 - loss: 0.6467


413/1875 â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 856us/step - accuracy: 0.8238 - loss: 0.6079


476/1875 â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 848us/step - accuracy: 0.8326 - loss: 0.5774


535/1875 â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 849us/step - accuracy: 0.8394 - loss: 0.5536


600/1875 â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 841us/step - accuracy: 0.8458 - loss: 0.5309


661/1875 â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â” 1s 840us/step - accuracy: 0.8511 - loss: 0.5123


723/1875 â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â” 0s 837us/step - accuracy: 0.8559 - loss: 0.4955


783/1875 â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â” 0s 838us/step - accuracy: 0.8600 - loss: 0.4811


847/1875 â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â” 0s 834us/step - accuracy: 0.8640 - loss: 0.4671


912/1875 â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â” 0s 830us/step - accuracy: 0.8677 - loss: 0.4544


976/1875 â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â” 0s 827us/step - accuracy: 0.8709 - loss: 0.4429


1040/1875 â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â” 0s 825us/step - accuracy: 0.8738 - loss: 0.4325


1104/1875 â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â” 0s 822us/step - accuracy: 0.8766 - loss: 0.4229


1168/1875 â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â”â” 0s 821us/step - accuracy: 0.8791 - loss: 0.4140


1233/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â” 0s 818us/step - accuracy: 0.8815 - loss: 0.4056


1296/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â”â” 0s 817us/step - accuracy: 0.8837 - loss: 0.3980


1361/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â”â” 0s 815us/step - accuracy: 0.8858 - loss: 0.3907


1424/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â” 0s 814us/step - accuracy: 0.8877 - loss: 0.3840


1488/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â”â” 0s 813us/step - accuracy: 0.8895 - loss: 0.3776


1550/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â”â” 0s 813us/step - accuracy: 0.8912 - loss: 0.3718


1613/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â” 0s 813us/step - accuracy: 0.8928 - loss: 0.3662


1678/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â”â” 0s 811us/step - accuracy: 0.8944 - loss: 0.3607


1744/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ”â” 0s 809us/step - accuracy: 0.8959 - loss: 0.3555


1810/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ” 0s 808us/step - accuracy: 0.8973 - loss: 0.3504


1874/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[37mâ” 0s 807us/step - accuracy: 0.8987 - loss: 0.3457


1875/1875 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 808us/step - accuracy: 0.8987 - loss: 0.3456

<keras.src.callbacks.history.History at 0x7f31884b3070>
```

{{% /details %}}

## íŠœë‹ ëª©í‘œ ì§€ì • {#specify-the-tuning-objective}

ì§€ê¸ˆê¹Œì§€ì˜ ëª¨ë“  ì˜ˆì œì—ì„œëŠ” ê²€ì¦ ì •í™•ë„(`"val_accuracy"`)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.
ì‚¬ì‹¤, íŠœë‹ ëª©í‘œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë©”íŠ¸ë¦­ì€ ë¬´ì—‡ì´ë“  ê°€ëŠ¥í•©ë‹ˆë‹¤.
ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë©”íŠ¸ë¦­ì€ ê²€ì¦ ì†ì‹¤ì¸ `"val_loss"`ì…ë‹ˆë‹¤.

### ë¹ŒíŠ¸ì¸ ë©”íŠ¸ë¦­ì„ ëª©í‘œë¡œ ì‚¬ìš© {#built-in-metric-as-the-objective}

Kerasì—ëŠ” ëª©í‘œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¹ŒíŠ¸ì¸ ë©”íŠ¸ë¦­ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤.
[ë¹ŒíŠ¸ì¸ ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸]({{< relref "/docs/api/metrics" >}})ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

ë¹ŒíŠ¸ì¸ ë©”íŠ¸ë¦­ì„ ëª©í‘œë¡œ ì‚¬ìš©í•˜ë ¤ë©´, ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:

- ëª¨ë¸ì„ ë‚´ì¥ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì»´íŒŒì¼í•˜ì„¸ìš”.
  ì˜ˆë¥¼ ë“¤ì–´, `MeanAbsoluteError()`ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´,
  `metrics=[MeanAbsoluteError()]`ë¡œ ëª¨ë¸ì„ ì»´íŒŒì¼í•´ì•¼ í•©ë‹ˆë‹¤.
  ë˜í•œ í•´ë‹¹ ë©”íŠ¸ë¦­ì˜ ì´ë¦„ ë¬¸ìì—´ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤: `metrics=["mean_absolute_error"]`.
  ë©”íŠ¸ë¦­ì˜ ì´ë¦„ ë¬¸ìì—´ì€ í•­ìƒ í´ë˜ìŠ¤ ì´ë¦„ì„ ìŠ¤ë„¤ì´í¬ ì¼€ì´ìŠ¤ë¡œ ë³€í™˜í•œ í˜•ì‹ì…ë‹ˆë‹¤.
- ëª©í‘œ ì´ë¦„ ë¬¸ìì—´ì„ ì‹ë³„í•˜ì„¸ìš”.
  ëª©í‘œ ì´ë¦„ ë¬¸ìì—´ì€ í•­ìƒ `f"val_{metric_name_string}"` í˜•ì‹ì…ë‹ˆë‹¤.
  ì˜ˆë¥¼ ë“¤ì–´, ê²€ì¦ ë°ì´í„°ì— ëŒ€í•´ í‰ê°€í•œ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ì˜ ëª©í‘œ ì´ë¦„ ë¬¸ìì—´ì€ `"val_mean_absolute_error"`ê°€ ë©ë‹ˆë‹¤.
- ì´ë¥¼ [`keras_tuner.Objective`]({{< relref "/docs/api/keras_tuner/tuners/objective#objective-class" >}})ë¡œ ë˜í•‘í•˜ì„¸ìš”.
  ì¼ë°˜ì ìœ¼ë¡œ ëª©í‘œë¥¼ [`keras_tuner.Objective`]({{< relref "/docs/api/keras_tuner/tuners/objective#objective-class" >}}) ê°ì²´ë¡œ ë˜í•‘í•´ì„œ ëª©í‘œë¥¼ ìµœì í™”í•  ë°©í–¥ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
  ì˜ˆë¥¼ ë“¤ì–´, í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ë ¤ë©´,
  `keras_tuner.Objective("val_mean_absolute_error", "min")`ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  ë°©í–¥ì€ `"min"` ë˜ëŠ” `"max"` ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
- íŠœë„ˆì— ë˜í•‘ëœ ëª©í‘œë¥¼ ì „ë‹¬í•˜ì„¸ìš”.

ë‹¤ìŒì€ ìµœì†Œí•œì˜ ì½”ë“œ ì˜ˆì‹œì…ë‹ˆë‹¤.

```python
def build_regressor(hp):
    model = keras.Sequential(
        [
            layers.Dense(units=hp.Int("units", 32, 128, 32), activation="relu"),
            layers.Dense(units=1),
        ]
    )
    model.compile(
        optimizer="adam",
        loss="mean_squared_error",
        # ëª©í‘œëŠ” ë©”íŠ¸ë¦­ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
        metrics=[keras.metrics.MeanAbsoluteError()],
    )
    return model


tuner = keras_tuner.RandomSearch(
    hypermodel=build_regressor,
    # ëª©í‘œ ì´ë¦„ê³¼ ë°©í–¥.
    # ì´ë¦„ì€ f"val_{snake_case_metric_class_name}" í˜•ì‹ì…ë‹ˆë‹¤.
    objective=keras_tuner.Objective("val_mean_absolute_error", direction="min"),
    max_trials=3,
    overwrite=True,
    directory="my_dir",
    project_name="built_in_metrics",
)

tuner.search(
    x=np.random.rand(100, 10),
    y=np.random.rand(100, 1),
    validation_data=(np.random.rand(20, 10), np.random.rand(20, 1)),
)

tuner.results_summary()
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Trial 3 Complete [00h 00m 01s]
val_mean_absolute_error: 0.39589792490005493
```

```plain
Best val_mean_absolute_error So Far: 0.34321871399879456
Total elapsed time: 00h 00m 03s
Results summary
Results in my_dir/built_in_metrics
Showing 10 best trials
Objective(name="val_mean_absolute_error", direction="min")
```

```plain
Trial 1 summary
Hyperparameters:
units: 32
Score: 0.34321871399879456
```

```plain
Trial 2 summary
Hyperparameters:
units: 128
Score: 0.39589792490005493
```

```plain
Trial 0 summary
Hyperparameters:
units: 96
Score: 0.5005304217338562
```

{{% /details %}}

### ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ì„ ëª©í‘œë¡œ ì‚¬ìš© {#custom-metric-as-the-objective}

ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ì„ êµ¬í˜„í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ì˜ ëª©í‘œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ í‰ê·  ì œê³± ì˜¤ì°¨(MSE)ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.
ë¨¼ì €, [`keras.metrics.Metric`]({{< relref "/docs/api/metrics/base_metric#metric-class" >}})ë¥¼ ì„œë¸Œí´ë˜ì‹±í•˜ì—¬ MSE ë©”íŠ¸ë¦­ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
`super().__init__()`ì˜ `name` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë©”íŠ¸ë¦­ì˜ ì´ë¦„ì„ ì§€ì •í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”.
ì´ ì´ë¦„ì€ ë‚˜ì¤‘ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

ì°¸ê³ ë¡œ, MSEëŠ” ì‚¬ì‹¤ ë¹ŒíŠ¸ì¸ ë©”íŠ¸ë¦­ì´ë©°,
[`keras.metrics.MeanSquaredError`]({{< relref "/docs/api/metrics/regression_metrics#meansquarederror-class" >}})ë¥¼ í†µí•´ import í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ ì˜ˆì‹œëŠ” ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ì„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ëª©í‘œë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì´ íŠœí† ë¦¬ì–¼]({{< relref "/docs/api/metrics/#creating-custom-metrics" >}})ì„ ì°¸ì¡°í•˜ì„¸ìš”.
ë§Œì•½ `update_state(y_true, y_pred, sample_weight)`ì™€ëŠ” ë‹¤ë¥¸ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë©”íŠ¸ë¦­ì„ ë§Œë“¤ê³ ì í•œë‹¤ë©´,
[ì´ íŠœí† ë¦¬ì–¼]({{< relref "/docs/guides/custom_train_step_in_tensorflow/#going-lower-level" >}})ë¥¼ ë”°ë¼,
`train_step()` ë©”ì„œë“œë¥¼ ì¬ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from keras import ops


class CustomMetric(keras.metrics.Metric):
    def __init__(self, **kwargs):
        # ë©”íŠ¸ë¦­ ì´ë¦„ì„ "custom_metric"ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.
        super().__init__(name="custom_metric", **kwargs)
        self.sum = self.add_weight(name="sum", initializer="zeros")
        self.count = self.add_weight(name="count", dtype="int32", initializer="zeros")

    def update_state(self, y_true, y_pred, sample_weight=None):
        values = ops.square(y_true - y_pred)
        count = ops.shape(y_true)[0]
        if sample_weight is not None:
            sample_weight = ops.cast(sample_weight, self.dtype)
            values *= sample_weight
            count *= sample_weight
        self.sum.assign_add(ops.sum(values))
        self.count.assign_add(count)

    def result(self):
        return self.sum / ops.cast(self.count, "float32")

    def reset_state(self):
        self.sum.assign(0)
        self.count.assign(0)
```

ì»¤ìŠ¤í…€ ëª©í‘œë¡œ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

```python
def build_regressor(hp):
    model = keras.Sequential(
        [
            layers.Dense(units=hp.Int("units", 32, 128, 32), activation="relu"),
            layers.Dense(units=1),
        ]
    )
    model.compile(
        optimizer="adam",
        loss="mean_squared_error",
        # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ì„ metricsì— ì¶”ê°€í•©ë‹ˆë‹¤.
        metrics=[CustomMetric()],
    )
    return model


tuner = keras_tuner.RandomSearch(
    hypermodel=build_regressor,
    # ëª©í‘œì˜ ì´ë¦„ê³¼ ë°©í–¥ì„ ì§€ì •í•©ë‹ˆë‹¤.
    objective=keras_tuner.Objective("val_custom_metric", direction="min"),
    max_trials=3,
    overwrite=True,
    directory="my_dir",
    project_name="custom_metrics",
)

tuner.search(
    x=np.random.rand(100, 10),
    y=np.random.rand(100, 1),
    validation_data=(np.random.rand(20, 10), np.random.rand(20, 1)),
)

tuner.results_summary()
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Trial 3 Complete [00h 00m 01s]
val_custom_metric: 0.2830956280231476
```

```plain
Best val_custom_metric So Far: 0.2529197633266449
Total elapsed time: 00h 00m 02s
Results summary
Results in my_dir/custom_metrics
Showing 10 best trials
Objective(name="val_custom_metric", direction="min")
```

```plain
Trial 0 summary
Hyperparameters:
units: 32
Score: 0.2529197633266449
```

```plain
Trial 2 summary
Hyperparameters:
units: 128
Score: 0.2830956280231476
```

```plain
Trial 1 summary
Hyperparameters:
units: 96
Score: 0.4656866192817688
```

{{% /details %}}

ì»¤ìŠ¤í…€ ëª©í‘œë¥¼ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°,
`HyperModel.fit()`ì—ì„œ ì§ì ‘ ëª¨ë¸ì„ í‰ê°€í•˜ê³  ëª©í‘œ ê°’ì„ ë°˜í™˜í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
ì´ ê²½ìš° ëª©í‘œ ê°’ì€ ê¸°ë³¸ì ìœ¼ë¡œ ìµœì†Œí™”ë©ë‹ˆë‹¤.
ì´ëŸ¬í•œ ê²½ìš°, íŠœë„ˆë¥¼ ì´ˆê¸°í™”í•  ë•Œ `objective`ë¥¼ ì§€ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜, ì´ ê²½ìš° ë©”íŠ¸ë¦­ ê°’ì€ Keras ë¡œê·¸ì—ì„œ ì¶”ì ë˜ì§€ ì•Šìœ¼ë©°, KerasTuner ë¡œê·¸ì—ë§Œ ê¸°ë¡ë©ë‹ˆë‹¤.
ë”°ë¼ì„œ, ì´ ê°’ë“¤ì€ Keras ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ëŠ” TensorBoard ë·°ì—ì„œ í‘œì‹œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

```python
class HyperRegressor(keras_tuner.HyperModel):
    def build(self, hp):
        model = keras.Sequential(
            [
                layers.Dense(units=hp.Int("units", 32, 128, 32), activation="relu"),
                layers.Dense(units=1),
            ]
        )
        model.compile(
            optimizer="adam",
            loss="mean_squared_error",
        )
        return model

    def fit(self, hp, model, x, y, validation_data, **kwargs):
        model.fit(x, y, **kwargs)
        x_val, y_val = validation_data
        y_pred = model.predict(x_val)
        # ìµœì†Œí™”í•  ë‹¨ì¼ float ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        return np.mean(np.abs(y_pred - y_val))


tuner = keras_tuner.RandomSearch(
    hypermodel=HyperRegressor(),
    # ëª©í‘œë¥¼ ì§€ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
    # ëª©í‘œëŠ” `HyperModel.fit()`ì˜ ë°˜í™˜ ê°’ì…ë‹ˆë‹¤.
    max_trials=3,
    overwrite=True,
    directory="my_dir",
    project_name="custom_eval",
)
tuner.search(
    x=np.random.rand(100, 10),
    y=np.random.rand(100, 1),
    validation_data=(np.random.rand(20, 10), np.random.rand(20, 1)),
)

tuner.results_summary()
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Trial 3 Complete [00h 00m 01s]
default_objective: 0.6571611521766413
```

```plain
Best default_objective So Far: 0.40719249752993525
Total elapsed time: 00h 00m 02s
Results summary
Results in my_dir/custom_eval
Showing 10 best trials
Objective(name="default_objective", direction="min")
```

```plain
Trial 1 summary
Hyperparameters:
units: 128
Score: 0.40719249752993525
```

```plain
Trial 0 summary
Hyperparameters:
units: 96
Score: 0.4992297225533352
```

```plain
Trial 2 summary
Hyperparameters:
units: 32
Score: 0.6571611521766413
```

{{% /details %}}

KerasTunerì—ì„œ ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì„ ì¶”ì í•˜ë©´ì„œ ê·¸ì¤‘ í•˜ë‚˜ë§Œ ëª©í‘œë¡œ ì‚¬ìš©í•  ê²½ìš°,
ë©”íŠ¸ë¦­ ì´ë¦„ì„ í‚¤ë¡œ í•˜ê³  ë©”íŠ¸ë¦­ ê°’ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, `{"metric_a": 1.0, "metric_b": 2.0}`ì„ ë°˜í™˜í•˜ê³ ,
í‚¤ ì¤‘ í•˜ë‚˜ë¥¼ ëª©í‘œ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, `keras_tuner.Objective("metric_a", "min")`ì™€ ê°™ì´ ì„¤ì •í•©ë‹ˆë‹¤.

```python
class HyperRegressor(keras_tuner.HyperModel):
    def build(self, hp):
        model = keras.Sequential(
            [
                layers.Dense(units=hp.Int("units", 32, 128, 32), activation="relu"),
                layers.Dense(units=1),
            ]
        )
        model.compile(
            optimizer="adam",
            loss="mean_squared_error",
        )
        return model

    def fit(self, hp, model, x, y, validation_data, **kwargs):
        model.fit(x, y, **kwargs)
        x_val, y_val = validation_data
        y_pred = model.predict(x_val)
        # KerasTunerê°€ ì¶”ì í•  ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return {
            "metric_a": -np.mean(np.abs(y_pred - y_val)),
            "metric_b": np.mean(np.square(y_pred - y_val)),
        }


tuner = keras_tuner.RandomSearch(
    hypermodel=HyperRegressor(),
    # ëª©í‘œëŠ” ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
    # ìŒì˜ MAEë¥¼ ìµœëŒ€í™”, ì¦‰ MAEë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.
    objective=keras_tuner.Objective("metric_a", "max"),
    max_trials=3,
    overwrite=True,
    directory="my_dir",
    project_name="custom_eval_dict",
)
tuner.search(
    x=np.random.rand(100, 10),
    y=np.random.rand(100, 1),
    validation_data=(np.random.rand(20, 10), np.random.rand(20, 1)),
)

tuner.results_summary()
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Trial 3 Complete [00h 00m 01s]
metric_a: -0.39470441501524833
```

```plain
Best metric_a So Far: -0.3836997988261662
Total elapsed time: 00h 00m 02s
Results summary
Results in my_dir/custom_eval_dict
Showing 10 best trials
Objective(name="metric_a", direction="max")
```

```plain
Trial 1 summary
Hyperparameters:
units: 64
Score: -0.3836997988261662
```

```plain
Trial 2 summary
Hyperparameters:
units: 32
Score: -0.39470441501524833
```

```plain
Trial 0 summary
Hyperparameters:
units: 96
Score: -0.46081380465766364
```

{{% /details %}}

## ì—”ë“œíˆ¬ì—”ë“œ ì›Œí¬í”Œë¡œìš° íŠœë‹ {#tune-end-to-end-workflows}

ì¼ë¶€ ê²½ìš°ì—ëŠ”, ì½”ë“œë¥¼ ë¹Œë“œ ë° fit í•¨ìˆ˜ë¡œ ì •ë ¬í•˜ëŠ” ê²ƒì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ ê²½ìš° `Tuner.run_trial()`ì„ ì¬ì •ì˜í•˜ì—¬ ì—”ë“œíˆ¬ì—”ë“œ ì›Œí¬í”Œë¡œìš°ë¥¼ í•œê³³ì— ìœ ì§€í•  ìˆ˜ ìˆìœ¼ë©°,
ì´ë¥¼ í†µí•´ íŠ¸ë¼ì´ì–¼ì„ ì™„ì „íˆ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë¥¼ ì¼ì¢…ì˜ ë¸”ë™ë°•ìŠ¤ ì˜µí‹°ë§ˆì´ì €ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì–´ë–¤ í•¨ìˆ˜ì´ë“  íŠœë‹ {#tune-any-function}

ì˜ˆë¥¼ ë“¤ì–´, `f(x)=x*x+1`ì„ ìµœì†Œí™”í•˜ëŠ” `x` ê°’ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì•„ë˜ ì½”ë“œì—ì„œëŠ” `x`ë¥¼ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì •ì˜í•˜ê³ , `f(x)`ë¥¼ ëª©í‘œ ê°’ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
íŠœë„ˆë¥¼ ì´ˆê¸°í™”í•  ë•Œ, `hypermodel`ê³¼ `objective` ì¸ìˆ˜ëŠ” ìƒëµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
class MyTuner(keras_tuner.RandomSearch):
    def run_trial(self, trial, *args, **kwargs):
        # íŠ¸ë¼ì´ì–¼ì—ì„œ hp ê°€ì ¸ì˜¤ê¸°
        hp = trial.hyperparameters
        # "x"ë¥¼ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì •ì˜
        x = hp.Float("x", min_value=-1.0, max_value=1.0)
        # ìµœì†Œí™”í•  ëª©í‘œ ê°’ ë°˜í™˜
        return x * x + 1


tuner = MyTuner(
    # hypermodelì´ë‚˜ objectiveë¥¼ ì§€ì •í•˜ì§€ ì•ŠìŒ
    max_trials=20,
    overwrite=True,
    directory="my_dir",
    project_name="tune_anything",
)

# run_trial()ì—ì„œ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´,
# search()ì— ì•„ë¬´ê²ƒë„ ì „ë‹¬í•  í•„ìš” ì—†ìŒ
tuner.search()
print(tuner.get_best_hyperparameters()[0].get("x"))
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
Trial 20 Complete [00h 00m 00s]
default_objective: 1.6547719581194267
```

```plain
Best default_objective So Far: 1.0013236767905302
Total elapsed time: 00h 00m 00s
0.03638236922645777
```

{{% /details %}}

### Keras ì½”ë“œ ë¶„ë¦¬ ìœ ì§€ {#keep-keras-code-separate}

Keras ì½”ë“œë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ KerasTunerë¥¼ ì‚¬ìš©í•˜ì—¬ íŠœë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
Keras ì½”ë“œë¥¼ ìˆ˜ì •í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ì— ìœ ìš©í•©ë‹ˆë‹¤.

ì´ ë°©ì‹ì€ ë” ë§ì€ ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
ëª¨ë¸ ë¹Œë“œ ë° íŠ¸ë ˆì´ë‹ ì½”ë“œë¥¼ ë”°ë¡œ ë¶„ë¦¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜, ì´ ì›Œí¬í”Œë¡œìš°ëŠ” ëª¨ë¸ ì €ì¥ì´ë‚˜ TensorBoard í”ŒëŸ¬ê·¸ì¸ê³¼ì˜ ì—°ê²°ì„ ì œê³µí•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.

ëª¨ë¸ì„ ì €ì¥í•˜ë ¤ë©´, ê° íŠ¸ë¼ì´ì–¼ì„ ê³ ìœ í•˜ê²Œ ì‹ë³„í•˜ëŠ” ë¬¸ìì—´ì¸ `trial.trial_id`ë¥¼ ì‚¬ìš©í•˜ì—¬,
ì„œë¡œ ë‹¤ë¥¸ ê²½ë¡œë¥¼ êµ¬ì„±í•´ ê° íŠ¸ë¼ì´ì–¼ì—ì„œ ìƒì„±ëœ ëª¨ë¸ì„ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import os


def keras_code(units, optimizer, saving_path):
    # ëª¨ë¸ ë¹Œë“œ
    model = keras.Sequential(
        [
            layers.Dense(units=units, activation="relu"),
            layers.Dense(units=1),
        ]
    )
    model.compile(
        optimizer=optimizer,
        loss="mean_squared_error",
    )

    # ë°ì´í„° ì¤€ë¹„
    x_train = np.random.rand(100, 10)
    y_train = np.random.rand(100, 1)
    x_val = np.random.rand(20, 10)
    y_val = np.random.rand(20, 1)

    # ëª¨ë¸ íŠ¸ë ˆì´ë‹ ë° í‰ê°€
    model.fit(x_train, y_train)

    # ëª¨ë¸ ì €ì¥
    model.save(saving_path)

    # ëª©í‘œ ê°’ìœ¼ë¡œ ë‹¨ì¼ floatë¥¼ ë°˜í™˜.
    # {metric_name: metric_value} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    y_pred = model.predict(x_val)
    return np.mean(np.abs(y_pred - y_val))


class MyTuner(keras_tuner.RandomSearch):
    def run_trial(self, trial, **kwargs):
        hp = trial.hyperparameters
        return keras_code(
            units=hp.Int("units", 32, 128, 32),
            optimizer=hp.Choice("optimizer", ["adam", "adadelta"]),
            saving_path=os.path.join("/tmp", f"{trial.trial_id}.keras"),
        )


tuner = MyTuner(
    max_trials=3,
    overwrite=True,
    directory="my_dir",
    project_name="keep_code_separate",
)
tuner.search()
# ëª¨ë¸ ì¬íŠ¸ë ˆì´ë‹
best_hp = tuner.get_best_hyperparameters()[0]
keras_code(**best_hp.values, saving_path="/tmp/best_model.keras")
```

{{% details title="{{< t f_result >}}" closed="true" %}}

```plain
x_train shape: (60000, 28, 28, 1)
y_train shape: (60000,)
60000 train samples
10000 test samples
```

```plain
Trial 3 Complete [00h 00m 00s]
default_objective: 0.18014027375230962
```

```plain
Best default_objective So Far: 0.18014027375230962
Total elapsed time: 00h 00m 03s
```

```plain
1/4 â”â”â”â”â”[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 172ms/step - loss: 0.5030


4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 60ms/step - loss: 0.5288


4/4 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 61ms/step - loss: 0.5367

1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 27ms/step


1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 28ms/step

0.5918120126201316
```

{{% /details %}}

## KerasTunerì—ëŠ” ì‚¬ì „ ì œì‘ëœ íŠœë‹ ê°€ëŠ¥í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ HyperResNet ë° HyperXceptionì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. {#kerastuner-includes-pre-made-tunable-applications-hyperresnet-and-hyperxception}

ì´ë“¤ì€ ì»´í“¨í„° ë¹„ì „ì„ ìœ„í•œ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ í•˜ì´í¼ëª¨ë¸ì…ë‹ˆë‹¤.

ì´ ëª¨ë¸ë“¤ì€ `loss="categorical_crossentropy"`ì™€ `metrics=["accuracy"]`ë¡œ ì‚¬ì „ ì»´íŒŒì¼ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```python
from keras_tuner.applications import HyperResNet

hypermodel = HyperResNet(input_shape=(28, 28, 1), classes=10)

tuner = keras_tuner.RandomSearch(
    hypermodel,
    objective="val_accuracy",
    max_trials=2,
    overwrite=True,
    directory="my_dir",
    project_name="built_in_hypermodel",
)
```
